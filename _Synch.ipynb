{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "> Here I am connecting space on my google drive (for Colab conevience) to sync with github (for verson control)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load my Github API token into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/Coding/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "%cd \"/content/drive/MyDrive/Coding/ModelAssistedLabel/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dir = \"/content/drive/MyDrive/API Tokens\"\n",
    "f = open(f\"{token_dir}/ModelAssistedLabel repo.txt\")\n",
    "api_token = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure nbdev is good to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "  import nbdev\n",
    "except Exception as e:\n",
    "  !pip install nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_split.ipynb.\n",
      "Converted 02_train.ipynb.\n",
      "Converted 03_detect.ipynb.\n",
      "Converted index.ipynb.\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/01_split.ipynb\n",
      "converting /content/drive/My Drive/Coding/ModelAssistedLabel/index.ipynb to README.md\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib\n",
    "!nbdev_build_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git reset HEAD <file>...\" to unstage)\n",
      "\n",
      "\t\u001b[32mmodified:   00_config.ipynb\u001b[m\n",
      "\t\u001b[32mmodified:   01_split.ipynb\u001b[m\n",
      "\t\u001b[32mmodified:   02_train.ipynb\u001b[m\n",
      "\t\u001b[32mmodified:   03_detect.ipynb\u001b[m\n",
      "\t\u001b[32mmodified:   _Synch.ipynb\u001b[m\n",
      "\t\u001b[32mmodified:   docs/config.html\u001b[m\n",
      "\t\u001b[32mmodified:   docs/detect.html\u001b[m\n",
      "\t\u001b[32mmodified:   docs/train.html\u001b[m\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
      "  (commit or discard the untracked or modified content in submodules)\n",
      "\n",
      "\t\u001b[31mmodified:   yolov5\u001b[m (modified content, untracked content)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master ff7a96b] touchup\n",
      " 6 files changed, 50 insertions(+), 32 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"touchup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting objects: 10, done.\n",
      "Delta compression using up to 2 threads.\n",
      "Compressing objects:  10% (1/10)   \rCompressing objects:  20% (2/10)   \rCompressing objects:  30% (3/10)   \rCompressing objects:  40% (4/10)   \rCompressing objects:  50% (5/10)   \rCompressing objects:  60% (6/10)   \rCompressing objects:  70% (7/10)   \rCompressing objects:  80% (8/10)   \rCompressing objects:  90% (9/10)   \rCompressing objects: 100% (10/10)   \rCompressing objects: 100% (10/10), done.\n",
      "Writing objects:  10% (1/10)   \rWriting objects:  20% (2/10)   \rWriting objects:  30% (3/10)   \rWriting objects:  40% (4/10)   \rWriting objects:  50% (5/10)   \rWriting objects:  60% (6/10)   \rWriting objects:  70% (7/10)   \rWriting objects:  80% (8/10)   \rWriting objects:  90% (9/10)   \rWriting objects: 100% (10/10)   \rWriting objects: 100% (10/10), 1.61 KiB | 126.00 KiB/s, done.\n",
      "Total 10 (delta 9), reused 0 (delta 0)\n",
      "remote: Resolving deltas:   0% (0/9)\u001b[K\rremote: Resolving deltas:  11% (1/9)\u001b[K\rremote: Resolving deltas:  22% (2/9)\u001b[K\rremote: Resolving deltas:  33% (3/9)\u001b[K\rremote: Resolving deltas:  44% (4/9)\u001b[K\rremote: Resolving deltas:  55% (5/9)\u001b[K\rremote: Resolving deltas:  66% (6/9)\u001b[K\rremote: Resolving deltas:  77% (7/9)\u001b[K\rremote: Resolving deltas:  88% (8/9)\u001b[K\rremote: Resolving deltas: 100% (9/9)\u001b[K\rremote: Resolving deltas: 100% (9/9), completed with 9 local objects.\u001b[K\n",
      "To https://github.com/PhilBrockman/ModelAssistedLabel.git\n",
      "   6477965..ff7a96b  master -> master\n"
     ]
    }
   ],
   "source": [
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git commit -m: updated header boilerplate\n",
      "hooks\n",
      "diffs\n",
      "Converted 00_config.ipynb.\n",
      "No export destination, ignored:\n",
      "# export\n",
      "\n",
      "import glob\n",
      "from os.path import join\n",
      "import os\n",
      "\n",
      "class FileUtilities:\n",
      "  \"\"\"\n",
      "  Set of utility functions for managing the requirements of the Ultralytics repo\n",
      "  \"\"\"\n",
      "\n",
      "  def resource_map():\n",
      "    \"\"\"\n",
      "    Explicity define the extensions for images and labels. Check the `Default` class's\n",
      "    `resource_map` values\n",
      "    \"\"\"\n",
      "    return Defaults().resource_map\n",
      "\n",
      "  def collect_files(walk_dir, recursive):\n",
      "    \"\"\"\n",
      "    By default, returns all the \".jpg\" and \".txt\" files in a directory. The filetypes\n",
      "    are specified by the :resource_map:.\n",
      "\n",
      "    Args:\n",
      "      walk_dir: directory from which to pull resources\n",
      "      recursive: if `True`, resursively searches the folder for the desired resource.\n",
      "    \n",
      "    Returns:\n",
      "      A dictionary keyed to the :resource_map: with each value being an array of \n",
      "      the keyed type.\n",
      "    \"\"\"\n",
      "    res = {}\n",
      "    for key, extension in FileUtilities.resource_map().items():\n",
      "      resource_generator = glob.iglob(walk_dir + '/**/*' + extension, recursive=recursive)\n",
      "      res[key] = [{\"pair_id\": os.path.basename(x)[:-1*len(extension)], \"path\": x, \"basename\":os.path.basename(x)} for x in resource_generator]\n",
      "    return res\n",
      "\n",
      "  def matched(file_collection):\n",
      "    \"\"\"\n",
      "    Pairs up an image and label based on a shared resource name.\n",
      "\n",
      "    Arges:\n",
      "      res: the result of a \n",
      "    \"\"\"\n",
      "    bn = lambda x: set([z[\"pair_id\"] for z in x])\n",
      "    matched = (bn(file_collection[\"labels\"]).intersection(bn(file_collection[\"images\"])))\n",
      "    pairs = []\n",
      "    for resource in matched:\n",
      "      tmp = {}\n",
      "      for k in FileUtilities.resource_map():\n",
      "        tmp[k] = [x for x in file_collection[k] if x[\"pair_id\"] == resource][0]\n",
      "      pairs.append(tmp)\n",
      "      \n",
      "    return pairs\n",
      "\n",
      "  def match_files(walk_dir, recursive=True):\n",
      "    \"\"\"\n",
      "    From a bag of resources, find the paired images and labels.\n",
      "\n",
      "    Args:\n",
      "      walk_dir: recursively search for images/labels within this folder\n",
      "\n",
      "    Returns:\n",
      "      matched pairs of images and text within the `walk_dir`\n",
      "    \"\"\"\n",
      "    return FileUtilities.matched(FileUtilities.collect_files(walk_dir, recursive=recursive))\n",
      "\n",
      "No export destination, ignored:\n",
      "# export\n",
      "from ModelAssistedLabel.core import Defaults\n",
      "from datetime import datetime\n",
      "import math, random, shutil\n",
      "os.system(\"pip install progressbar2\")\n",
      "import progressbar\n",
      "\n",
      "class Generation:\n",
      "  \"\"\"\n",
      "    Container and organizer of photos for a given repository. This class \"softly\"\n",
      "    organizes the files upon the setting of the `split` attribute via `set_split`.\n",
      "\n",
      "    The split can then be written to disk by calling `write_split_to_disk`. The\n",
      "    relevant data will be zipped in `out_dir`\n",
      "  \"\"\"\n",
      "\n",
      "  def __init__(self, repo, out_dir, data_yaml=None, verbose=True, resource_dirs = [\"train\", \"valid\", \"test\"]):\n",
      "    \"\"\"\n",
      "      Args:\n",
      "        repo: <string> path to the parent directory of the repository.\n",
      "        out_dir: directory in which the zip file will be written\n",
      "        data_yaml: bridge between the \"class indices\" and \"class labels\"\n",
      "        verbose: spam `standard out` with info about files\n",
      "        resource_dirs: Ultralytics default names\n",
      "    \"\"\"\n",
      "    self.repo = repo\n",
      "    self.split = None\n",
      "    self.data_yaml = data_yaml\n",
      "    self.out_dir = out_dir\n",
      "    self.verbose = verbose\n",
      "    self.resource_dirs = resource_dirs\n",
      "\n",
      "  def set_split_from_disk(self):\n",
      "    \"sets the value of `self.split` to images present in train/valid/test folders on disk.\"\n",
      "    self.split = [{x: os.listdir(x)} for x in self.resource_dirs]\n",
      "\n",
      "  def set_split(self, split_ratio = None, MAX_SIZE=None):\n",
      "    \"\"\"\n",
      "    Sets the value of `self.split` \n",
      "\n",
      "    Args:\n",
      "      split_ratio: relative fractions of split between test train and validation\n",
      "      sets.\n",
      "      MAX_SIZE: The total number of images to be used in the image set. By default\n",
      "      includes all available images\n",
      "    \"\"\"\n",
      "    if split_ratio is None:\n",
      "      split_ratio = Defaults().split_ratio\n",
      "\n",
      "    files = FileUtilities.match_files(self.repo)\n",
      "    random.shuffle(files)\n",
      "    if MAX_SIZE is not None:\n",
      "      files = files[:MAX_SIZE]\n",
      "\n",
      "    train = math.ceil(len(files) * split_ratio[\"train\"])\n",
      "    valid = train + math.ceil(len(files) * split_ratio[\"valid\"])\n",
      "\n",
      "    split =  {\"train\": files[:train],\n",
      "    \"valid\": files[train: valid],\n",
      "    \"test\": files[valid:]}\n",
      "\n",
      "    assert sum([len(split[x]) for x in split]) == len(files)\n",
      "    self.split = split\n",
      "  \n",
      "  def get_split(self):\n",
      "    return [{x: len(self.split[x])} for x in self.split]\n",
      "\n",
      "  def write_split_to_disk(self, descriptor = \"\", autoname_output=True):\n",
      "    \"\"\"\n",
      "    Takes the given `self.split` and writes the split of the data to disk. Also\n",
      "    writes a data.yaml file to retain class label information.\n",
      "\n",
      "    Args:\n",
      "      descriptor: <str> a unique identifier for the output's filename\n",
      "      autoname_output: <bool> if True, `descriptor` field is a component of the\n",
      "      output's filename. Otherwise, sets the output filename to `{descriptor}.zip`\n",
      "\n",
      "    Returns:\n",
      "      A path to the zipped information.\n",
      "    \"\"\"\n",
      "    assert self.split is not None\n",
      "\n",
      "    if autoname_output:\n",
      "      out_folder = self.__default_filename__(descriptor)\n",
      "    else:\n",
      "      assert len(descriptor) > 0, \"need to provide a filename with `descriptor` argument\"\n",
      "      out_folder = descriptor\n",
      "      \n",
      "    dirs = self.__write_images__() #write images\n",
      "    print('dirs', dirs)\n",
      "    zipped = self.__zip_dirs__(out_folder, dirs) #zip folders\n",
      "    return zipped\n",
      "\n",
      "\n",
      "  def __zip_dirs__(self, zip_name, dirs):\n",
      "    \"\"\"\n",
      "    Takes an array of resources and places them all as the children in a specified\n",
      "    `zip_name`.\n",
      "\n",
      "    Args:\n",
      "      zip_name: Ultimately will be transformed into `{zip_name}.zip`\n",
      "      dirs: resources to become zipped\n",
      "\n",
      "    Returns:\n",
      "      the name of the zip file uniting the resources in `dirs`\n",
      "    \"\"\"\n",
      "    outpath = os.path.join(self.out_dir, zip_name)\n",
      "    assert not os.path.exists(outpath)\n",
      "    os.makedirs(outpath, exist_ok=True)\n",
      "    yaml_file = self.__write_data_yaml__(folder=outpath)\n",
      "    print(\"yaml\", yaml_file)\n",
      "    for subdir in self.split:\n",
      "      if self.verbose:\n",
      "        print(\"subdir\", subdir)\n",
      "        print(\"\\toutdir\", outpath)\n",
      "      shutil.move(subdir, outpath)\n",
      "\n",
      "    print(\"os.listdir\", os.listdir(outpath))\n",
      "\n",
      "    os.system(f'zip -r \"{outpath}.zip\" \"{outpath}\"')\n",
      "    os.system(f'rm -f -r \"{outpath}\"')\n",
      "    return f\"{outpath}.zip\"\n",
      "\n",
      "  \n",
      "  def __write_images__(self):\n",
      "    \"\"\"\n",
      "    If the dataset has already been split, then write the files to disk accordingly.\n",
      "    All resources are present two levels deep. The top folders are named according\n",
      "    to \"test\"/\"train\"/\"valid\". The mid-level folders are named \"images\" or \"labels\".\n",
      "    Resources can be found in the corresponding folder.\n",
      "\n",
      "    Returns:\n",
      "      A list of directories to the test/train/valid split\n",
      "    \"\"\"\n",
      "    assert self.split is not None\n",
      "    directories = []\n",
      "    counter = 0\n",
      "    print()\n",
      "    for dirname, pairs in self.split.items():\n",
      "      dir = join(\"./\", dirname) #test/valid/train\n",
      "      directories.append(dir)\n",
      "      for pair in pairs:\n",
      "        counter += 1\n",
      "        for resource, data in pair.items():\n",
      "          subdir = join(dir, resource)\n",
      "          os.makedirs(subdir, exist_ok=True)\n",
      "\n",
      "          target = data[\"path\"]\n",
      "          full_out = os.path.join(subdir, data[\"basename\"])\n",
      "\n",
      "          if not os.path.exists(full_out): \n",
      "            shutil.copyfile(target, full_out)\n",
      "            if self.verbose:\n",
      "              print(f\"({counter}) copying:\", target, end='\\r')\n",
      "    return directories\n",
      "    \n",
      "  def __default_filename__(self, prefix=\"\"):\n",
      "    \"\"\"\n",
      "    Helper to ease the burden of continually generating unique names or accidentally\n",
      "    overwriting important data.\n",
      "\n",
      "    Args:\n",
      "      prefix: zipfile identifier\n",
      "    \"\"\"\n",
      "    now = datetime.now() # current date and time\n",
      "    timestamp = now.strftime(\" %y-%m-%d %H-%M-%S\")\n",
      "    zipname = self.repo.split(\"/\")[-1] + prefix + timestamp\n",
      "    return zipname\n",
      "\n",
      "  def __write_data_yaml__(self, folder, filename=\"data.yaml\"):\n",
      "    \"\"\"\n",
      "    Write `self.data_yaml` to disk.\n",
      "\n",
      "    Args:\n",
      "      folder: directory in which to write the data\n",
      "      filename: optionally rename the yaml data's file\n",
      "    \"\"\"\n",
      "    outfile = os.path.join(folder, filename)\n",
      "    \n",
      "    if os.path.exists(outfile):\n",
      "      os.remove(outfile)\n",
      "\n",
      "    f = open(outfile,\"w\")\n",
      "    if self.data_yaml is None:\n",
      "      f.writelines(Defaults().data_yaml)\n",
      "    else:  \n",
      "      f.writelines(self.data_yaml)\n",
      "    f.close()\n",
      "    assert os.path.exists(outfile)\n",
      "    return outfile\n",
      "  \n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/nbdev_build_lib\", line 8, in <module>\n",
      "    sys.exit(nbdev_build_lib())\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/fastcore/script.py\", line 105, in _f\n",
      "    tfunc(**merge(args, args_from_prog(func, xtra)))\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/nbdev/export2html.py\", line 484, in nbdev_build_lib\n",
      "    notebook2script(fname=fname, bare=bare, recursive=Config().get('recursive', 'False').lower() == 'true')\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/nbdev/export.py\", line 423, in notebook2script\n",
      "    for f in sorted(files): d = _notebook2script(f, modules, silent=silent, to_dict=d, bare=bare)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/nbdev/export.py\", line 354, in _notebook2script\n",
      "    if to_dict is None: _add2all(fname_out, [f\"'{f}'\" for f in names if '.' not in f and len(f) > 0] + extra)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/nbdev/export.py\", line 206, in _add2all\n",
      "    with open(fname, 'r', encoding='utf8') as f: text = f.read()\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/My Drive/Coding/ModelAssistedLabel/ModelAssistedLabel/None.py'\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/03_detect.ipynb\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/01_split.ipynb\n",
      "An error occurred while executing the following cell:\n",
      "------------------\n",
      "show_doc(Detector, default_cls_level=2)\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)\n",
      "\u001b[0;32m<ipython-input-2-42e26899db41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_cls_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Detector' is not defined\n",
      "NameError: name 'Detector' is not defined\n",
      "\n",
      "An error occurred while executing the following cell:\n",
      "------------------\n",
      "show_doc(FileUtilities, default_cls_level=2)\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)\n",
      "\u001b[0;32m<ipython-input-2-c38ac90b638a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFileUtilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_cls_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FileUtilities' is not defined\n",
      "NameError: name 'FileUtilities' is not defined\n",
      "\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/02_train.ipynb\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/index.ipynb\n",
      "An error occurred while executing the following cell:\n",
      "------------------\n",
      "show_doc(Trainer, default_cls_level=2)\n",
      "------------------\n",
      "\n",
      "\u001b[0;31m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)\n",
      "\u001b[0;32m<ipython-input-2-4aea4c357aab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_cls_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined\n",
      "NameError: name 'Trainer' is not defined\n",
      "\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/00_config.ipynb\n",
      "Conversion failed on the following:\n",
      "03_detect.ipynb\n",
      "01_split.ipynb\n",
      "02_train.ipynb\n",
      "converting /content/drive/My Drive/Coding/ModelAssistedLabel/index.ipynb to README.md\n",
      "[master e4e2a77] updated header boilerplate\n",
      " 11 files changed, 139 insertions(+), 376 deletions(-)\n",
      " rewrite ModelAssistedLabel/detect.py (98%)\n",
      "Counting objects: 15, done.\n",
      "Delta compression using up to 2 threads.\n",
      "Compressing objects: 100% (15/15), done.\n",
      "Writing objects: 100% (15/15), 2.46 KiB | 193.00 KiB/s, done.\n",
      "Total 15 (delta 13), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (13/13), completed with 13 local objects.\u001b[K\n",
      "To https://github.com/PhilBrockman/ModelAssistedLabel.git\n",
      "   6032b18..e4e2a77  master -> master\n",
      "Branch 'master' set up to track remote branch 'master' from 'origin'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "message = lambda x: input(x)\n",
    "\n",
    "def dirty_clean(text, prefix=\"<a href=\\\"\", repo=\"ModelAssistedLabel\", b=\"https://github.com/PhilBrockman/ModelAssistedLabel/tree/master/ModelAssistedLabel\"):\n",
    "  return text.replace(prefix+repo, prefix+b)\n",
    "\n",
    "\n",
    "gitmsg = message(\"git commit -m: \")\n",
    "hooks = message(\"hooks\")\n",
    "diffs = message(\"diffs\")\n",
    "\n",
    "committing = len(gitmsg) > 0\n",
    "\n",
    "if committing or len(message(\"bulid libs?\")) > 0:\n",
    "  !nbdev_build_lib\n",
    "  !nbdev_build_docs\n",
    "\n",
    "if committing:\n",
    "  if len(hooks) > 0:\n",
    "    !nbdev_install_git_hooks\n",
    "\n",
    "  if len(diffs) > 0: \n",
    "    !nbdev_diff_nbs\n",
    "\n",
    "  for filename in [x for x in os.listdir(\"docs\") if x.endswith(\".html\")]:\n",
    "    f = open(f\"docs/{filename}\",\"r\")\n",
    "\n",
    "    content = \"\".join(f.readlines())\n",
    "    content = dirty_clean(content)\n",
    "\n",
    "    f = open(f\"docs/{filename}\",\"w+\")\n",
    "    f.writelines(content)\n",
    "    f.close()\n",
    "\n",
    "  !git config --global user.email \"phil.brockman@gmail.com\"\n",
    "  !git config --global user.name \"Phil Brockman\"\n",
    "  !git add .\n",
    "  !git add -u .\n",
    "  !git commit -m \"{gitmsg}\"\n",
    "  !git remote set-url origin https://PhilBrockman:{api_token}@github.com/PhilBrockman/ModelAssistedLabel.git\n",
    "  !git push -u origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
