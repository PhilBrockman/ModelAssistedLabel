{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%%capture\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 1.8.0+cu101 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n",
      "/content/drive/MyDrive/Coding/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%cd \"/content/drive/MyDrive/Coding/ModelAssistedLabel\"\n",
    "%run \"./00_ultralytics.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline is flowing!\n",
      "Converted 00_ultralytics.ipynb.\n",
      "Converted 01_split.ipynb.\n",
      "Converted 02_augment.ipynb.\n",
      "Converted index.ipynb.\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/01_split.ipynb\n",
      "converting /content/drive/My Drive/Coding/ModelAssistedLabel/index.ipynb to README.md\n",
      "Executing: git config --local include.path ../.gitconfig\n",
      "Success: hooks are installed and repo's .gitconfig is now trusted\n"
     ]
    }
   ],
   "source": [
    "%run \"_Synch.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelAssistedLabel.core import Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-dd4d4a9d43a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_yaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Defaults' object has no attribute 'data_yaml'"
     ]
    }
   ],
   "source": [
    "d = Defaults()\n",
    "d.data_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import os\n",
    "# define pathway to the weights\n",
    "weight_filenames = {\n",
    "    \"lcd\": \"21-2-20-94-universal-lcd.pt\",\n",
    "    \"digits\":'21-2-25 1k-digits YOLOv5-weights.pt'\n",
    "    }\n",
    "\n",
    "# detectors = []\n",
    "# for filename in weight_filenames:\n",
    "# weights_path = os.path.join(resource_folder, weights_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FileSystems 01\n",
    "\n",
    "> handling the logistics of grouping moving and overall wrangling groups of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = \".jpg\"\n",
    "labels = \".txt\"\n",
    "\n",
    "resource_map = {\"images\": images, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import glob\n",
    "from os.path import join\n",
    "\n",
    "class FileUtilities:\n",
    "  def collect_files(walk_dir, recursive):\n",
    "    \"\"\"\n",
    "    By default, returns all the \".jpg\" and \".txt\" files in a directory. The filetypes\n",
    "    are specified by the :resource_map:.\n",
    "\n",
    "    Args:\n",
    "      walk_dir: directory from which to pull resources\n",
    "      recursive: if `True`, resursively searches the folder for the desired resource.\n",
    "    \n",
    "    Returns:\n",
    "      A dictionary keyed to the :resource_map: with each value being an array of \n",
    "      the keyed type.\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for key, extension in resource_map.items():\n",
    "      resource_generator = glob.iglob(walk_dir + '/**/*' + extension, recursive=recursive)\n",
    "      res[key] = [{\"pair_id\": os.path.basename(x)[:-1*len(extension)], \"path\": x, \"basename\":os.path.basename(x)} for x in resource_generator]\n",
    "    return res\n",
    "\n",
    "  def matched(file_collection):\n",
    "    \"\"\"\n",
    "    Pairs up an image and label based on a shared resource name.\n",
    "\n",
    "    Arges:\n",
    "      res: the result of a \n",
    "    \"\"\"\n",
    "    bn = lambda x: set([z[\"pair_id\"] for z in x])\n",
    "    matched = (bn(file_collection[\"labels\"]).intersection(bn(file_collection[\"images\"])))\n",
    "    pairs = []\n",
    "    for resource in matched:\n",
    "      tmp = {}\n",
    "      for k in resource_map:\n",
    "        tmp[k] = [x for x in file_collection[k] if x[\"pair_id\"] == resource][0]\n",
    "      pairs.append(tmp)\n",
    "      \n",
    "    return pairs\n",
    "\n",
    "  def match_files(walk_dir, recursive=True):\n",
    "    return FileUtilities.matched(FileUtilities.collect_files(walk_dir, recursive=recursive))\n",
    "\n",
    "  def mkdir(dir):\n",
    "    import os\n",
    "    if not os.path.exists(dir):\n",
    "      os.mkdir(f\"{dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from ModelAssistedLabel.core import Defaults\n",
    "from datetime import datetime\n",
    "import math, random\n",
    "\n",
    "class Generation:\n",
    "  \"\"\"\n",
    "    Container and organizer of photos for a given repository.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, repo, out_dir, data_yaml):\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        repo: <string> path to the parent directory of the repository.\n",
    "    \"\"\"\n",
    "    self.repo = repo\n",
    "    self.split = None\n",
    "    self.data_yaml = data_yaml\n",
    "    self.out_dir = out_dir\n",
    "\n",
    "  def set_split(self, split_ratio = None, MAX_SIZE=None):\n",
    "    \"\"\"\n",
    "    Sets the value of `self.split` \n",
    "\n",
    "    Args:\n",
    "      split_ratio: relative fractions of split between test train and validation\n",
    "      sets.\n",
    "      MAX_SIZE: The total number of images to be used in the image set \n",
    "    \"\"\"\n",
    "    if split_ratio is None:\n",
    "      split_ratio = Defaults().split_ratio\n",
    "\n",
    "    files = FileUtilities.match_files(self.repo)\n",
    "    random.shuffle(files)\n",
    "    if MAX_SIZE:\n",
    "      files = files[:MAX_SIZE]\n",
    "\n",
    "    train = math.ceil(len(files) * split_ratio[\"train\"])\n",
    "    valid = train + math.ceil(len(files) * split_ratio[\"valid\"])\n",
    "\n",
    "    split =  {\"train\": files[:train],\n",
    "    \"valid\": files[train: valid],\n",
    "    \"test\": files[valid:]}\n",
    "\n",
    "    assert sum([len(split[x]) for x in split]) == len(files)\n",
    "    self.split = split\n",
    "  \n",
    "\n",
    "  def process_split(self, descriptor = \"\", autoname_output=True):\n",
    "    \"\"\"\n",
    "    Takes the given `self.split` and writes the split of the data to disk. Also\n",
    "    writes a data.yaml file to retain class label information.\n",
    "\n",
    "    Args:\n",
    "      descriptor: <str> a unique identifier for the output's filename\n",
    "      autoname_output: <bool> if True, `descriptor` field is a component of the\n",
    "      output's filename. Otherwise, it is the entire name.\n",
    "\n",
    "    Returns:\n",
    "      A path to the zipped information.\n",
    "    \"\"\"\n",
    "    assert self.split is not None\n",
    "\n",
    "    if autoname_output:\n",
    "      out_folder = self.default_filename(descriptor)\n",
    "    else:\n",
    "      assert len(descriptor) > 0, \"need to provide a filename with `descriptor` argument\"\n",
    "      out_folder = descriptor\n",
    "      \n",
    "    dirs = self.write_images() #write images\n",
    "    zipped = self.zip_dirs(out_folder, dirs) #zip folders\n",
    "    os.system(f\"mv '{zipped}' '{self.out_dir}'\") #move the output\n",
    "    return f\"{self.out_dir}/{zipped}\"\n",
    "\n",
    "\n",
    "  def zip_dirs(self, folder, dirs):\n",
    "    \"\"\"\n",
    "    Takes an array of resources and places them all as the children in a specified\n",
    "    `folder`.\n",
    "\n",
    "    Args:\n",
    "      folder: Ultimately will be transformed into `folder.zip`\n",
    "      dirs: resources to become zipped\n",
    "\n",
    "    Returns:\n",
    "      the name of the zip file uniting the resources in `dirs`\n",
    "    \"\"\"\n",
    "    FileUtilities.mkdir(folder)\n",
    "    self.write_data_yaml(folder)\n",
    "    for subdir in self.split:\n",
    "      os.system(f\"mv './{subdir}' '{folder}/'\")\n",
    "\n",
    "    os.system(f'zip -r \"{folder}.zip\" \"{folder}\"')\n",
    "    os.system(f'rm -f -r \"{folder}\"')\n",
    "    return f\"{folder}.zip\"\n",
    "\n",
    "  \n",
    "  def write_images(self):\n",
    "    \"\"\"\n",
    "    If the dataset has already been split, then write the files to disk accordingly.\n",
    "    All resources are present two levels deep. The top folders are named according\n",
    "    to \"test\"/\"train\"/\"valid\". The mid-level folders are named \"images\" or \"labels\".\n",
    "    Resources can be found in the corresponding folder.\n",
    "\n",
    "    Returns:\n",
    "      A list of directories to the test/train/valid split\n",
    "    \"\"\"\n",
    "    assert self.split is not None\n",
    "    directories = []\n",
    "    for dirname, pairs in self.split.items(): \n",
    "      dir = join(\"./\", dirname) #test/valid/train\n",
    "      FileUtilities.mkdir(dir)\n",
    "      directories.append(dir)\n",
    "      for pair in pairs:\n",
    "        for resource, data in pair.items():\n",
    "          subdir = join(dir, resource)\n",
    "          FileUtilities.mkdir(subdir)\n",
    "\n",
    "          target = data[\"path\"]\n",
    "          destination = join(subdir, data[\"basename\"])\n",
    "          print(\"target/dest\", target, \"|\", destination)\n",
    "          if not os.path.exists(destination): \n",
    "            os.system(f\"cp '{target}' '{destination}'\")\n",
    "    return directories\n",
    "    \n",
    "  def default_filename(self, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Helper to ease the burden of continually generating unique names or accidentally\n",
    "    overwriting important data.\n",
    "\n",
    "    Args:\n",
    "      prefix: \n",
    "    \"\"\"\n",
    "    now = datetime.now() # current date and time\n",
    "    timestamp = now.strftime(\" %y-%m-%d %H-%M-%S\")\n",
    "    zipname = self.repo.split(\"/\")[-1] + prefix + timestamp\n",
    "    return zipname\n",
    "\n",
    "  def write_data_yaml(self, folder=\"./\"):\n",
    "    f = open(join(folder, \"data.yaml\"),\"w+\")\n",
    "    f.writelines(self.data_yaml)\n",
    "    f.close()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a use case. Define the `repo` or the location of the images and their associated labels. `out_dir` is where the zipped and organized information is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-04f02ca391c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m g = Generation(repo = \"/content/drive/MyDrive/Coding/Roboflow Export (841)\", \n\u001b[1;32m      3\u001b[0m                \u001b[0mout_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Coding/01_train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                data_yaml=Defaults().data_yaml)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_SIZE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Defaults' object has no attribute 'data_yaml'"
     ]
    }
   ],
   "source": [
    "g = Generation(repo = \"/content/drive/MyDrive/Coding/Roboflow Export (841)\", \n",
    "               out_dir = \"/content/drive/MyDrive/Coding/01_train\",\n",
    "               data_yaml=Defaults().data_yaml)\n",
    "\n",
    "g.set_split(MAX_SIZE=4)\n",
    "out_dir = g.process_split(\" processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that no files will be overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/content/drive/MyDrive/Coding/01_train/Roboflow Export (841) processor 21-03-15 18-19-56.zip': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "test_eq_type(os.path.exists(os.path.basename(out_dir)), False)\n",
    "z = os.path.basename(out_dir)\n",
    "!mv \"{out_dir}\" . #help clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip and check if the number of files is as expected according to `g.split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./Roboflow Export (841) processor 21-03-15 18-19-56.zip\n",
      "   creating: Roboflow Export (841) processor 21-03-15 18-19-56/\n",
      "   creating: Roboflow Export (841) processor 21-03-15 18-19-56/train/\n",
      "   creating: Roboflow Export (841) processor 21-03-15 18-19-56/train/images/\n",
      "  inflating: Roboflow Export (841) processor 21-03-15 18-19-56/train/images/save_dirrtake-77-jpg-cropped-jpg-jpg_jpg.rf.fc755d0260916bf6d6126082a672aa3b.jpg  \n",
      "  inflating: Roboflow Export (841) processor 21-03-15 18-19-56/train/images/digittake-357-jpg_jpg.rf.6446f16c5c7157a36793a382780bbb31.jpg  \n",
      "  inflating: Roboflow Export (841) processor 21-03-15 18-19-56/train/images/digittake-33-jpg_jpg.rf.f1d00d9a9f1e64e8da2e528824bbb27f.jpg  \n",
      "   creating: Roboflow Export (841) processor 21-03-15 18-19-56/train/labels/\n",
      "  inflating: Roboflow Export (841) processor 21-03-15 18-19-56/train/labels/save_dirrtake-77-jpg-cropped-jpg-jpg_jpg.rf.fc755d0260916bf6d6126082a672aa3b.txt  \n",
      "  inflating: Roboflow Export (841) processor 21-03-15 18-19-56/train/labels/digittake-357-jpg_jpg.rf.6446f16c5c7157a36793a382780bbb31.txt  \n",
      "  inflating: Roboflow Export (841) processor 21-03-15 18-19-56/train/labels/digittake-33-jpg_jpg.rf.f1d00d9a9f1e64e8da2e528824bbb27f.txt  \n",
      "   creating: Roboflow Export (841) processor 21-03-15 18-19-56/valid/\n",
      "   creating: Roboflow Export (841) processor 21-03-15 18-19-56/valid/images/\n",
      "  inflating: Roboflow Export (841) processor 21-03-15 18-19-56/valid/images/digittake-32-jpg_jpg.rf.288a105b2a8f24157752afc8e81ef150.jpg  \n",
      "   creating: Roboflow Export (841) processor 21-03-15 18-19-56/valid/labels/\n",
      "  inflating: Roboflow Export (841) processor 21-03-15 18-19-56/valid/labels/digittake-32-jpg_jpg.rf.288a105b2a8f24157752afc8e81ef150.txt  \n",
      "   creating: Roboflow Export (841) processor 21-03-15 18-19-56/test/\n",
      "  inflating: Roboflow Export (841) processor 21-03-15 18-19-56/data.yaml  \n"
     ]
    }
   ],
   "source": [
    "!unzip \"./{z}\"\n",
    "ls = !ls \"{z[:-4]}/train/labels\"\n",
    "test_eq(len(ls), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the test zip file and zipped file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f -r \"{z[:-4]}\"\n",
    "!rm \"{z}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
