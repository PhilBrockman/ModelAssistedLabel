{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # default_exp fileManagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "# %load_ext autoreload \n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Coding/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%cd \"/content/drive/MyDrive/Coding/ModelAssistedLabel\"\n",
    "# %run \"./00_ultralytics.ipynb\"\n",
    "# \n",
    "from ModelAssistedLabel.core import Defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstracting File Management\n",
    "\n",
    "Each file has its own assumptions about file structure. I use these classes to organize images and labels into the appropriate location at the appropriate time.\n",
    "\n",
    "```\n",
    "- OTHER_DIR_WITH_CUSTOM_DATA\n",
    "  - bag-of-images-and-labels\n",
    "\n",
    "- ROOT DIRECTORY\n",
    "  - test/\n",
    "  - valid/\n",
    "  - train/\n",
    "  - data.yaml\n",
    "  - yolov5/\n",
    "      - train.py\n",
    "      - detect.py\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import glob\n",
    "from os.path import join\n",
    "import os\n",
    "\n",
    "class FileUtilities:\n",
    "  \"\"\"\n",
    "  Set of utility functions for managing the requirements of the Ultralytics repo\n",
    "  \"\"\"\n",
    "\n",
    "  def resource_map():\n",
    "    \"\"\"\n",
    "    Explicity define the extensions for images and labels. Check the `Default` class's\n",
    "    `resource_map` values\n",
    "    \"\"\"\n",
    "    return Defaults().resource_map\n",
    "\n",
    "  def collect_files(walk_dir, recursive):\n",
    "    \"\"\"\n",
    "    By default, returns all the \".jpg\" and \".txt\" files in a directory. The filetypes\n",
    "    are specified by the :resource_map:.\n",
    "\n",
    "    Args:\n",
    "      walk_dir: directory from which to pull resources\n",
    "      recursive: if `True`, resursively searches the folder for the desired resource.\n",
    "    \n",
    "    Returns:\n",
    "      A dictionary keyed to the :resource_map: with each value being an array of \n",
    "      the keyed type.\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for key, extension in FileUtilities.resource_map().items():\n",
    "      resource_generator = glob.iglob(walk_dir + '/**/*' + extension, recursive=recursive)\n",
    "      res[key] = [{\"pair_id\": os.path.basename(x)[:-1*len(extension)], \"path\": x, \"basename\":os.path.basename(x)} for x in resource_generator]\n",
    "    return res\n",
    "\n",
    "  def matched(file_collection):\n",
    "    \"\"\"\n",
    "    Pairs up an image and label based on a shared resource name.\n",
    "\n",
    "    Arges:\n",
    "      res: the result of a \n",
    "    \"\"\"\n",
    "    bn = lambda x: set([z[\"pair_id\"] for z in x])\n",
    "    matched = (bn(file_collection[\"labels\"]).intersection(bn(file_collection[\"images\"])))\n",
    "    pairs = []\n",
    "    for resource in matched:\n",
    "      tmp = {}\n",
    "      for k in FileUtilities.resource_map():\n",
    "        tmp[k] = [x for x in file_collection[k] if x[\"pair_id\"] == resource][0]\n",
    "      pairs.append(tmp)\n",
    "      \n",
    "    return pairs\n",
    "\n",
    "  def match_files(walk_dir, recursive=True):\n",
    "    \"\"\"\n",
    "    From a bag of resources, find the paired images and labels.\n",
    "\n",
    "    Args:\n",
    "      walk_dir: recursively search for images/labels within this folder\n",
    "\n",
    "    Returns:\n",
    "      matched pairs of images and text within the `walk_dir`\n",
    "    \"\"\"\n",
    "    return FileUtilities.matched(FileUtilities.collect_files(walk_dir, recursive=recursive))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from ModelAssistedLabel.core import Defaults\n",
    "from datetime import datetime\n",
    "import math, random, shutil\n",
    "os.system(\"pip install progressbar2\")\n",
    "import progressbar\n",
    "\n",
    "class Generation:\n",
    "  \"\"\"\n",
    "    Container and organizer of photos for a given repository. This class \"softly\"\n",
    "    organizes the files upon the setting of the `split` attribute via `set_split`.\n",
    "\n",
    "    The split can then be written to disk by calling `write_split_to_disk`. The\n",
    "    relevant data will be zipped in `out_dir`\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, repo, out_dir, data_yaml=None, verbose=True, resource_dirs = [\"train\", \"valid\", \"test\"]):\n",
    "    \"\"\"\n",
    "      Args:\n",
    "        repo: <string> path to the parent directory of the repository.\n",
    "        out_dir: directory in which the zip file will be written\n",
    "        data_yaml: bridge between the \"class indices\" and \"class labels\"\n",
    "        verbose: spam `standard out` with info about files\n",
    "        resource_dirs: Ultralytics default names\n",
    "    \"\"\"\n",
    "    self.repo = repo\n",
    "    self.split = None\n",
    "    self.data_yaml = data_yaml\n",
    "    self.out_dir = out_dir\n",
    "    self.verbose = verbose\n",
    "    self.resource_dirs = resource_dirs\n",
    "\n",
    "  def set_split_from_disk(self):\n",
    "    \"sets the value of `self.split` to images present in train/valid/test folders on disk.\"\n",
    "    self.split = [{x: os.listdir(x)} for x in self.resource_dirs]\n",
    "\n",
    "  def set_split(self, split_ratio = None, MAX_SIZE=None):\n",
    "    \"\"\"\n",
    "    Sets the value of `self.split` \n",
    "\n",
    "    Args:\n",
    "      split_ratio: relative fractions of split between test train and validation\n",
    "      sets.\n",
    "      MAX_SIZE: The total number of images to be used in the image set. By default\n",
    "      includes all available images\n",
    "    \"\"\"\n",
    "    if split_ratio is None:\n",
    "      split_ratio = Defaults().split_ratio\n",
    "\n",
    "    files = FileUtilities.match_files(self.repo)\n",
    "    random.shuffle(files)\n",
    "    if MAX_SIZE is not None:\n",
    "      files = files[:MAX_SIZE]\n",
    "\n",
    "    train = math.ceil(len(files) * split_ratio[\"train\"])\n",
    "    valid = train + math.ceil(len(files) * split_ratio[\"valid\"])\n",
    "\n",
    "    split =  {\"train\": files[:train],\n",
    "    \"valid\": files[train: valid],\n",
    "    \"test\": files[valid:]}\n",
    "\n",
    "    assert sum([len(split[x]) for x in split]) == len(files)\n",
    "    self.split = split\n",
    "  \n",
    "  def get_split(self):\n",
    "    return [{x: len(self.split[x])} for x in self.split]\n",
    "\n",
    "  def write_split_to_disk(self, descriptor = \"\", autoname_output=True):\n",
    "    \"\"\"\n",
    "    Takes the given `self.split` and writes the split of the data to disk. Also\n",
    "    writes a data.yaml file to retain class label information.\n",
    "\n",
    "    Args:\n",
    "      descriptor: <str> a unique identifier for the output's filename\n",
    "      autoname_output: <bool> if True, `descriptor` field is a component of the\n",
    "      output's filename. Otherwise, sets the output filename to `{descriptor}.zip`\n",
    "\n",
    "    Returns:\n",
    "      A path to the zipped information.\n",
    "    \"\"\"\n",
    "    assert self.split is not None\n",
    "\n",
    "    if autoname_output:\n",
    "      out_folder = self.__default_filename__(descriptor)\n",
    "    else:\n",
    "      assert len(descriptor) > 0, \"need to provide a filename with `descriptor` argument\"\n",
    "      out_folder = descriptor\n",
    "      \n",
    "    dirs = self.__write_images__() #write images\n",
    "    print('dirs', dirs)\n",
    "    zipped = self.__zip_dirs__(out_folder, dirs) #zip folders\n",
    "    return zipped\n",
    "\n",
    "\n",
    "  def __zip_dirs__(self, zip_name, dirs):\n",
    "    \"\"\"\n",
    "    Takes an array of resources and places them all as the children in a specified\n",
    "    `zip_name`.\n",
    "\n",
    "    Args:\n",
    "      zip_name: Ultimately will be transformed into `{zip_name}.zip`\n",
    "      dirs: resources to become zipped\n",
    "\n",
    "    Returns:\n",
    "      the name of the zip file uniting the resources in `dirs`\n",
    "    \"\"\"\n",
    "    outpath = os.path.join(self.out_dir, zip_name)\n",
    "    assert not os.path.exists(outpath)\n",
    "    os.makedirs(outpath, exist_ok=True)\n",
    "    yaml_file = self.__write_data_yaml__(folder=outpath)\n",
    "    print(\"yaml\", yaml_file)\n",
    "    for subdir in self.split:\n",
    "      if self.verbose:\n",
    "        print(\"subdir\", subdir)\n",
    "        print(\"\\toutdir\", outpath)\n",
    "      shutil.move(subdir, outpath)\n",
    "\n",
    "    print(\"os.listdir\", os.listdir(outpath))\n",
    "\n",
    "    os.system(f'zip -r \"{outpath}.zip\" \"{outpath}\"')\n",
    "    os.system(f'rm -f -r \"{outpath}\"')\n",
    "    return f\"{outpath}.zip\"\n",
    "\n",
    "  \n",
    "  def __write_images__(self):\n",
    "    \"\"\"\n",
    "    If the dataset has already been split, then write the files to disk accordingly.\n",
    "    All resources are present two levels deep. The top folders are named according\n",
    "    to \"test\"/\"train\"/\"valid\". The mid-level folders are named \"images\" or \"labels\".\n",
    "    Resources can be found in the corresponding folder.\n",
    "\n",
    "    Returns:\n",
    "      A list of directories to the test/train/valid split\n",
    "    \"\"\"\n",
    "    assert self.split is not None\n",
    "    directories = []\n",
    "    counter = 0\n",
    "    print()\n",
    "    for dirname, pairs in self.split.items():\n",
    "      dir = join(\"./\", dirname) #test/valid/train\n",
    "      directories.append(dir)\n",
    "      for pair in pairs:\n",
    "        counter += 1\n",
    "        for resource, data in pair.items():\n",
    "          subdir = join(dir, resource)\n",
    "          os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "          target = data[\"path\"]\n",
    "          full_out = os.path.join(subdir, data[\"basename\"])\n",
    "\n",
    "          if not os.path.exists(full_out): \n",
    "            shutil.copyfile(target, full_out)\n",
    "            if self.verbose:\n",
    "              print(f\"({counter}) copying:\", target, end='\\r')\n",
    "    return directories\n",
    "    \n",
    "  def __default_filename__(self, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Helper to ease the burden of continually generating unique names or accidentally\n",
    "    overwriting important data.\n",
    "\n",
    "    Args:\n",
    "      prefix: zipfile identifier\n",
    "    \"\"\"\n",
    "    now = datetime.now() # current date and time\n",
    "    timestamp = now.strftime(\" %y-%m-%d %H-%M-%S\")\n",
    "    zipname = self.repo.split(\"/\")[-1] + prefix + timestamp\n",
    "    return zipname\n",
    "\n",
    "  def __write_data_yaml__(self, folder, filename=\"data.yaml\"):\n",
    "    \"\"\"\n",
    "    Write `self.data_yaml` to disk.\n",
    "\n",
    "    Args:\n",
    "      folder: directory in which to write the data\n",
    "      filename: optionally rename the yaml data's file\n",
    "    \"\"\"\n",
    "    outfile = os.path.join(folder, filename)\n",
    "    \n",
    "    if os.path.exists(outfile):\n",
    "      os.remove(outfile)\n",
    "\n",
    "    f = open(outfile,\"w\")\n",
    "    if self.data_yaml is None:\n",
    "      f.writelines(Defaults().data_yaml)\n",
    "    else:  \n",
    "      f.writelines(self.data_yaml)\n",
    "    f.close()\n",
    "    assert os.path.exists(outfile)\n",
    "    return outfile\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a two-step confirmation before making any changes to disk. First, crawl through the `repo` and collect all images and collect all labels. Labels must be in [YOLOv5 format](https://github.com/AlexeyAB/Yolo_mark/issues/60#issuecomment-401854885)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Generation(repo=\"./Image Repo/labeled/Final Roboflow Export (841)\", \n",
    "               out_dir=\"ipynb_tests/01_split_datadump\",\n",
    "               verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a custom split or use only a maximum number of annotated images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train': 10}, {'valid': 6}, {'test': 4}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_split = {\"train\": .5, \"valid\": .3, \"test\": .2}\n",
    "g.set_split(split_ratio=custom_split, MAX_SIZE=20)\n",
    "g.get_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or split the entire repository on the default 70%/20%/10% of train/valid/test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train': 589}, {'valid': 169}, {'test': 83}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.set_split()\n",
    "g.get_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the desired split to a zip folder that is sent to the `out_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dirs ['./train', './valid', './test']\n",
      "yaml ipynb_tests/01_split_datadump/Final Roboflow Export (841)<01_split_all> 21-03-21 14-30-17/data.yaml\n",
      "subdir train\n",
      "\toutdir ipynb_tests/01_split_datadump/Final Roboflow Export (841)<01_split_all> 21-03-21 14-30-17\n",
      "subdir valid\n",
      "\toutdir ipynb_tests/01_split_datadump/Final Roboflow Export (841)<01_split_all> 21-03-21 14-30-17\n",
      "subdir test\n",
      "\toutdir ipynb_tests/01_split_datadump/Final Roboflow Export (841)<01_split_all> 21-03-21 14-30-17\n",
      "os.listdir ['train', 'valid', 'test', 'data.yaml']\n"
     ]
    }
   ],
   "source": [
    "zipped = g.write_split_to_disk(descriptor=\"<01_split_all>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Complete: \", zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
