# AUTOGENERATED! DO NOT EDIT! File to edit: 03_detect.ipynb (unless otherwise specified).

__all__ = ['Defaults', 'FileUtilities', 'Generation', 'Trainer', 'AutoWeights', 'Detector', 'make_dir', 'touch_file',
           'draw_screen', 'Frame', 'Prediction', 'preds', 'DataSet', 'bb_intersection_over_union', 'sorted_api',
           'digit_arr_to_string', 'corners', 'calculate_iou', 'yoloBox_to_standard', 'raw_parse_from_json',
           'parse_from_json']

# Cell
class Defaults:
  """
  Helps keep a DRY principle across this project. The split ratio was defined based
  on information found on Roboflow. `self.trainer_template` is pulled from the
  YOLOv5 tutorial, as is the code within `prepare_YOLOv5`
  """

  def __init__(self):
    self.root = "/content/drive/MyDrive/Coding/ModelAssistedLabel/"
    self.split_ratio = {
              "train": .7,
              "valid": .2,
              "test": .1
            }
    self.data_yaml = "\n".join(["train: ../train/images",
    "val: ../valid/images",
    "",
    "nc: 10",
    "names: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']"])
    self.resource_map = {"images": ".jpg", "labels": ".txt"}

    self.trainer_template = f"""# parameters
    nc: {10}  # number of classes
    depth_multiple: 0.33  # model depth multiple
    width_multiple: 0.50  # layer channel multiple

    # anchors
    anchors:
      - [10,13, 16,30, 33,23]  # P3/8
      - [30,61, 62,45, 59,119]  # P4/16
      - [116,90, 156,198, 373,326]  # P5/32

    # YOLOv5 backbone
    backbone:
      # [from, number, module, args]
      [[-1, 1, Focus, [64, 3]],  # 0-P1/2
      [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
      [-1, 3, BottleneckCSP, [128]],
      [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
      [-1, 9, BottleneckCSP, [256]],
      [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
      [-1, 9, BottleneckCSP, [512]],
      [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
      [-1, 1, SPP, [1024, [5, 9, 13]]],
      [-1, 3, BottleneckCSP, [1024, False]],  # 9
      ]

    # YOLOv5 head
    head:
      [[-1, 1, Conv, [512, 1, 1]],
      [-1, 1, nn.Upsample, [None, 2, 'nearest']],
      [[-1, 6], 1, Concat, [1]],  # cat backbone P4
      [-1, 3, BottleneckCSP, [512, False]],  # 13

      [-1, 1, Conv, [256, 1, 1]],
      [-1, 1, nn.Upsample, [None, 2, 'nearest']],
      [[-1, 4], 1, Concat, [1]],  # cat backbone P3
      [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)

      [-1, 1, Conv, [256, 3, 2]],
      [[-1, 14], 1, Concat, [1]],  # cat head P4
      [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)

      [-1, 1, Conv, [512, 3, 2]],
      [[-1, 10], 1, Concat, [1]],  # cat head P5
      [-1, 3, BottleneckCSP, [1024,
      False]],  # 23 (P5/32-large)

      [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
      ]"""

  def prepare_YOLOv5():
    """
    * Clone repository if the YOLOv5 directory does not exist.
    * Install requirements.txt
    * Check that GPU is enabled.
    """
    # safety for re-executions
    if not os.path.exists("yolov5"):
      # clone YOLOv5 and reset to a specific git checkpoint that has been verified working
      os.system("git clone https://github.com/ultralytics/yolov5")  # clone repo
      os.system("git reset --hard 68211f72c99915a15855f7b99bf5d93f5631330f") # standardize models

    # enter the yolov5 directory
    os.chdir("yolov5")

    # install dependencies as necessary
    os.system("pip install -qr requirements.txt")  # install dependencies (ignore errors)
    import torch

    from IPython.display import Image, clear_output  # to display images
    # from utils.google_utils import gdrive_download  # to download models/datasets

    clear_output()

    if torch.cuda.is_available():
      print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0)))
    else:
      raise Exception("You need to enable your GPU access to this runtime environment")

    # return to parent directory
    os.chdir("..")

  #   calls = {}
  #   calls["nbdev"] = (["pip install nbdev"])
  #   calls["lib/docs"] = (["nbdev_build_lib", "nbdev_build_docs"])
  #   self.calls = calls

  # def call_all(self, arr):
  #   for x in arr:
  #     os.system(x)

  # def nbdev(self):
  #   for x in self.calls:
  #     self.call_all(calls[x])

# Cell

import glob
from os.path import join
import os

class FileUtilities:
  """
  Set of utility functions for managing the requirements of the Ultralytics repo
  """

  def resource_map():
    """
    Explicity define the extensions for images and labels. Check the `Default` class's
    `resource_map` values
    """
    return Defaults().resource_map

  def collect_files(walk_dir, recursive):
    """
    By default, returns all the ".jpg" and ".txt" files in a directory. The filetypes
    are specified by the :resource_map:.

    Args:
      walk_dir: directory from which to pull resources
      recursive: if `True`, resursively searches the folder for the desired resource.

    Returns:
      A dictionary keyed to the :resource_map: with each value being an array of
      the keyed type.
    """
    res = {}
    for key, extension in FileUtilities.resource_map().items():
      resource_generator = glob.iglob(walk_dir + '/**/*' + extension, recursive=recursive)
      res[key] = [{"pair_id": os.path.basename(x)[:-1*len(extension)], "path": x, "basename":os.path.basename(x)} for x in resource_generator]
    return res

  def matched(file_collection):
    """
    Pairs up an image and label based on a shared resource name.

    Arges:
      res: the result of a
    """
    bn = lambda x: set([z["pair_id"] for z in x])
    matched = (bn(file_collection["labels"]).intersection(bn(file_collection["images"])))
    pairs = []
    for resource in matched:
      tmp = {}
      for k in FileUtilities.resource_map():
        tmp[k] = [x for x in file_collection[k] if x["pair_id"] == resource][0]
      pairs.append(tmp)

    return pairs

  def match_files(walk_dir, recursive=True):
    """
    From a bag of resources, find the paired images and labels.

    Args:
      walk_dir: recursively search for images/labels within this folder

    Returns:
      matched pairs of images and text within the `walk_dir`
    """
    return FileUtilities.matched(FileUtilities.collect_files(walk_dir, recursive=recursive))

  def mkdir(dir):
    """
    Don't overwrite an existing file when calling mkdir.
    """
    import os
    if not os.path.exists(dir):
      os.mkdir(f"{dir}")


# Cell
from .core import Defaults
from datetime import datetime
import math, random

class Generation:
  """
    Container and organizer of photos for a given repository. This class "softly"
    organizes the files upon the setting of the `split` attribute via `set_split`.

    The split can then be written to disk by calling `write_split_to_disk`. The
    relevant data will be zipped in `out_dir`
  """

  def __init__(self, repo, out_dir, data_yaml):
    """
      Args:
        repo: <string> path to the parent directory of the repository.
    """
    self.repo = repo
    self.split = None
    self.data_yaml = data_yaml
    self.out_dir = out_dir

  def set_split(self, split_ratio = None, MAX_SIZE=None):
    """
    Sets the value of `self.split`

    Args:
      split_ratio: relative fractions of split between test train and validation
      sets.
      MAX_SIZE: The total number of images to be used in the image set
    """
    if split_ratio is None:
      split_ratio = Defaults().split_ratio

    files = FileUtilities.match_files(self.repo)
    random.shuffle(files)
    if MAX_SIZE:
      files = files[:MAX_SIZE]

    train = math.ceil(len(files) * split_ratio["train"])
    valid = train + math.ceil(len(files) * split_ratio["valid"])

    split =  {"train": files[:train],
    "valid": files[train: valid],
    "test": files[valid:]}

    assert sum([len(split[x]) for x in split]) == len(files)
    self.split = split


  def write_split_to_disk(self, descriptor = "", autoname_output=True):
    """
    Takes the given `self.split` and writes the split of the data to disk. Also
    writes a data.yaml file to retain class label information.

    Args:
      descriptor: <str> a unique identifier for the output's filename
      autoname_output: <bool> if True, `descriptor` field is a component of the
      output's filename. Otherwise, it is the entire name.

    Returns:
      A path to the zipped information.
    """
    assert self.split is not None

    if autoname_output:
      out_folder = self.__default_filename__(descriptor)
    else:
      assert len(descriptor) > 0, "need to provide a filename with `descriptor` argument"
      out_folder = descriptor

    dirs = self.__write_images__() #write images
    zipped = self.__zip_dirs__(out_folder, dirs) #zip folders
    os.system(f"mv '{zipped}' '{self.out_dir}'") #move the output
    return f"{self.out_dir}/{zipped}"


  def __zip_dirs__(self, folder, dirs):
    """
    Takes an array of resources and places them all as the children in a specified
    `folder`.

    Args:
      folder: Ultimately will be transformed into `folder.zip`
      dirs: resources to become zipped

    Returns:
      the name of the zip file uniting the resources in `dirs`
    """
    FileUtilities.mkdir(folder)
    self.__write_data_yaml__(folder)
    for subdir in self.split:
      os.system(f"mv './{subdir}' '{folder}/'")

    os.system(f'zip -r "{folder}.zip" "{folder}"')
    os.system(f'rm -f -r "{folder}"')
    return f"{folder}.zip"


  def __write_images__(self):
    """
    If the dataset has already been split, then write the files to disk accordingly.
    All resources are present two levels deep. The top folders are named according
    to "test"/"train"/"valid". The mid-level folders are named "images" or "labels".
    Resources can be found in the corresponding folder.

    Returns:
      A list of directories to the test/train/valid split
    """
    assert self.split is not None
    directories = []
    for dirname, pairs in self.split.items():
      dir = join("./", dirname) #test/valid/train
      FileUtilities.mkdir(dir)
      directories.append(dir)
      for pair in pairs:
        for resource, data in pair.items():
          subdir = join(dir, resource)
          FileUtilities.mkdir(subdir)

          target = data["path"]
          destination = join(subdir, data["basename"])
          print("target/dest", target, "|", destination)
          if not os.path.exists(destination):
            os.system(f"cp '{target}' '{destination}'")
    return directories

  def __default_filename__(self, prefix=""):
    """
    Helper to ease the burden of continually generating unique names or accidentally
    overwriting important data.

    Args:
      prefix: zipfile identifier
    """
    now = datetime.now() # current date and time
    timestamp = now.strftime(" %y-%m-%d %H-%M-%S")
    zipname = self.repo.split("/")[-1] + prefix + timestamp
    return zipname

  def __write_data_yaml__(self, folder, filename="data.yaml"):
    """
    Write `self.data_yaml` to disk.

    Args:
      folder: directory in which to write the data
      filename: optionally rename the yaml data's file
    """
    f = open(os.path.join(folder, filename),"w+")
    f.writelines(self.data_yaml)
    f.close()


# Cell
from .core import Defaults
import os

class Trainer():
  """A wrapper for Ultralytic's `test.py`

  Write the backbone of the model to file and then run YOLOv5's train file."""

  def __init__(self, name, yaml_file = "models/custom_yolov5s.yaml"):
    """
    sets the current directory to the project's root as defined in Defaults.

    Args:
      name: identifier for results
      yaml_file: path to write the file
    """
    os.chdir(Defaults().root)
    self.yaml_file = yaml_file
    self.name = name
    self.template = Defaults().trainer_template

  def write_yaml(self):
    """
    Records YOLOv5 architecture
    """
    f = open(f"yolov5/{self.yaml_file}","w+")
    f.writelines(self.template)
    f.close()

  def train(self, epochs):
    """
    wrapper for train.py.

    Args:
      epochs: number of iterations
    """
    self.write_yaml()
    os.chdir("yolov5")
    os.system("pip install -r requirements.txt")
    os.system(f"python train.py --img 416 --batch 16 --epochs {epochs} --data '../data.yaml' --cfg {self.yaml_file} --weights '' --name {self.name}  --cache")
    os.chdir("..")

# Cell
from .core import Trainer, Generation, Defaults
from datetime import datetime

class AutoWeights():
  """Given a bag of images (.jpg) and labels (.txt) in YOLOv5 format in a repository,
  initialize the ROOT directory with a train-valid-test split and a file needed
  by the Ultralytics repository. Pairs are identified if `image[:-4] == label[:-4]`

  Then call `generate_weights` to run `train.py`. The resultant file will try to
  be moved to the `out_dir` and if a conflict exists, a new name will be made.
  """
  def __init__(self, resource_dir, name="DEFAULT", out_dir=".", MAX_SIZE=5, custom_data_yaml=None, verbose=True, train_path = "yolov5/runs/train"):
    """
    Args:
      resource_dir: location of the bag of images/labels
      out_dir: where the results of train.py are moved
      MAX_SIZE: parameter for `Generation`
      custom_data_yaml: see `Defaults`'s `data_yaml` for the default value
      verbose: Print summary information
      train_path: path to Ultralytic's default output folder
    """
    self.resource_dir = resource_dir
    self.name = name
    self.resource_paths = ["test/", "train/", "valid/", "data.yaml"]
    for r in self.resource_paths: #make sure none of these paths already exist
      assert os.path.exists(r) is False, f"{r} exists... You may be in the middle of an active project"
    self.out_dir = out_dir
    self.train_path = train_path
    #automatically build the resource paths and prepare for traniing
    self.__prepare_split__(MAX_SIZE=MAX_SIZE, data_yaml=custom_data_yaml, verbose=verbose)
    assert self.g is not None

  def generate_weights(self, epochs, tidy_weights=True):
    """
    Creates a `Trainer` object and trains for a given amount of time.

    Args:
      epochs: number of iterations (according to docs, over 3000 is not uncommon)
      tidy_weights: if True, remove all of the resources in `self.resources`

    Returns:
      path to the output folder of train.py
    """
    t = Trainer(self.name)
    ldir = lambda path: set(os.listdir(path))

    before = ldir(self.train_path)
    t.train(epochs)
    after = ldir(self.train_path)

    assert len(after) == len(before)+1 #only should have made one new file
    diff = list(after - before)[0]

    results_path = os.path.join(self.train_path, diff)

    if tidy_weights:
      results_path = self.__tidy_weights__(results_path = results_path)

    self.__cleanup__()
    self.last_results_path = results_path
    return results_path

  def __prepare_split__(self, MAX_SIZE, data_yaml, verbose):
    """
    Gets the local filesystem ready to run the wrapper for "train.py".

    Args:
      MAX_SIZE:
      data_yaml:
      verbose: print summary information for the split
    """
    if data_yaml is None:
      data_yaml = Defaults().data_yaml
    self.g = Generation(repo=self.resource_dir, out_dir=self.out_dir, data_yaml=data_yaml)
    self.g.set_split(MAX_SIZE=MAX_SIZE)
    self.__split_and_organize_folders__(verbose=verbose)

  def __split_and_organize_folders__(self, verbose):
    """
    Assume zip file contains 4 items:
      * data.yaml
      * train/
      * valid/
      * test/

    Extract these 4 resources to the ROOT directory and remove the original
    part of the file.

    Args:
      verbose: print summary information about the split
    """
    assert self.g is not None

    def tostr(split):
      return [{k: len(v)} for k,v in split.items()]
    def sumg(split):
      return sum([list(x.values())[0] for x in tostr(split)])
    if verbose:
      print("summary: ", tostr(self.g.split))
      print("checksum:", sumg(self.g.split))

    zipped = self.g.write_split_to_disk(self.name) #create a zip file in the ROOT directory
    local = os.path.basename(zipped)
    os.system(f'unzip "{local}"') #grab data
    #move the contents of the zip file into postion within the ROOT directory
    for content in self.resource_paths:
      os.system(f"mv '{local[:-4]}/{content}' .")
    #remove zip file
    os.system(f"rm -f -r '{local}'")
    #removed the folder that was taken out of the zip
    os.system(f"rm -f -r '{local[:-4]}'")

  def __cleanup__(self):
    """
    Removes all resources in `self.resource_paths` from the filesystem.
    """
    for r in self.resource_paths:
      os.system(f"rm -f -r {r}")

  def __tidy_weights__(self, results_path):
    """
    Moves the results to a desired directly while ensuring that no data is overwritten

    Args:
      results_path: path to the folder that has desired information

    Returns:
      Path to the newly-moved results
    """
    while True:
      now = datetime.now()
      out = f"{os.path.basename(results_path)}-{now.strftime('%f')}" #basename + "random" string
      outfolder = os.path.join(self.out_dir, out)
      if not os.path.exists(outfolder): #only stops if a unique name has been found
        break
    os.system(f"mv '{results_path}' '{outfolder}'")
    return outfolder

# Cell
import os
os.chdir("yolov5")
from pathlib import Path
from utils.plots import plot_one_box

from utils.general import check_img_size, non_max_suppression
from utils.datasets import LoadStreams, LoadImages
from utils.general import scale_coords, xyxy2xywh
from utils.torch_utils import select_device
from models.experimental import attempt_load
import cv2, os, base64, json
os.chdir("..")

os.system("pip install colour")
os.system("pip install pillow")

class Detector:
  """A wrapper for training loading saved YOLOv5 weights

  requirements:
    GPU enabled"""
  def __init__(self, weight_path, conf_threshold = .4, iou_threshold = .45, imgsz = 416, save_dir="save_dir", save_labeled_img = True, log_bboxes_as_txt = True, ):
    """ Constructor. I pulled the default numeric values above directly from the
    detect.py file. I added the option to save model output to both images and
    to txt files

    Args:
      weight_path: the path to which the saved weights are stored
      conf_threshold: lower bound on the acceptable level of uncertainty for a
                      bounding box
      iou_threshold: IoU helps determine how overlapped two shapes are.
      imgsz: resolution of image to process (assumes square)
      save_labeled_img: save a copy of the image with visibile bounding boxing
      log_bboxes_as_txt: create 1 (True) or 0 (False) txt files per bounding box
    """

    self.weight_path = weight_path
    self.conf_threshold = conf_threshold
    self.iou_threshold = iou_threshold
    self.imgsz = imgsz
    self.device = select_device()
    self.model = attempt_load(self.weight_path, map_location=self.device)  # load FP32 model
    self.names = self.model.module.names if hasattr(self.model, 'module') else self.model.names
    self.imgsz = check_img_size(self.imgsz, s=self.model.stride.max())  # check img_size
    self.half = self.device.type != 'cuda'
    if self.half:
      self.model.half()  # to FP16

    self.save_labeled_img = save_labeled_img
    self.log_bboxes_as_txt = log_bboxes_as_txt
    self.save_dir = save_dir

  def process_image(self, source, save_unscuffed=True):
    """Runs on the model with pre-specified weights an input image. See original
    detect.py for more details

    Args:
      source: A string path to pre-specified weights for the model
      save_unscuffed: create copy of the pre-image

    Reurns:
      A JSON-serializable object encoding bounding box information
    """
    results = []
    img = torch.zeros((1, 3, self.imgsz, self.imgsz), device=self.device)  # init img
    _ = self.model(img.half() if self.half else img) if self.device.type != 'cpu' else None  # run once
    dataset = LoadImages(source, img_size=self.imgsz)
    save_dir = Path(self.save_dir)
    make_dir(save_dir)
    for path, img, im0s, vid_cap in dataset:
      tmp = {}
      tmp["txt"] = []
      img = torch.from_numpy(img).to(self.device)
      img = img.half() if self.half else img.float()  # uint8 to fp16/32
      img /= 255.0  # 0 - 255 to 0.0 - 1.0
      if img.ndimension() == 3:
        img = img.unsqueeze(0)

      pred = self.model(img, augment=False)[0]
      pred = non_max_suppression(pred, self.conf_threshold, self.iou_threshold, agnostic=False)

      for i, det in enumerate(pred):
        p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)
        p = Path(p)
        save_path = str(save_dir / p.name)

        make_dir(str(save_dir / 'labels'))
        txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')

        s += '%gx%g ' % img.shape[2:]  # print string
        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh

        if len(det):
          det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()
          for c in det[:, -1].unique():
              n = (det[:, -1] == c).sum()  # detections per class
              s += f'{n} {self.names[int(c)]}s, '  # add to string

          if save_unscuffed:
            tmp["unscuffed"] = f"{save_dir}/unscuffed-{p.name}"
            cv2.imwrite(tmp["unscuffed"], im0)

          for *xyxy, conf, cls in reversed(det):
            if self.log_bboxes_as_txt:
              xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh
              line = (cls, *xywh, conf) if True else (cls, *xywh)  # label format
              i = 0
              run = True
              while run:
                txt_path_ =  f'{txt_path}-{i}.txt'
                if not os.path.exists(txt_path_):
                  open(txt_path_, 'a').close()
                  tmp["txt"].append(txt_path_)
                  run = False
                i += 1
              with open(txt_path_, 'a') as f:
                        f.write(('%g ' * len(line)).rstrip() % line + '\n')
              if True:
                label = f'{self.names[int(cls)]} {conf:.2f}'
                plot_one_box(xyxy, im0, label=label, color=[0,0,200], line_thickness=5)

            # save image with bboxes drawn on top
            if self.save_labeled_img:
              tmp["labeled"] = f"{save_dir}/labeled-{p.name}"
              cv2.imwrite(tmp["labeled"], im0)

    return tmp

# Cell

def make_dir(dir):
  """makes a directory provided that the directiory doesn't already exist

  Args:
    dir: Directory to create a path towards
  """

  if not os.path.exists(dir):
    os.makedirs(dir)


def touch_file(fpath):
  """`touch`es a file"""
  open(fpath, 'a').close()


def _itername(pre, post):
  """If function terminates, returns the lowest conflict-free file path
  formatted as '{pre}X{post}' where X is the string representation of a natural
  number

  args:
    pre: filename before the counter
    post: filename after the counter

  returns:
    A unique structured filename
  """
  counter = 0
  while True:
    counter += 1
    fpath = f'{pre}{counter}{post}'
    if not os.path.exists(fpath):
      return fpath


# Cell
import cv2,  base64

def draw_screen(image):
  """Converts a cv2 imread to base 64

  Args:
    image: resultant value of cv2 imread
  """
  _, buffer = cv2.imencode('.jpg', image)
  jpg_as_text = base64.b64encode(buffer)
  return jpg_as_text.decode('ascii')

# Cell
import cv2

class Frame:
  """Container for holding an image along with any predictions"""
  def __init__(self, jpg_file, predictions = []):
    """Args:
      jpg_file: string path to a jpeg image
      predictions: array of text files containing bounding box information for
        this image"""
    self.source = jpg_file
    self.image = cv2.imread(self.source)

    #this line make JSON serialization difficult. Yolo(v5) to the consequences!
    self.predictions = [Prediction(x, self) for x in predictions]

# Cell

class Prediction:
  def __init__(self, txt_file=None, parent = None):
    """
    Parses the first line of a txt file. Uses YOLOv5 format:

    CLASS, XCENTER, YCENTER, WIDTH, HEIGHT, CONFIDENCE

    Note that x&y axis have been normed to the interval 0-1. So expect CLASS
    to be an `int` while the remaining values will be of type `float`.

    In the event that a text file is not specified, all values will be set to -1.
    """
    if txt_file:
      self.txt_file = txt_file
      s = open(txt_file, "r").read().split('\n')[0]
      self.original_txt = s
      s = s.split(" ")
    else:
      s = [-1]*6
    self.class_label = s[0]
    self.confidence = s[-1]
    s = [float(x) for x in s[1:5]]
    labels = "xcent ycent width height".split(" ")
    self.bbox = (dict(zip(labels, s))) #zooped
    self.parent = parent

# Cell

def preds(i):
  """
  Returns:
    Extracts the array of predictions from model's results
  """
  #legacy compatability
  if "digits" in i.keys():
    preds = i["digits"]["txt"]
  else:
    preds = i["txt"]
  return preds

class DataSet():
  """holds together a series of Frame objects with the eventual goal of exporting
  the data contained therein to a JSON-friendly format"""
  def __init__(self, image_list):
    """Highly tuned to the Detector class

    image_list: an array of frames processed by a Detector"""
    self.frames = [Frame(jpg_file=image["preimage location"], predictions=preds(image)) for image in image_list]

  def export(self, base64Image=True):
    """
    Exports all of the data I could access from detect.py

    Args:
      base64Image: very heavy string. If you don't need the image in base64, set to false

    Returns:
      JSON-friendly object (array of dictionaries (that themselves contain arrays :( )))
    """
    out = []
    for s in self.frames:
      try:
        tmp = {}
        tmp["filename"] = s.source
        tmp["predictions"] = []
        if base64Image:
          tmp['base64'] = draw_screen(s.image)
        for prediction in s.predictions:
          td = {}
          td['bbox'] = prediction.bbox
          td["class index"] = prediction.class_label
          td['confidence'] = prediction.confidence
          tmp["predictions"].append(td)
        out.append(tmp)
      except Exception as e:
        print(f"failed on {s.source} with error {e}")
    return out




# Cell
def bb_intersection_over_union(boxA, boxB):
  "Source: https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/"
  # determine the (x, y)-coordinates of the intersection rectangle
  xA = max(boxA[0], boxB[0])
  yA = max(boxA[1], boxB[1])
  xB = min(boxA[2], boxB[2])
  yB = min(boxA[3], boxB[3])

  # compute the area of intersection rectangle
  interArea = (xB - xA) * (yB - yA)

  # compute the area of both the prediction and ground-truth
  # rectangles
  boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
  boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])

  # compute the intersection over union by taking the intersection
  # area and dividing it by the sum of prediction + ground-truth
  # areas - the interesection area
  iou = interArea / float(boxAArea + boxBArea - interArea)

  # return the intersection over union value
  return iou

# Cell
def sorted_api(predictions):
  """sorts a list of prediction in ascending x order

  Args:
    an array of dictionaries containing bbox predictions

  Returns:
    the bounding boxes read off from left to right"""
  return sorted(predictions, key=lambda x: x['x'])

def digit_arr_to_string(predictions):
  """"Human-friendly interpretation of predictions"""
  return "".join(str(int(x['class'])%90) for x in sorted_api(predictions))


def corners(r):
  """
  Defines the relevante coordinate information

  Args:
    r: converts a DataFrame row to a tuple

  Returns:
    Caretensian coordinate information (anchor in the lower left)
  """
  return (r['x'], r['y'], r['x'] + r['width'], r['y'] + r['height'])

def calculate_iou(frame, threshold = .4):
  """
  Determines the pair of bounding boxes that has the highest IoU over the :threshold:

  Args:
    frame: the output of either a YOLOv5 model call or an API call
    threshold: maximum percentage of overlap allowed between the bounding boxes
  """
  previous = None
  overlappers = None

  for i in range(len(sorted_api(frame))):
    row = sorted_api(frame)[i]
    row["index"] = i
    if previous is None:
      previous = row
      continue

    iou = bb_intersection_over_union(corners(previous), corners(row))
    if iou > threshold:
      tmp = {row["confidence"]: row, previous["confidence"]: previous}
      overlappers = {
        "iou": iou,
        "least confident": tmp[min(tmp.keys())]
      }

    previous = row
  return overlappers

# Cell

import PIL

def yoloBox_to_standard(result):
  """
  The standard YOLOv5 coordinate format is normed to 1. Need to extract the
  original's image width and height to convert to a standard cartesian plane.

  Args:
    result: a dictionary that includes both
        * a key called "filename" that points to the original image
        * a key called "predictions" created when the image is parsed with the
          YOLOv5 model

  Returns:
    Convert the predictions converted to a full-scale Cartesian coordinate system.
  """
  out = {}
  for k,v in result.items():
    out[k] = v

  PILim= PIL.Image.open(result["preimage location"])
  width, height = PILim.width, PILim.height

  out["predictions"] = []
  for prediction in result["predictions"]:
    bbox = prediction["bbox"]
    out["predictions"].append({
        "class": (int(prediction["class index"])+1)%10,
        "confidence": prediction["confidence"],
        "height": PILim.height*bbox["height"],
        "width": PILim.width*bbox["width"],
        "x": PILim.width*(bbox["xcent"] - bbox["width"]/2),
        "y": PILim.height*(bbox["ycent"] - bbox["height"]/2)
        })
  return out

# Cell

import io, PIL, base64

def raw_parse_from_json(json_elements, loaded_detector):
  """
  Helper to interpret raw base 64 images. Creates a new JPG file from the base64
  data and feeds the image to the Detector

  Args:
    json_elements: array of dictionaries from file. Assert existence of a "base64"
      key where the images are stored
    loaded_detector: a valid Detector

  returns:
    the result of processessing all of the base64 images
  """
  out = []
  for idx in range(len(json_elements)):
    from64name = _itername("(", ") image.jpg")
    b64image = json_elements[idx]["base64"]
    im = PIL.Image.open(io.BytesIO(base64.b64decode(b64image)))
    im.save(from64name)

    single = loaded_detector.process_image(from64name)
    single["preimage location"] = from64name
    out.append(single)
  return out

def parse_from_json(json_elements, loaded_detector):
  """
  Gets a JSON file containing the base64 images of data to label pushed through
  a Detector. Post-processing automatically de-norms the YOLOv5 bounding boxes
  based on the original image's height and width

  Args:
    json_elements: array of dictionaries from file. Assert existence of a "base64"
      key where the images are stored
    loaded_detector: a valid Detector

  returns:
    the bounding boxes associated with each image
  """
  preprocessed = raw_parse_from_json(json_elements, loaded_detector)
  ds = DataSet(preprocessed)
  processed=ds.export()

  for i in range(len(processed)):
    processed[i]["preimage location"] = processed[i]["filename"]

  postprocess = []
  for x in processed:
    postprocess.append(yoloBox_to_standard(x))
  return postprocess