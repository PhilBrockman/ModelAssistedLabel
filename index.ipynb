{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-asisted Labeling with YOLOv5\n",
    "> custom image set annotation with a model's help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![base64 splash](https://github.com/PhilBrockman/ModelAssistedLabel/blob/master/modelassistedlabel%20splash.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My exercise equipment, despite even being electronic, doesn’t connect to a network. But I still want the \"smart workout\" experience.\n",
    "\n",
    "Maybe if I instead point my webcam at the equipment’s LCD output, I can make a machine learn to identify and interpret useful information. Perfect! I’ll just utilize object detection to determine the location and identity of the machine’s analog readout. \n",
    "\n",
    "First question, just a tiny one, how do you do that?  \n",
    "\n",
    "After wading through several guides, I found [Roboflow's YOLOv5 tutorial]( https://models.roboflow.com/object-detection/yolov5). They helped provide a hands-on and accessible experience in machine learning. But unfortunately, I didn't have much luck parsing my digital screens with available models/datasets. Instead, I decided to start building my own dataset.\n",
    "\n",
    "I realize that if I label enough digits, I can train a weak(er) YOLO model to tell me what it sees. I can then take that information and pre-label my images with those predictions. I would later learn that this bootstrapping of annotation is called machine-assisted labeling.\n",
    "\n",
    "The pieces come together.  I can focus on writing code while I use Roboflow to sort, generate, and deliver my images. I sleuth through [Ultralytic's](https://github.com/ultralytics/yolov5) original project and build wrappers around the essential functions in `detect.py` and `train.py`.\n",
    "\n",
    "This repository contains the tools that let me \"pre-label\" my images before sending them off for human inspection and correction. I would typically only use the cells under \"labing a new set of images\" and \"exporting annotated images\" once my weights have been generated. \n",
    "\n",
    "> Note: In `./Image Repo` I provide access to 841 labeled images (lumped in one folder) and 600 unlabeled images (seperated into three sets of 200 images - lighting condition is the same within each run, but differs between runs). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: [Open In Colab](https://colab.research.google.com/github/PhilBrockman/ModelAssistedLabel/blob/master/index.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ModelAssistedLabel'...\n",
      "remote: Enumerating objects: 463, done.\u001b[K\n",
      "remote: Counting objects: 100% (463/463), done.\u001b[K\n",
      "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
      "remote: Total 4440 (delta 332), reused 439 (delta 312), pack-reused 3977\u001b[K\n",
      "Receiving objects: 100% (4440/4440), 210.50 MiB | 13.74 MiB/s, done.\n",
      "Resolving deltas: 100% (1355/1355), done.\n",
      "Checking out files: 100% (2381/2381), done.\n",
      "/content/drive/MyDrive/vision.philbrockman.com/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "# clone this repository\n",
    "!git clone -b future_forward https://github.com/PhilBrockman/ModelAssistedLabel.git\n",
    "%cd \"ModelAssistedLabel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Inputs:\n",
    "\n",
    "-  **labeled images**\n",
    "    + All of the images and labels must be in a common folder (subfolders allowed).\n",
    "    + Labels must be in [YOLOv5 format](https://github.com/AlexeyAB/Yolo_mark/issues/60#issuecomment-401854885).\n",
    "\n",
    "    > Note: Image/label pairs are based on their base filename. For example `image.jpg/image.txt` would be paired as would `other_image5.jpg/other_image5.txt`.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these images have already been labeled\n",
    "labeled_images   = \"Image Repo/labeled/Final Roboflow Export (841)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **unlabeled images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these images need to be labeled\n",
    "unlabeled_images = \"Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output:\n",
    "\n",
    "* **Folder** that contains: \n",
    "    - `images/`\n",
    "      + a copy of every image in **Unlabeled Data**\n",
    "    - `labels/`\n",
    "      + result of running object detection on each image\n",
    "    - a results folder produced by Ultralytic's `train.py` on the **Labeled Data** (if not using pre-trained weights)\n",
    "    - `class labels.txt` to preserve the identity of the classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seven segment digits - 1\n"
     ]
    }
   ],
   "source": [
    "from ModelAssistedLabel.config import Defaults\n",
    "import os\n",
    "\n",
    "project_name = \"seven segment digits - \"\n",
    "export_folder = Defaults._itername(project_name)\n",
    "\n",
    "print(export_folder)\n",
    "# make the export folder\n",
    "os.mkdir(export_folder)\n",
    "\n",
    "# make the images and labels subfolders\n",
    "for resource_folder in [\"images\", \"labels\"]:\n",
    "  os.mkdir(os.path.join(export_folder, resource_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of my classes are digits. Under the hood, the YOLOv5 model is working of the index of the class, rather than the human-readable name. Consequently, the identities of each class index must be supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the class labels with index 0 on line 1, index 1 on line 2, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(export_folder, \"label_map.txt\"), \"w\") as label_map:\n",
    "  label_map.writelines(\"\\n\".join(class_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several values are stored by the `Defaults` class. Any value can be overridden (and new values can be added. Make sure to `save()` any changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Defined Keys: --\n",
      "config_file\n",
      "root\n",
      "split_ratio\n",
      "data_yaml\n",
      "resource_map\n",
      "trainer_template\n"
     ]
    }
   ],
   "source": [
    "d = Defaults()\n",
    "print(\" -- Defined Keys: --\")\n",
    "print(\"\\n\".join([x for x in d.__dict__.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speciy the absolute path of the root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/vision.philbrockman.com/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.root = \"/content/drive/MyDrive/vision.philbrockman.com/ModelAssistedLabel/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save any changes and enter root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving to /content/drive/MyDrive/vision.philbrockman.com/ModelAssistedLabel/\n"
     ]
    }
   ],
   "source": [
    "d.save()\n",
    "d.to_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I borrow the instructions to set up the Ultralytics repo from [the Roboflow tutorial]( https://models.roboflow.com/object-detection/yolov5). (If I'd be allowed one undo on this project, I wish I would have intially forked this project from that tutorial.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 5532 (delta 1), reused 0 (delta 0), pack-reused 5525\n",
      "Receiving objects: 100% (5532/5532), 8.15 MiB | 7.60 MiB/s, done.\n",
      "Resolving deltas: 100% (3776/3776), done.\n",
      "/content/drive/My Drive/vision.philbrockman.com/ModelAssistedLabel/yolov5\n",
      "/content/drive/My Drive/vision.philbrockman.com/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "# clone YOLOv5 repository\n",
    "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "\n",
    "%cd yolov5\n",
    "# install dependencies as necessary\n",
    "!pip install -qr requirements.txt  # install dependencies (ignore errors)\n",
    "import torch\n",
    "\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "from utils.google_utils import gdrive_download  # to download models/datasets\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure GPU is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 1.8.0+cu101 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n",
      "moving to /content/drive/MyDrive/vision.philbrockman.com/ModelAssistedLabel/\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) ))\n",
    "  d.to_root() #step up a level\n",
    "else:\n",
    "   raise Exception(\"enable GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At some point, a model needs to be trained. The Ultralytics repo likely has more flexiblity than I'm granting, but I use custom classes to arrange my folders and files in a consistent manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Filesystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Generation` class helps convert an unordered folder of images and labels into a format compatible with YOLOv5. By default, the train/valid/test split is set to 70%/20%/10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train': 589}, {'valid': 169}, {'test': 83}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ModelAssistedLabel.fileManagement import Generation\n",
    "\n",
    "backup_dir = \"archive/Generation/zips\"\n",
    "\n",
    "g = Generation(repo=labeled_images, \n",
    "               out_dir=backup_dir,\n",
    "               verbose=True)\n",
    "\n",
    "g.set_split()\n",
    "g.get_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backups are built into this system. As an intermediate step, the split must be written to disk in `g.out_dir`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dirs ['./train', './valid', './test']\n",
      "yaml archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 05-46-03/data.yaml\n",
      "subdir train\n",
      "\toutdir archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 05-46-03\n",
      "subdir valid\n",
      "\toutdir archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 05-46-03\n",
      "subdir test\n",
      "\toutdir archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 05-46-03\n",
      "os.listdir ['train', 'valid', 'test', 'data.yaml']\n"
     ]
    }
   ],
   "source": [
    "zipped = g.write_split_to_disk(descriptor=export_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the images need to be written in a way so that the Ultralytics repository can understand their content. The `Autoweights` class both organizes data and create weights. Running an \"initialize\" command makes changes to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv 'unzipped/archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 05-46-03/train' .\n",
      "mv 'unzipped/archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 05-46-03/valid' .\n",
      "mv 'unzipped/archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 05-46-03/test' .\n",
      "mv 'unzipped/archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 05-46-03/data.yaml' .\n"
     ]
    }
   ],
   "source": [
    "from ModelAssistedLabel.train import AutoWeights\n",
    "#configure a basic AutoWeights class instance\n",
    "aw = AutoWeights(name=export_folder, out_dir=backup_dir)\n",
    "\n",
    "# create train/valid/test split from a bag of labeled images (recusively seek out images/labels)\n",
    "aw.initialize_images_from_zip(zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peep on the sizes of the train/valid/test groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/images\n",
      "\t > 589 files\n",
      "train/labels\n",
      "\t > 589 files\n",
      "valid/images\n",
      "\t > 169 files\n",
      "valid/labels\n",
      "\t > 169 files\n",
      "test/images\n",
      "\t > 83 files\n",
      "test/labels\n",
      "\t > 83 files\n",
      "File:  data.yaml\n"
     ]
    }
   ],
   "source": [
    "aw.traverse_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running `train.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the images written to disk, we can run the Ultralytics training algorithm. I loved watching the progress fly by in real time on the original `train.py`. Fortunately, the Ultralytics folk write the results file to disk so the model's training data is still accessible!\n",
    "\n",
    "> Note: this output has already been calculated and stored in `pre-trained/results` for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 10.8 s, total: 1min 14s\n",
      "Wall time: 4h 57min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov5/runs/train/seven segment digits - 1/'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "aw.generate_weights(epochs=2000, yaml_data=Defaults().trainer_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results folder is stored as an attribute as well, and it has a lot of data stored therein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('yolov5/runs/train/seven segment digits - 1/', 22)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "aw.last_results_path, len(os.listdir(aw.last_results_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the weights are stored in a subfolder called (aptly) \"weights\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last.pt', 'best.pt']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(aw.last_results_path + \"/weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the last couple lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' 1995/1999      1.8G   0.02351   0.01559  0.006725   0.04583       125       416    0.9915    0.9908    0.9929    0.8725   0.02014   0.01494  0.004582\\n',\n",
       " ' 1996/1999      1.8G   0.02363   0.01608  0.006827   0.04654       150       416    0.9915    0.9909     0.993    0.8726   0.02014   0.01495  0.004582\\n',\n",
       " ' 1997/1999      1.8G    0.0242   0.01487  0.007266   0.04633       161       416    0.9914    0.9909     0.993    0.8725   0.02014   0.01494  0.004582\\n',\n",
       " ' 1998/1999      1.8G   0.02356   0.01581  0.006952   0.04632       102       416    0.9915    0.9909     0.993    0.8726   0.02014   0.01493  0.004582\\n',\n",
       " ' 1999/1999      1.8G   0.02305   0.01591  0.006753   0.04571       185       416    0.9915    0.9909    0.9929    0.8722   0.02014   0.01492  0.004582\\n']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(aw.last_results_path + \"results.txt\") as results_file:\n",
    "  results = results_file.readlines()\n",
    "print(\"Epoch   gpu_mem       box       obj       cls     total    labels  img_size\")\n",
    "results[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine-assisted Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling a new set of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `Viewer` class doesn't care how recently your weights were generated so you can plug in existing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "from ModelAssistedLabel.detect import Viewer\n",
    "\n",
    "# access the folder of results from the AutoWeights instance\n",
    "results_folder = aw.last_results_path\n",
    "\n",
    "# I'm choosing to use the best weight.\n",
    "weight_path = results_folder + \"/weights/best.pt\"\n",
    "\n",
    "# Viewer needs a set of weights and an array of labels for the detected object types\n",
    "v = Viewer(weight_path, class_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selects all images in the unlabeled folder and let's us look through the computer's eyes at the images.\n",
    "\n",
    "Note that the model has trouble in images with glare on the LCD. For more clear screens, instead set `unlabeled_images=\"21-3-22 rowing (200) 7:50-12:50\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Output hidden; open in https://colab.research.google.com to view."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import random, glob\n",
    "\n",
    "images = glob.glob(f\"./{unlabeled_images}/*.jpg\")\n",
    "\n",
    "for image in random.sample(images,5):\n",
    "  v.plot_for(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for image in images:\n",
    "  results.append(v.predict_for(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting annotated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that image/label pairs have a common root filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving yolov5 results folder: yolov5/runs/train/seven segment digits - 1/\n"
     ]
    }
   ],
   "source": [
    "import random, PIL, shutil\n",
    "salt = lambda: str(random.random())[2:]\n",
    "\n",
    "for result in results:\n",
    "  #generate a likely-to-be-unique filename\n",
    "  shared_root = Defaults._itername(f\"{project_name}-{salt()}\")\n",
    "\n",
    "  #save the image to the outfile\n",
    "  image = PIL.Image.open(result[\"image path\"])\n",
    "  image.save(os.path.join(export_folder, \"images\", f\"{shared_root}.jpg\"))\n",
    "\n",
    "  #save the predictions to the outfile\n",
    "  predictions = result[\"predictions\"]\n",
    "  with open(os.path.join(export_folder, \"labels\", f\"{shared_root}.txt\"), \"w\") as prediction_file:\n",
    "    prediction_file.writelines(\"\\n\".join([x[\"yolov5 format\"] for x in predictions]))\n",
    "\n",
    "#check if weights were generated\n",
    "if aw is not None and os.path.exists(aw.last_results_path):\n",
    "  print(f\"Moving yolov5 results folder: {aw.last_results_path}\")\n",
    "  shutil.move(aw.last_results_path, export_folder)\n",
    "else:\n",
    "  print(\"No weights to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are ready for human verification. As the model grows more accurate, I would alter camera position or lighting until the model starts to stumble again. \n",
    "\n",
    "I labeled dozens upon dozens and dozens of images with Roboflow and would recommend their free annotation service! However, to be transparent, I developed [an annotator](https://github.com/PhilBrockman/autobbox) in React that better suited my physical needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "My original goal of \"smartifying\" my rowing machine is closer than ever. \n",
    "\n",
    "It is possible to parse workout information (thought currently, I only have access to a maximum of 4 digits). I wonder if the model could keep up if there were 20+ digits to capture.\n",
    "\n",
    "I know that lighting and camera position have an effect on accuracy. Here's how I'm holding my computer steady as I modify the lighting: [standing](https://raw.githubusercontent.com/PhilBrockman/ModelAssistedLabel/master/DIY-laptop-mount.jpg), [floor 1](https://raw.githubusercontent.com/PhilBrockman/ModelAssistedLabel/master/DIY-computer-capture.jpg), [floor 2](https://github.com/PhilBrockman/ModelAssistedLabel/blob/master/DIY-capture.jpeg?raw=true).\n",
    "\n",
    "Here are 3 runs captured under different lighting conditions:\n",
    "* `21-3-22 rowing (200) 7:50-12:50` (direct lighting from one light source)\n",
    "* `21-3-22 rowing (200) 1:53-7:00` (direct lighting from one light source with glare)\n",
    "* `21-3-18 rowing 8-12 ` (direct light and ambient lamps turned on)\n",
    "\n",
    "> Note:  All unlabeled images were taken inside a blacked-out room. The are stored in `Image Repo/unlabeled/`\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lingering Questions\n",
    "\n",
    "My dataset of 841 images is eclectic. There's images from other rowing machines and others from [a kind stranger's github repo](https://github.com/SachaIZADI/Seven-Segment-OCR). Some images have been cropped to only include the display. Did having varied data slow me down overall? Or did it make the models more robust? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
