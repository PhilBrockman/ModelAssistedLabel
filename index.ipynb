{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-assisted Labeling with YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![base64 splash](https://github.com/PhilBrockman/ModelAssistedLabel/blob/master/modelassistedlabel%20splash.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My exercise equipment doesn’t connect to a network. But I still want the \"smart workout experience\" when I'm using a \"dumb\" rowing machine.\n",
    "\n",
    "If I maybe point my webcam at the equipment’s LCD output, I can make my computer interpret the digits. Perfect! I’ll just utilize object detection to determine the location and identity of digits on the machine’s readout. \n",
    "\n",
    "First question -- just a tiny one -- how do you do that?  \n",
    "\n",
    "After wading through several guides, I found [Roboflow's YOLOv5 tutorial]( https://models.roboflow.com/object-detection/yolov5). They provide a hands-on and accessible experience in machine learning. But unfortunately, I couldn't progress immediately on my specific project. Instead, I had to build my own dataset.\n",
    "\n",
    "As I labeled digits on image after image, tedium tore at me. I envisioned using the YOLOv5 model predictions as \"pre-labels\". I sleuth through [Ultralytic's](https://github.com/ultralytics/yolov5) original project, and I build wrappers around `detect.py` and `train.py`. I determine that my vision could be a reality.\n",
    "\n",
    "This repository contains the tools that let me \"pre-label\" my images before sending them off for human inspection and correction. I also provide labeled and unlabeled images to demonstrate the tools.\n",
    "\n",
    "* `Image Repo/`\n",
    " - `labeled/`\n",
    "    + `Final Roboflow Export (841)/` \n",
    "      + 841 labeled image dataset\n",
    " - `unlabeled/`\n",
    "    + `21-3-22 rowing (200) 7:50-12:50/`\n",
    "      - 200 images with direct lighting from one light source\n",
    "    + `21-3-22 rowing (200) 1:53-7:00/` \n",
    "      - 200 images with direct lighting from one light source and intermittent glare\n",
    "    + `21-3-18 rowing 8-12/` \n",
    "      - 200 images with direct light and ambient lamps turned on\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: [Open In Colab](https://colab.research.google.com/github/PhilBrockman/ModelAssistedLabel/blob/master/index.ipynb) (and enable GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"seven segment digits - \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I back up the input files after writing them to disk just in case something breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_dir = \"archive/Generation/zips\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ModelAssistedLabel'...\n",
      "remote: Enumerating objects: 1023, done.\u001b[K\n",
      "remote: Counting objects: 100% (1023/1023), done.\u001b[K\n",
      "remote: Compressing objects: 100% (626/626), done.\u001b[K\n",
      "remote: Total 5000 (delta 455), reused 907 (delta 359), pack-reused 3977\u001b[K\n",
      "Receiving objects: 100% (5000/5000), 252.34 MiB | 32.54 MiB/s, done.\n",
      "Resolving deltas: 100% (1478/1478), done.\n",
      "Checking out files: 100% (2807/2807), done.\n",
      "/content/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/PhilBrockman/ModelAssistedLabel.git\n",
    "%cd \"ModelAssistedLabel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Inputs\n",
    "\n",
    "-  **labeled images**\n",
    "    + All of the images and labels must be in a common folder (subfolders allowed).\n",
    "    + Labels must be in [YOLOv5 format](https://github.com/AlexeyAB/Yolo_mark/issues/60#issuecomment-401854885).\n",
    "\n",
    "    > Note: Image/label pairs are based on their base filename. For example `image.jpg/image.txt` would be paired as would `other_image5.jpg/other_image5.txt`.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these images have already been labeled\n",
    "labeled_images = \"Image Repo/labeled/Final Roboflow Export (841)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **unlabeled images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these images need to be annotated\n",
    "unlabeled_images = \"Image Repo/unlabeled/21-3-22 rowing (200) 7:50-12:50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "* Folder that contains: \n",
    "    - `images/`\n",
    "      + a copy of every image in **Unlabeled Data**\n",
    "    - `labels/`\n",
    "      + result of running object detection on each image\n",
    "    - `class labels.txt` to preserve the identity of the classes\n",
    "    - (if not using pre-trained weights) a results folder produced by Ultralytic's `train.py` on the **Labeled Data** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by building the folder structure for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seven segment digits - 1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ModelAssistedLabel.config import Defaults\n",
    "import os\n",
    "\n",
    "def mk_MAL_dirs(root_project_name):\n",
    "  \"Builds the expected output folder for a given project\"\n",
    "  export_folder = Defaults._itername(root_project_name)\n",
    "\n",
    "  # make the export folder\n",
    "  os.mkdir(export_folder)\n",
    "\n",
    "  # make the images and labels subfolders\n",
    "  for resource_folder in [\"images\", \"labels\"]:\n",
    "    os.mkdir(os.path.join(export_folder, resource_folder))\n",
    "\n",
    "  return export_folder\n",
    "\n",
    "export_folder = mk_MAL_dirs(project_name)\n",
    "export_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several values are stored by the `Defaults` class. Any value can be overridden (and new values can be added. Make sure to `save()` any changes! (changes are written to the `config_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### changing a Default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Defined Keys: --\n",
      "config_file\n",
      "root\n",
      "split_ratio\n",
      "data_yaml\n",
      "resource_map\n",
      "trainer_template\n"
     ]
    }
   ],
   "source": [
    "print(\" -- Defined Keys: --\")\n",
    "print(\"\\n\".join([x for x in d.__dict__.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speciy the absolute path of the root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.root = \"/content/ModelAssistedLabel/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save changes and enter root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving to /content/ModelAssistedLabel/\n"
     ]
    }
   ],
   "source": [
    "d.save()\n",
    "d.to_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cloning YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 26, done.\u001b[K\n",
      "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
      "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
      "remote: Total 5595 (delta 7), reused 10 (delta 1), pack-reused 5569\u001b[K\n",
      "Receiving objects: 100% (5595/5595), 8.46 MiB | 33.32 MiB/s, done.\n",
      "Resolving deltas: 100% (3815/3815), done.\n",
      "/content/ModelAssistedLabel/yolov5\n",
      "\u001b[K     |████████████████████████████████| 645kB 8.0MB/s \n",
      "\u001b[?25h/content/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "# clone YOLOv5 repository\n",
    "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "\n",
    "%cd yolov5\n",
    "# install dependencies as necessary\n",
    "!pip install -qr requirements.txt  # install dependencies (ignore errors)\n",
    "import torch\n",
    "\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "from utils.google_utils import gdrive_download  # to download models/datasets\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define class map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(`data_yaml` value was pulled from Roboflow tutorial)\n",
    "\n",
    "I have `nc = 10` classes and their `names` are all string types. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.data_yaml = \"\"\"train: ../train/images\n",
    "val: ../valid/images\n",
    "\n",
    "nc: 10\n",
    "names: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\"\"\"\n",
    "\n",
    "d.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the class names are accessible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.get_class_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the class labels with index 0 on line 1, index 1 on line 2, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(export_folder, \"label_map.txt\"), \"w\") as label_map:\n",
    "  label_map.writelines(\"\\n\".join(d.get_class_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure GPU is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 1.8.0+cu101 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n",
      "moving to /content/drive/MyDrive/vision.philbrockman.com/ModelAssistedLabel/\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) ))\n",
    "  d.to_root() #step up a level\n",
    "else:\n",
    "   raise Exception(\"enable GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Filesystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Generation` class helps convert an unordered folder of images and labels into a format compatible with YOLOv5. By default, the train/valid/test split is set to 70%/20%/10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train': 589}, {'valid': 169}, {'test': 83}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ModelAssistedLabel.fileManagement import Generation\n",
    "\n",
    "g = Generation(repo=labeled_images, \n",
    "               out_dir=backup_dir,\n",
    "               verbose=True)\n",
    "\n",
    "g.set_split()\n",
    "g.get_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backups are built into this system. As an intermediate step, the split must be written to disk in `g.out_dir`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dirs ['./train', './valid', './test']\n",
      "yaml archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-29 17-47-41/data.yaml\n",
      "subdir train\n",
      "\toutdir archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-29 17-47-41\n",
      "subdir valid\n",
      "\toutdir archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-29 17-47-41\n",
      "subdir test\n",
      "\toutdir archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-29 17-47-41\n",
      "os.listdir ['test', 'data.yaml', 'valid', 'train']\n"
     ]
    }
   ],
   "source": [
    "zipped = g.write_split_to_disk(descriptor=export_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the images need to be written in a way so that the Ultralytics repository can understand their content. The `Autoweights` class both organizes data and create weights. Running an \"initialize\" command makes changes to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv 'unzipped/archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-29 17-47-41/test' .\n",
      "mv 'unzipped/archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-29 17-47-41/data.yaml' .\n",
      "mv 'unzipped/archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-29 17-47-41/valid' .\n",
      "mv 'unzipped/archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-29 17-47-41/train' .\n"
     ]
    }
   ],
   "source": [
    "from ModelAssistedLabel.train import AutoWeights\n",
    "#configure a basic AutoWeights class instance\n",
    "aw = AutoWeights(name=export_folder, out_dir=backup_dir)\n",
    "\n",
    "# create train/valid/test split from a bag of labeled images (recusively seek out images/labels)\n",
    "aw.initialize_images_from_zip(zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peep on the sizes of the train/valid/test groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/labels\n",
      "\t > 83 files\n",
      "test/images\n",
      "\t > 83 files\n",
      "File:  data.yaml\n",
      "valid/labels\n",
      "\t > 169 files\n",
      "valid/images\n",
      "\t > 169 files\n",
      "train/labels\n",
      "\t > 589 files\n",
      "train/images\n",
      "\t > 589 files\n"
     ]
    }
   ],
   "source": [
    "aw.traverse_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running `train.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the images written to disk, we can run the Ultralytics training algorithm. I loved watching the progress fly by in real time on the original `train.py`. Fortunately, the Ultralytics folk write the results file to disk so the model's training data is still accessible!\n",
    "\n",
    "> Note: this output has already been calculated and stored in `pre-trained/results` for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 10.8 s, total: 1min 14s\n",
      "Wall time: 4h 57min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov5/runs/train/seven segment digits - 1/'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "aw.generate_weights(epochs=2000, yaml_data=Defaults().trainer_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results folder is stored as an attribute as well, and it has a lot of data stored therein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('yolov5/runs/train/seven segment digits - 1/', 22)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "aw.last_results_path, len(os.listdir(aw.last_results_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the weights are stored in a subfolder called (aptly) \"weights\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last.pt', 'best.pt']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(aw.last_results_path + \"/weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine-assisted Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling a New Set of Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Viewer` class needs weights and class labels to operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "from ModelAssistedLabel.detect import Viewer\n",
    "\n",
    "# access the folder of results from the AutoWeights instance\n",
    "results_folder = aw.last_results_path\n",
    "\n",
    "# I'm choosing to use the best weight.\n",
    "weight_path = results_folder + \"/weights/best.pt\"\n",
    "\n",
    "# Viewer needs a set of weights and an array of labels for the detected object types\n",
    "v = Viewer(weight_path, Defaults().get_class_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's us look through the computer's eyes at the images.\n",
    "\n",
    "With the existing dataset, the model performs best under direct lighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Output hidden; open in https://colab.research.google.com to view."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import random, glob\n",
    "\n",
    "images = glob.glob(f\"./{unlabeled_images}/*.jpg\")\n",
    "\n",
    "for image in random.sample(images,5):\n",
    "  print(image)\n",
    "  v.plot_for(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this small sample, the model performs quite adequately. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predictions spot-check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that every image in the current batch should have exactly 4 image predictions, lets take a look at the numbers of predictions being made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axis.Ticker at 0x7f8b7d53b150>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = pd.Series([len(x[\"predictions\"]) for x in results]).hist()\n",
    "ax.grid(False)\n",
    "ax.set_title(label = \"Number of predictions per image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage in development, the YOLOv5 model could still be prone to false positives. However, I interpret the above data to suggest that there are overlapping bounding boxes that need to be resolved.\n",
    "\n",
    "There are a couple of possible entry points to a programitic solution: the `Detector` class has an `iou_threshold` attribute (read about [Intersection Over Union](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)) that can be fiddled with. \n",
    "\n",
    "Further, the `results` array stores the confidence of predictions - confidence may play a role in detecting extraneous predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Annotated Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that image/label pairs have a common root filename and collect relevant files in a single folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, PIL, shutil\n",
    "salt = lambda: str(random.random())[2:]\n",
    "\n",
    "for result in results:\n",
    "  #generate a likely-to-be-unique filename\n",
    "  shared_root = Defaults._itername(f\"{project_name}-{salt()}\")\n",
    "\n",
    "  #save the image to the outfile\n",
    "  image = PIL.Image.open(result[\"image path\"])\n",
    "  image.save(os.path.join(export_folder, \"images\", f\"{shared_root}.jpg\"))\n",
    "\n",
    "  #save the predictions to the outfile\n",
    "  predictions = result[\"predictions\"]\n",
    "  with open(os.path.join(export_folder, \"labels\", f\"{shared_root}.txt\"), \"w\") as prediction_file:\n",
    "    prediction_file.writelines(\"\\n\".join([x[\"yolov5 format\"] for x in predictions]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, if results need to get saved, make sure they get saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving yolov5 results: yolov5/runs/train/seven segment digits - 1/\n"
     ]
    }
   ],
   "source": [
    "moved = False #set a flag\n",
    "\n",
    "try: \n",
    "  if os.path.exists(aw.last_results_path):\n",
    "    # `aw` exists and it has been executed \n",
    "    print(f\"Moving yolov5 results: {aw.last_results_path}\")\n",
    "    shutil.move(aw.last_results_path, export_folder)\n",
    "\n",
    "    #flip the flag\n",
    "    moved = True\n",
    "except NameError:\n",
    "  pass\n",
    "\n",
    "if not(moved):\n",
    "  # either the AutoWeigts didn't pan out or it wasn't used\n",
    "  print(\"No weights to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seven segment digits - 1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I labeled dozens upon dozens and dozens of images with Roboflow and would recommend their free annotation service! However, to be transparent, I developed [an annotator](https://github.com/PhilBrockman/autobbox) in React that better suited my physical needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building up an Image Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind the `Generation` class recursively finds all images and lables. So as long as the newly annotated images and the original Image Set are in the same folder, `Generation`'s constructor function will find them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up\n",
    "\n",
    "I have uncovered a camera and lighting positioning that allows for my model to read the LCD at with high fidelty. I'm using object detection as a form of OCR and it's working!\n",
    "\n",
    "I see three main areas for development with this project. The first would be bolstering the dataset (and staying in the machine learning space). The second would be logic interpreting parsed data (building the \"smart\" software).\n",
    "\n",
    "The third area of development is refactoring. I made a decision early on to hard to hardcode the path to training and validation images. It's worth revisiting the way `Defaults.data_yaml` is constructed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on the Image Set\n",
    "\n",
    "This dataset of 841 images is a mishmash. There's images from a different rowing machine and also from  [this](https://github.com/SachaIZADI/Seven-Segment-OCR) repo. Some scenes are illuminated with sunlight. Others have been cropped to include only the LCD. Digits like 7, 8, and 9 are underrepresented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recording from Laptop\n",
    "\n",
    "This is my setup for how I'm currently fixing the position my black-shelled laptop while recording: [pic1](https://raw.githubusercontent.com/PhilBrockman/ModelAssistedLabel/master/DIY-laptop-mount.jpg), [pic2](https://raw.githubusercontent.com/PhilBrockman/ModelAssistedLabel/master/DIY-computer-capture.jpg), [pic3](https://github.com/PhilBrockman/ModelAssistedLabel/blob/master/DIY-capture.jpeg?raw=true).\n",
    "\n",
    "I use the `_capture.ipynb` notebook to capture images on a bit of a delay to prevent repeat images from cluttering the dataset. For me, it was much easier to get recording working from a local notebook than from a Colab notebook but YMMV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Me\n",
    "\n",
    "I am finishing my fourth year as a public school teacher in Kentucky. This summer, I am moving to the Bay Area to pursue a career in tech. When I’m not coding, I enjoy playing violin! Reach me at phil.brockman@gmail.com.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
