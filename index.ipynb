{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-asisted Labeling with YOLOv5\n",
    "> custom image set annotation with a model's help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![base64 splash](https://github.com/PhilBrockman/ModelAssistedLabel/blob/master/modelassistedlabel%20splash.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My exercise equipment, despite even being electronic, doesn’t connect to a network.\n",
    "\n",
    "But if I instead point my webcam at the equipment’s LCD output, I can make a machine learn to identify and interpret useful information. Perfect! I’ll just utilize object detection to determine the location and identity of the machine’s analog readout. \n",
    "\n",
    "First question, just a tiny one, how do you do that?  \n",
    "\n",
    "After wading through several guides, I found [Roboflow's YOLOv5 tutorial]( https://models.roboflow.com/object-detection/yolov5). They helped provide a hands-on and accessible experience in machine learning.\n",
    "\n",
    "Unfortunately, I didn't have much luck with existing models being able to readily parse digits. Instead, I decided to start building my own dataset.\n",
    "\n",
    "I shouldn't have been caught off-guard by the tedium of manually annotating images. As my mind starts to drift, I wonder if I’m a reCAPTCHA interface that’s gained sentience, and I break through. If I label enough digits, I can train a YOLO model to tell me what it sees. I can then take that information and pre-label my images with those predictions. \n",
    "\n",
    "The pieces come together.  I can focus on writing code while I use Roboflow to sort, generate, and deliver my images. I sleuth through [Ultralytic's](https://github.com/ultralytics/yolov5) original project and build wrappers around the essential functions in `detect.py` and `train.py`.\n",
    "\n",
    "This repository contains the tools that let me \"pre-label\" my images before sending them off for human inspection and correction.\n",
    "\n",
    "I use the `Viewer` class to \n",
    "\n",
    "\n",
    "\n",
    "> Note: In `./Image Repo` I provide access to 841 labeled images (lumped in one folder) and 600 unlabeled images (seperated into three sets of 200 images - lighting condition is the same within each run, but differs between runs). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: [Open In Colab](https://colab.research.google.com/github/PhilBrockman/ModelAssistedLabel/blob/master/index.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ModelAssistedLabel'...\n",
      "remote: Enumerating objects: 424, done.\u001b[K\n",
      "remote: Counting objects: 100% (424/424), done.\u001b[K\n",
      "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
      "remote: Total 4401 (delta 305), reused 414 (delta 299), pack-reused 3977\u001b[K\n",
      "Receiving objects: 100% (4401/4401), 207.35 MiB | 14.02 MiB/s, done.\n",
      "Resolving deltas: 100% (1328/1328), done.\n",
      "Checking out files: 100% (2375/2375), done.\n",
      "/content/drive/My Drive/Coding/vision.philbrockman.com/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "# clone this repository\n",
    "!git clone -b future_forward https://github.com/PhilBrockman/ModelAssistedLabel.git\n",
    "%cd \"ModelAssistedLabel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Inputs:\n",
    "\n",
    "-  **labeled images**\n",
    "    + All of the images and labels must be in a common folder (subfolders allowed).\n",
    "    + Labels must be in [YOLOv5 format](https://github.com/AlexeyAB/Yolo_mark/issues/60#issuecomment-401854885).\n",
    "\n",
    "    > Note: Image/label pairs are based on their base filename. For example `image.jpg/image.txt` would be paired as would `other_image5.jpg/other_image5.txt`.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these images have already been labeled\n",
    "labeled_images   = \"Image Repo/labeled/Final Roboflow Export (841)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **unlabeled images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these images need to be labeled\n",
    "unlabeled_images = \"Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output:\n",
    "\n",
    "* **Folder** that contains: \n",
    "    - `images/`\n",
    "      + a copy of every image in **Unlabeled Data**\n",
    "    - `labels/`\n",
    "      + result of running object detection on each image\n",
    "    - a results folder produced by Ultralytic's `train.py` on the **Labeled Data** (if not using pre-trained weights)\n",
    "    - `class labels.txt` to preserve the identity of the classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seven segment digits - 1\n"
     ]
    }
   ],
   "source": [
    "from ModelAssistedLabel.config import Defaults\n",
    "import os\n",
    "\n",
    "project_name = \"seven segment digits - \"\n",
    "export_folder = Defaults._itername(project_name)\n",
    "\n",
    "print(export_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the export folder\n",
    "os.mkdir(export_folder)\n",
    "\n",
    "# make the images and labels subfolders\n",
    "for resource_folder in [\"images\", \"labels\"]:\n",
    "  os.mkdir(os.path.join(export_folder, resource_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several values are stored by the `Defaults` class. Any value can be overridden (and new values can be added. Make sure to `save()` any changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Defined Keys: --\n",
      "config_file\n",
      "root\n",
      "split_ratio\n",
      "data_yaml\n",
      "resource_map\n",
      "trainer_template\n"
     ]
    }
   ],
   "source": [
    "d = Defaults()\n",
    "print(\" -- Defined Keys: --\")\n",
    "print(\"\\n\".join([x for x in d.__dict__.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speciy the absolute path of the root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Coding/vision.philbrockman.com/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.root = \"/content/drive/MyDrive/vision.philbrockman.com/ModelAssistedLabel/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save any changes and enter root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving to /content/drive/MyDrive/vision.philbrockman.com/ModelAssistedLabel/\n"
     ]
    }
   ],
   "source": [
    "d.save()\n",
    "d.to_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I borrow the instructions to set up the Ultralytics repo from [the Roboflow tutorial]( https://models.roboflow.com/object-detection/yolov5). (If I'd be allowed one undo on this project, I wish I would have intially forked this project from that tutorial.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 5532 (delta 1), reused 0 (delta 0), pack-reused 5525\u001b[K\n",
      "Receiving objects: 100% (5532/5532), 8.15 MiB | 8.33 MiB/s, done.\n",
      "Resolving deltas: 100% (3776/3776), done.\n",
      "/content/drive/My Drive/vision.philbrockman.com/ModelAssistedLabel/yolov5\n",
      "/content/drive/My Drive/vision.philbrockman.com/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "# clone YOLOv5 repository\n",
    "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "\n",
    "%cd yolov5\n",
    "# install dependencies as necessary\n",
    "!pip install -qr requirements.txt  # install dependencies (ignore errors)\n",
    "import torch\n",
    "\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "from utils.google_utils import gdrive_download  # to download models/datasets\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure GPU is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 1.8.0+cu101 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n",
      "moving to /content/drive/MyDrive/vision.philbrockman.com/ModelAssistedLabel/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of utils failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ModuleNotFoundError: spec not found for the module 'utils'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) ))\n",
    "  d.to_root() #step up a level\n",
    "else:\n",
    "   raise Exception(\"enable GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of my classes are digits. Under the hood, the YOLOv5 model is working of the index of the class, rather than the human-readable name. Consequently, the identities of each class index must be supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train': 589}, {'valid': 169}, {'test': 83}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ModelAssistedLabel.fileManagement import Generation\n",
    "\n",
    "backup_dir = \"archive/Generation/zips\"\n",
    "\n",
    "g = Generation(repo=labeled_images, \n",
    "               out_dir=backup_dir,\n",
    "               verbose=True)\n",
    "\n",
    "g.set_split()\n",
    "g.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dirs ['./train', './valid', './test']\n",
      "yaml archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 03-56-37/data.yaml\n",
      "subdir train\n",
      "\toutdir archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 03-56-37\n",
      "subdir valid\n",
      "\toutdir archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 03-56-37\n",
      "subdir test\n",
      "\toutdir archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 03-56-37\n",
      "os.listdir ['train', 'valid', 'test', 'data.yaml']\n"
     ]
    }
   ],
   "source": [
    "zipped = g.write_split_to_disk(descriptor=export_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the images need to be written in a way so that the Ultralytics repository can understand their content. The `Autoweights` class both organizes data and create weights. Running an \"initialize\" command makes changes to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv 'unzipped/archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 03-56-37/train' .\n",
      "mv 'unzipped/archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 03-56-37/valid' .\n",
      "mv 'unzipped/archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 03-56-37/test' .\n",
      "mv 'unzipped/archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 03-56-37/data.yaml' .\n"
     ]
    }
   ],
   "source": [
    "from ModelAssistedLabel.train import AutoWeights\n",
    "#configure a basic AutoWeights class instance\n",
    "aw = AutoWeights(name=export_folder, out_dir=backup_dir)\n",
    "\n",
    "# create train/valid/test split from a bag of labeled images (recusively seek out images/labels)\n",
    "aw.initialize_images_from_zip(zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peep on the sizes of the train/valid/test groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/images\n",
      "\t > 769 files\n",
      "train/labels\n",
      "\t > 769 files\n",
      "valid/images\n",
      "\t > 305 files\n",
      "valid/labels\n",
      "\t > 305 files\n",
      "test/images\n",
      "\t > 159 files\n",
      "test/labels\n",
      "\t > 159 files\n",
      "File:  data.yaml\n"
     ]
    }
   ],
   "source": [
    "aw.traverse_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the images written to disk, we can run the Ultralytics training algorithm. I loved watching the progress fly by in real time on the original `train.py`. Fortunately, the Ultralytics folk write the results file to disk so the model's training data is still accessible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelAssistedLabel.train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-044047f7d798>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\naw.generate_weights(epochs=2000, data_yaml=d.trainer_template)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_weights() got an unexpected keyword argument 'data_yaml'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "aw.generate_weights(epochs=2000, data_yaml=d.trainer_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results folder is stored as an attribute as well, and it has a lot of data stored therein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "aw.last_results_path, len(os.listdir(aw.last_results_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the weights are stored in a subfolder called (aptly) \"weights\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(aw.last_results_path + \"/weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the last couple lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(aw.last_results_path + \"results.txt\") as results_file:\n",
    "  results = results_file.readlines()\n",
    "print(\"Epoch   gpu_mem       box       obj       cls     total    labels  img_size\")\n",
    "results[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling a new set of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `Viewer` class doesn't care how recently your weights were generated so you can plug in existing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelAssistedLabel.detect import Viewer\n",
    "\n",
    "# access the folder of results from the AutoWeights instance\n",
    "results_folder = aw.last_results_path\n",
    "\n",
    "# I'm choosing to use the best weight.\n",
    "weight_path = results_folder + \"/weights/best.pt\"\n",
    "\n",
    "# Viewer needs a set of weights and an array of labels for the detected object types\n",
    "v = Viewer(weight_path, class_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selects all images in the unlabeled folder and let's us look through the computer's eyes at the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import random, glob\n",
    "\n",
    "images = glob.glob(f\"./{unlabeled_images}/*.jpg\")\n",
    "\n",
    "for image in random.sample(images,5):\n",
    "  v.plot_for(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for image in images:\n",
    "  results.append(v.predict_for(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting annotated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the class labels with index 0 on line 1, index 1 on line 2, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(export_folder, \"label_map.txt\"), \"w\") as label_map:\n",
    "  label_map.writelines(\"\\n\".join(class_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that image/label pairs have a common root filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, PIL, shutil\n",
    "salt = lambda: str(random.random())[2:]\n",
    "\n",
    "for result in results:\n",
    "  #generate a likely-to-be-unique filename\n",
    "  shared_root = Defaults._itername(f\"{project_name}-{salt()}\")\n",
    "\n",
    "  #save the image to the outfile\n",
    "  image = PIL.Image.open(result[\"image path\"])\n",
    "  image.save(os.path.join(export_folder, \"images\", f\"{shared_root}.jpg\"))\n",
    "\n",
    "  #save the predictions to the outfile\n",
    "  predictions = result[\"predictions\"]\n",
    "  with open(os.path.join(export_folder, \"labels\", f\"{shared_root}.txt\"), \"w\") as prediction_file:\n",
    "    prediction_file.writelines(\"\\n\".join([x[\"yolov5 format\"] for x in predictions]))\n",
    "\n",
    "#check if weights were generated\n",
    "if aw is not None and os.path.exists(aw.last_results_path):\n",
    "  print(f\"Moving yolov5 results folder: {aw.last_results_path}\")\n",
    "  shutil.move(aw.last_results_path, export_folder)\n",
    "else:\n",
    "  print(\"No weights to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I would have uploaded this set of image/label pairs to Roboflow for correction and annotation. As the model grows more accurate, I would alter camera position or lighting until the model started stumbling again. I want to be keeping the model on its toes!\n",
    "\n",
    "To be transparent, I developed a [custom React annotator](https://github.com/PhilBrockman/autobbox) that better suited my needs.\n",
    "\n",
    "I labeled dozens upon dozens and dozens of images with Roboflow and would recommend their free annotation service! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "My original goal of \"smartifying\" my rowing machine is closer than ever. \n",
    "\n",
    "It is possible to parse workout information (thought currently, I only have access to a maximum of 4 digits). I wonder if the model could keep up if there were 20+ digits to capture.\n",
    "\n",
    "I know that lighting and camera position have an effect on accuracy. Here's how I'm holding my computer steady as I modify the lighting: [standing](https://raw.githubusercontent.com/PhilBrockman/ModelAssistedLabel/master/DIY-laptop-mount.jpg), [floor 1](https://raw.githubusercontent.com/PhilBrockman/ModelAssistedLabel/master/DIY-computer-capture.jpg), [floor 2](https://github.com/PhilBrockman/ModelAssistedLabel/blob/master/DIY-capture.jpeg?raw=true).\n",
    "\n",
    "Here are 3 runs captured under different lighting conditions:\n",
    "* `21-3-22 rowing (200) 7:50-12:50` (direct lighting from one light source)\n",
    "* `21-3-22 rowing (200) 1:53-7:00` (direct lighting from one light source with glare)\n",
    "* `21-3-18 rowing 8-12 ` (direct light and ambient lamps turned on)\n",
    "\n",
    "> Note:  All unlabeled images were taken inside a blacked-out room. The are stored in `Image Repo/unlabeled/`\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lingering Questions\n",
    "\n",
    "My labeled images are disorderly. There's data from other rowing machines and from [a kind *stranger*'s github repo](https://github.com/SachaIZADI/Seven-Segment-OCR). Some images have been cropped to only include the display. Did having varied data slow me down overall? Or did it make the models more robust? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
