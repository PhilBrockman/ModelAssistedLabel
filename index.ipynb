{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-asisted Labeling with YOLOv5\n",
    "> custom image set annotation with a model's help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![base64 splash](https://github.com/PhilBrockman/ModelAssistedLabel/blob/master/modelassistedlabel%20splash.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My exercise equipment, despite even being electronic, doesn’t connect to a network.\n",
    "\n",
    "But if I instead point my webcam at the equipment’s LCD output, I can make a machine learn to identify and interpret useful information. Perfect! I’ll just utilize object detection to determine the location and identity of the machine’s analog readout. \n",
    "\n",
    "First question, just a tiny one, how do you do that?  \n",
    "\n",
    "After wading through several guides, I found [Roboflow's YOLOv5 tutorial]( https://models.roboflow.com/object-detection/yolov5). They helped provide a hands-on and accessible experience in machine learning.\n",
    "\n",
    "Unfortunately, I didn't have much luck with existing models being able to readily parse digits. Instead, I decided to start building my own dataset.\n",
    "\n",
    "I shouldn't have been caught off-guard by the tedium of manually annotating images. As my mind starts to drift, I wonder if I’m a reCAPTCHA interface that’s gained sentience, and I break through. If I label enough digits, I can train a YOLO model to tell me what it sees. I can then take that information and pre-label my images with those predictions. \n",
    "\n",
    "The pieces come together.  I can focus on writing code while I use Roboflow to sort, generate, and deliver my images. I sleuth through [Ultralytic's](https://github.com/ultralytics/yolov5) original project and build wrappers around the essential functions in `detect.py` and `train.py`.\n",
    "\n",
    "This repository contains the tools that let me \"pre-label\" my images before sending them off for human inspection and correction.\n",
    "\n",
    "I use the `Viewer` class to \n",
    "\n",
    "\n",
    "\n",
    "> Note: In `./Image Repo` I provide access to 841 labeled images (lumped in one folder) and 600 unlabeled images (seperated into three sets of 200 images - lighting condition is the same within each run, but differs between runs). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: [Open In Colab](https://colab.research.google.com/github/PhilBrockman/ModelAssistedLabel/blob/master/index.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ModelAssistedLabel'...\n",
      "remote: Enumerating objects: 25, done.\u001b[K\n",
      "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
      "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
      "remote: Total 4395 (delta 14), reused 11 (delta 6), pack-reused 4370\u001b[K\n",
      "Receiving objects: 100% (4395/4395), 205.74 MiB | 15.24 MiB/s, done.\n",
      "Resolving deltas: 100% (1339/1339), done.\n",
      "Checking out files: 100% (2375/2375), done.\n",
      "/content/drive/My Drive/vision.philbrockman.com/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "# clone this repository\n",
    "!git clone -b future_forward https://github.com/PhilBrockman/ModelAssistedLabel.git\n",
    "%cd \"ModelAssistedLabel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Inputs:\n",
    "\n",
    "-  **labeled images**\n",
    "    + All of the images and labels must be in a common folder (subfolders allowed).\n",
    "    + Labels must be in [YOLOv5 format](https://github.com/AlexeyAB/Yolo_mark/issues/60#issuecomment-401854885).\n",
    "\n",
    "    > Note: Image/label pairs are based on their base filename. For example `image.jpg/image.txt` would be paired as would `other_image5.jpg/other_image5.txt`.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these images have already been labeled\n",
    "labeled_images   = \"Image Repo/labeled/Final Roboflow Export (841)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - **unlabeled images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these images need to be labeled\n",
    "unlabeled_images = \"Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output:\n",
    "\n",
    "* **Folder** that contains: \n",
    "    - `images/`\n",
    "      + a copy of every image in **Unlabeled Data**\n",
    "    - `labels/`\n",
    "      + result of running object detection on each image\n",
    "    - a results folder produced by Ultralytic's `train.py` on the **Labeled Data** (if not using pre-trained weights)\n",
    "    - `class labels.txt` to preserve the identity of the classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seven segment digits - 1\n"
     ]
    }
   ],
   "source": [
    "from ModelAssistedLabel.config import Defaults\n",
    "import os\n",
    "\n",
    "project_name = \"seven segment digits - \"\n",
    "export_folder = Defaults._itername(project_name)\n",
    "\n",
    "print(export_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the export folder\n",
    "os.mkdir(export_folder)\n",
    "\n",
    "# make the images and labels subfolders\n",
    "for resource_folder in [\"images\", \"labels\"]:\n",
    "  os.mkdir(os.path.join(export_folder, resource_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several values are stored by the `Defaults` class. Any value can be overridden (and new values can be added. Make sure to `save()` any changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Defined Keys: --\n",
      "config_file\n",
      "root\n",
      "split_ratio\n",
      "data_yaml\n",
      "resource_map\n",
      "trainer_template\n"
     ]
    }
   ],
   "source": [
    "d = Defaults()\n",
    "print(\" -- Defined Keys: --\")\n",
    "print(\"\\n\".join([x for x in d.__dict__.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speciy the absolute path of the root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/vision.philbrockman.com/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.root = \"/content/drive/MyDrive/vision.philbrockman.com/ModelAssistedLabel/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save any changes and enter root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moving to /content/drive/MyDrive/vision.philbrockman.com/ModelAssistedLabel/\n"
     ]
    }
   ],
   "source": [
    "d.save()\n",
    "d.to_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I borrow the instructions to set up the Ultralytics repo from [the Roboflow tutorial]( https://models.roboflow.com/object-detection/yolov5). (If I'd be allowed one undo on this project, I wish I would have intially forked this project from that tutorial.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 5532 (delta 1), reused 0 (delta 0), pack-reused 5525\u001b[K\n",
      "Receiving objects: 100% (5532/5532), 8.15 MiB | 8.19 MiB/s, done.\n",
      "Resolving deltas: 100% (3776/3776), done.\n",
      "/content/drive/My Drive/vision.philbrockman.com/ModelAssistedLabel/yolov5\n",
      "\u001b[K     |████████████████████████████████| 645kB 4.1MB/s \n",
      "\u001b[?25h/content/drive/My Drive/vision.philbrockman.com/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "# clone YOLOv5 repository\n",
    "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "\n",
    "%cd yolov5\n",
    "# install dependencies as necessary\n",
    "!pip install -qr requirements.txt  # install dependencies (ignore errors)\n",
    "import torch\n",
    "\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "from utils.google_utils import gdrive_download  # to download models/datasets\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure GPU is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using torch 1.8.0+cu101 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n",
      "moving to /content/drive/MyDrive/vision.philbrockman.com/ModelAssistedLabel/\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) ))\n",
    "  d.to_root() #step up a level\n",
    "else:\n",
    "   raise Exception(\"enable GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of my classes are digits. Under the hood, the YOLOv5 model is working of the index of the class, rather than the human-readable name. Consequently, the identities of each class index must be supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train': 589}, {'valid': 169}, {'test': 83}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ModelAssistedLabel.fileManagement import Generation\n",
    "\n",
    "backup_dir = \"archive/Generation/zips\"\n",
    "\n",
    "g = Generation(repo=labeled_images, \n",
    "               out_dir=backup_dir,\n",
    "               verbose=True)\n",
    "\n",
    "g.set_split()\n",
    "g.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dirs ['./train', './valid', './test']\n",
      "yaml archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 03-46-34/data.yaml\n",
      "subdir train\n",
      "\toutdir archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 03-46-34\n",
      "subdir valid\n",
      "\toutdir archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 03-46-34\n",
      "subdir test\n",
      "\toutdir archive/Generation/zips/Final Roboflow Export (841)seven segment digits - 1 21-03-27 03-46-34\n",
      "os.listdir ['train', 'valid', 'test', 'data.yaml']\n"
     ]
    }
   ],
   "source": [
    "zipped = g.write_split_to_disk(descriptor=export_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the images need to be written in a way so that the Ultralytics repository can understand their content. The `Autoweights` class both organizes data and create weights. Running an \"initialize\" command makes changes to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelAssistedLabel.train import AutoWeights\n",
    "#configure a basic AutoWeights class instance\n",
    "aw = AutoWeights(name=export_folder, out_dir=backup_dir)\n",
    "\n",
    "# create train/valid/test split from a bag of labeled images (recusively seek out images/labels)\n",
    "aw.initialize_images_from_zip(zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peep on the sizes of the train/valid/test groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aw.traverse_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the images written to disk, we can run the Ultralytics training algorithm. I loved watching the progress fly by in real time on the original `train.py`. Fortunately, the Ultralytics folk write the results file to disk so the model's training data is still accessible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7e0dd2a6a859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aw.generate_weights(4)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/content/ModelAssistedLabel/ModelAssistedLabel/train.py\u001b[0m in \u001b[0;36mgenerate_weights\u001b[0;34m(self, epochs, rm_local_files)\u001b[0m\n\u001b[1;32m    116\u001b[0m       \u001b[0mpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'name'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "aw.generate_weights(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results folder is stored as an attribute as well, and it has a lot of data stored therein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('yolov5/runs/train/seven segment digits - 1/', 22)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "aw.last_results_path, len(os.listdir(aw.last_results_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the weights are stored in a subfolder called (aptly) \"weights\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best.pt', 'last.pt']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(aw.last_results_path + \"/weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the last couple lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['   41/1999      1.8G   0.04749   0.03455   0.05389    0.1359       118       416    0.8506    0.2276    0.3496    0.2358   0.03371   0.02204   0.04631\\n',\n",
       " '   42/1999      1.8G   0.04569   0.03393   0.05374    0.1334       159       416    0.6312    0.3083    0.3643    0.2213   0.03632   0.02228   0.04571\\n',\n",
       " '   43/1999      1.8G   0.04732   0.03478   0.05302    0.1351       161       416    0.8631    0.2412    0.3837    0.2486   0.03404   0.02165   0.04522\\n',\n",
       " '   44/1999      1.8G   0.04447   0.03223   0.05238    0.1291        99       416    0.7407    0.2552    0.3829    0.2582    0.0325   0.02133   0.04472\\n',\n",
       " '   45/1999      1.8G    0.0459   0.03297   0.05181    0.1307       130       416    0.6308     0.317    0.3895    0.2627    0.0343   0.02171   0.04392\\n']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(aw.last_results_path + \"results.txt\") as results_file:\n",
    "  results = results_file.readlines()\n",
    "print(\"Epoch   gpu_mem       box       obj       cls     total    labels  img_size\")\n",
    "results[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling a new set of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `Viewer` class doesn't care how recently your weights were generated so you can plug in existing weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "from ModelAssistedLabel.detect import Viewer\n",
    "\n",
    "# access the folder of results from the AutoWeights instance\n",
    "results_folder = aw.last_results_path\n",
    "\n",
    "# I'm choosing to use the best weight.\n",
    "weight_path = results_folder + \"/weights/best.pt\"\n",
    "\n",
    "# Viewer needs a set of weights and an array of labels for the detected object types\n",
    "v = Viewer(weight_path, class_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selects all images in the unlabeled folder and let's us look through the computer's eyes at the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Output hidden; open in https://colab.research.google.com to view."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import random, glob\n",
    "\n",
    "images = glob.glob(f\"./{unlabeled_images}/*.jpg\")\n",
    "\n",
    "for image in random.sample(images,5):\n",
    "  v.plot_for(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/125.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/129.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/152.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/92.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/197.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/75.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/48.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/107.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/141.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/120.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/13.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/99.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/77.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/8.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/166.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/18.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/95.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/68.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/157.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/23.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/182.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/60.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/72.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/35.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/21.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/124.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/181.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/113.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/154.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/94.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/183.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/134.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/90.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/98.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/87.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/136.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/103.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/191.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/123.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/41.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/52.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/138.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/171.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/25.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/178.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/172.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/190.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/38.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/84.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/20.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/36.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/64.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/91.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/126.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/57.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/159.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/7.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/74.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/139.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/196.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/151.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/179.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/71.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/119.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/55.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/17.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/6.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/112.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/146.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/49.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/133.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/173.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/199.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/185.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/198.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/106.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/140.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/142.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/51.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/2.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/121.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/177.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/158.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/53.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/111.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/24.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/73.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/79.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/67.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/180.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/130.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/70.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/12.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/15.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/45.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/186.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/101.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/1.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/176.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/88.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/144.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/165.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/174.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/47.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/34.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/169.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/132.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/137.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/22.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/127.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/58.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/54.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/46.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/31.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/86.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/76.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/16.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/115.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/189.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/27.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/163.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/102.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/164.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/85.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/167.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/4.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/135.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/96.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/150.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/29.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/37.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/82.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/184.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/78.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/160.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/114.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/50.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/148.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/56.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/42.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/110.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/89.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/104.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/149.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/59.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/162.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/168.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/39.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/69.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/0.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/97.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/117.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/116.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/153.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/195.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/11.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/105.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/40.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/65.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/32.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/33.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/122.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/19.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/161.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/108.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/188.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/80.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/30.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/147.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/63.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/156.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/118.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/131.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/61.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/170.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/10.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/14.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/128.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/28.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/26.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/81.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/100.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/145.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/193.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/194.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/43.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/143.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/62.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/83.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/93.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/9.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/109.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/175.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/66.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/187.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/192.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/155.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/44.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/3.jpg: image 1/1 /content/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/5.jpg: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for image in images:\n",
    "  results.append(v.predict_for(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting annotated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the class labels with index 0 on line 1, index 1 on line 2, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(export_folder, \"label_map.txt\"), \"w\") as label_map:\n",
    "  label_map.writelines(\"\\n\".join(class_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that image/label pairs have a common root filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving yolov5 results folder: yolov5/runs/train/seven segment digits - 1/\n"
     ]
    }
   ],
   "source": [
    "import random, PIL, shutil\n",
    "salt = lambda: str(random.random())[2:]\n",
    "\n",
    "for result in results:\n",
    "  #generate a likely-to-be-unique filename\n",
    "  shared_root = Defaults._itername(f\"{project_name}-{salt()}\")\n",
    "\n",
    "  #save the image to the outfile\n",
    "  image = PIL.Image.open(result[\"image path\"])\n",
    "  image.save(os.path.join(export_folder, \"images\", f\"{shared_root}.jpg\"))\n",
    "\n",
    "  #save the predictions to the outfile\n",
    "  predictions = result[\"predictions\"]\n",
    "  with open(os.path.join(export_folder, \"labels\", f\"{shared_root}.txt\"), \"w\") as prediction_file:\n",
    "    prediction_file.writelines(\"\\n\".join([x[\"yolov5 format\"] for x in predictions]))\n",
    "\n",
    "#check if weights were generated\n",
    "if aw is not None and os.path.exists(aw.last_results_path):\n",
    "  print(f\"Moving yolov5 results folder: {aw.last_results_path}\")\n",
    "  shutil.move(aw.last_results_path, export_folder)\n",
    "else:\n",
    "  print(\"No weights to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I would have uploaded this set of image/label pairs to Roboflow for correction and annotation. As the model grows more accurate, I would alter camera position or lighting until the model started stumbling again. I want to be keeping the model on its toes!\n",
    "\n",
    "To be transparent, I developed a [custom React annotator](https://github.com/PhilBrockman/autobbox) that better suited my needs.\n",
    "\n",
    "I labeled dozens upon dozens and dozens of images with Roboflow and would recommend their free annotation service! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "My original goal of \"smartifying\" my rowing machine is closer than ever. \n",
    "\n",
    "It is possible to parse workout information (thought currently, I only have access to a maximum of 4 digits). I wonder if the model could keep up if there were 20+ digits to capture.\n",
    "\n",
    "I know that lighting and camera position have an effect on accuracy. Here's how I'm holding my computer steady as I modify the lighting: [standing](https://raw.githubusercontent.com/PhilBrockman/ModelAssistedLabel/master/DIY-laptop-mount.jpg), [floor 1](https://raw.githubusercontent.com/PhilBrockman/ModelAssistedLabel/master/DIY-computer-capture.jpg), [floor 2](https://github.com/PhilBrockman/ModelAssistedLabel/blob/master/DIY-capture.jpeg?raw=true).\n",
    "\n",
    "Here are 3 runs captured under different lighting conditions:\n",
    "* `21-3-22 rowing (200) 7:50-12:50` (direct lighting from one light source)\n",
    "* `21-3-22 rowing (200) 1:53-7:00` (direct lighting from one light source with glare)\n",
    "* `21-3-18 rowing 8-12 ` (direct light and ambient lamps turned on)\n",
    "\n",
    "> Note:  All unlabeled images were taken inside a blacked-out room. The are stored in `Image Repo/unlabeled/`\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lingering Questions\n",
    "\n",
    "My labeled images are disorderly. There's data from other rowing machines and from [a kind *stranger*'s github repo](https://github.com/SachaIZADI/Seven-Segment-OCR). Some images have been cropped to only include the display. Did having varied data slow me down overall? Or did it make the models more robust? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
