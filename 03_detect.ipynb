{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/Coding/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# \"\"\"\n",
    "# the weights and images are stored my Google Drive\n",
    "# \"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# hope you're already connected to the google drive!\n",
    "%cd \"/content/drive/MyDrive/Coding/ModelAssistedLabel/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/Coding/ModelAssistedLabel\n",
      "install nbdev: f\n",
      "git commit -m: working on detector class\n",
      "hooksn\n",
      "diffsn\n",
      "Converted 00_config.ipynb.\n",
      "Converted 01_split.ipynb.\n",
      "Converted 02_train.ipynb.\n",
      "Converted 03_detect.ipynb.\n",
      "Converted index.ipynb.\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/03_detect.ipynb\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/index.ipynb\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/01_split.ipynb\n",
      "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (3.38.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from progressbar2) (1.15.0)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2) (2.5.6)\n",
      "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (3.38.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from progressbar2) (1.15.0)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2) (2.5.6)\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/02_train.ipynb\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.0.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.0.0)\n",
      "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (3.38.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from progressbar2) (1.15.0)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2) (2.5.6)\n",
      "converting /content/drive/My Drive/Coding/ModelAssistedLabel/index.ipynb to README.md\n",
      "Executing: git config --local include.path ../.gitconfig\n",
      "Success: hooks are installed and repo's .gitconfig is now trusted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%run \"_Synch.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Objects\n",
    "> wrapping `yolov5/detect.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import os, torch\n",
    "os.chdir(\"yolov5\")\n",
    "from pathlib import Path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.general import check_img_size, non_max_suppression\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import scale_coords, xyxy2xywh\n",
    "from utils.torch_utils import select_device\n",
    "from models.experimental import attempt_load\n",
    "import cv2, os, base64, json\n",
    "os.chdir(\"..\")\n",
    "\n",
    "os.system(\"pip install pillow\")\n",
    "\n",
    "class Detector:\n",
    "  \"\"\"A wrapper for training loading saved YOLOv5 weights\n",
    "  \n",
    "  requirements:\n",
    "    GPU enabled\"\"\"\n",
    "  def __init__(self, weight_path, conf_threshold = .4, iou_threshold = .45, imgsz = 416, save_dir=\"save_dir\", write_annotated_images_to_disk=False):\n",
    "    \"\"\" Constructor. I pulled the default numeric values above directly from the\n",
    "    detect.py file. I added the option to save model output to both images and \n",
    "    to txt files\n",
    "\n",
    "    Args:\n",
    "      weight_path: the path to which the saved weights are stored\n",
    "      conf_threshold: lower bound on the acceptable level of uncertainty for a \n",
    "                      bounding box\n",
    "      iou_threshold: IoU helps determine how overlapped two shapes are.\n",
    "      imgsz: resolution of image to process (assumes square)\n",
    "      save_dir: where to write annotated images\n",
    "      write_annotated_images_to_disk: save human-friendly annotated images to disk \n",
    "    \"\"\"\n",
    "    \n",
    "    self.weight_path = weight_path\n",
    "    self.conf_threshold = conf_threshold\n",
    "    self.iou_threshold = iou_threshold\n",
    "    self.imgsz = imgsz\n",
    "    self.device = select_device()\n",
    "    self.model = attempt_load(self.weight_path, map_location=self.device)  # load FP32 model\n",
    "    self.names = self.model.module.names if hasattr(self.model, 'module') else self.model.names\n",
    "    self.imgsz = check_img_size(self.imgsz, s=self.model.stride.max())  # check img_size\n",
    "    self.half = self.device.type != 'cuda'\n",
    "    if self.half:\n",
    "      self.model.half()  # to FP16\n",
    "\n",
    "    self.write_annotated_images_to_disk = write_annotated_images_to_disk\n",
    "    self.save_dir = save_dir\n",
    "\n",
    "  def make_dir(dir):\n",
    "    \"\"\"makes a directory provided that the directiory doesn't already exist\n",
    "    \n",
    "    Args:\n",
    "      dir: Directory to create a path towards\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir):\n",
    "      os.makedirs(dir)\n",
    "\n",
    "  def process_image(self, source):\n",
    "    \"\"\"Runs on the model with pre-specified weights an input image. See original\n",
    "    detect.py for more details\n",
    "\n",
    "    Args:\n",
    "      source: A string path to pre-specified weights for the model\n",
    "      save_unscuffed: create copy of the pre-image\n",
    "\n",
    "    Reurns:\n",
    "      A JSON-serializable object encoding bounding box information\n",
    "    \"\"\"\n",
    "    override = None\n",
    "    if os.path.exists(self.save_dir):\n",
    "      override = input(f\"Save directory '{self.save_dir}' exists. \\n 'Enter' to continue, anything else to cancel operation\")\n",
    "\n",
    "    if override is None or len(override) == 0:\n",
    "      return self.__process_image__(source)\n",
    "\n",
    "    assert False, \"this code shouldn't run\"\n",
    "\n",
    "\n",
    "  #not saving images speed up processing dramatically\n",
    "  def __process_image__(self, source):\n",
    "    \"helper for process_image\"\n",
    "    results = []\n",
    "    img = torch.zeros((1, 3, self.imgsz, self.imgsz), device=self.device)  # init img\n",
    "    _ = self.model(img.half() if self.half else img) if self.device.type != 'cpu' else None  # run once\n",
    "    dataset = LoadImages(source, img_size=self.imgsz)\n",
    "\n",
    "    if self.write_annotated_images_to_disk:\n",
    "      save_dir = Path(self.save_dir)\n",
    "      Detector.make_dir(save_dir)\n",
    "\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "      tmp = {}\n",
    "      img = torch.from_numpy(img).to(self.device)\n",
    "      img = img.half() if self.half else img.float()  # uint8 to fp16/32\n",
    "      img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "      \n",
    "      if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "      pred = self.model(img, augment=False)[0]\n",
    "      pred = non_max_suppression(pred, self.conf_threshold, self.iou_threshold, agnostic=False)\n",
    "      \n",
    "      for i, det in enumerate(pred):\n",
    "        p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
    "        p = Path(p) \n",
    "        \n",
    "        s += '%gx%g ' % img.shape[2:]  # print string\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "\n",
    "        if len(det):\n",
    "          det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "          for c in det[:, -1].unique():\n",
    "              n = (det[:, -1] == c).sum()  # detections per class\n",
    "              s += f'{n} {self.names[int(c)]}s, '  # add to string\n",
    "          \n",
    "          if self.write_annotated_images_to_disk:\n",
    "            tmp[\"unscuffed\"] = f\"{save_dir}/unscuffed-{p.name}\"\n",
    "            cv2.imwrite(tmp[\"unscuffed\"], im0)\n",
    "\n",
    "          tmp[\"predictions\"] = []\n",
    "          for *xyxy, conf, cls in reversed(det):\n",
    "            xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "            line = (cls, *xywh, conf) # label format\n",
    "            tmp[\"predictions\"].append(('%g ' * len(line)).rstrip() % line)\n",
    "\n",
    "            # save image with bboxes drawn on top\n",
    "            if self.write_annotated_images_to_disk: \n",
    "              label = f'{self.names[int(cls)]} {conf:.2f}'\n",
    "              plot_one_box(xyxy, im0, label=label, color=[0,0,200], line_thickness=5)\n",
    "              tmp[\"labeled\"] = f\"{save_dir}/labeled-{p.name}\"\n",
    "              cv2.imwrite(tmp[\"labeled\"], im0)\n",
    "\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "d = Detector(weight_path=\"pre-trained weights/21-2-25 1k-digits YOLOv5-weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls \"Image Repo/labeled/images/Final Roboflow Export (841)/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.process_image(\"Image Repo/labeled/images/Final Roboflow Export (841)/images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_split.ipynb.\n",
      "Converted 02_train.ipynb.\n",
      "Converted 03_detect.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'train/images': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls \"train/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from ModelAssistedLabel.core import Defaults\n",
    "import pandas as pd\n",
    "import re, os, IPython\n",
    "\n",
    "class ImageProcessor:\n",
    "  def __init__(self, with_weights, save_labeled_img=False, save_unscuffed=False, data_yaml = None):\n",
    "    self.with_weights = with_weights\n",
    "    self.d=Detector(self.with_weights, save_labeled_img=save_labeled_img, save_unscuffed=save_unscuffed)\n",
    "    if data_yaml is None:\n",
    "      data = Defaults().data_yaml.split(\"\\n\")[-1]\n",
    "      labels = [re.sub(\"\\D\", \"\", x) for x in data.split(\":\")[1].split(\",\")]\n",
    "    else:\n",
    "      labels = data_yaml\n",
    "    self.labels = labels\n",
    "\n",
    "  def process_new_images(self, folder_path):\n",
    "    dir = os.listdir(path)\n",
    "    results = []\n",
    "    for filename in dir:\n",
    "      results.append({filename: self.d.process_image(path+filename)})\n",
    "      IPython.display.clear_output()\n",
    "    self.results=[self.parse_prediction_for_frame(x) for x in results]\n",
    "    return len(results)\n",
    "\n",
    "  def parse_prediction_for_frame(self, r):\n",
    "    k = list(r.keys())[0]\n",
    "    predictions = pd.DataFrame([x.split(\" \") for x in r[k][\"predictions\"]], columns=[\"yaml class index\", \"xcent\", \"ycent\", \"wdith\", \"height\", \"confidence\"])\n",
    "    readable_class = predictions[\"yaml class index\"].apply(lambda x: self.labels[int(x)])\n",
    "    predictions.insert(0, \"class label\", readable_class)\n",
    "    predictions[\"filename\"] = k\n",
    "    return predictions.sort_values(by=\"xcent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "ip = ImageProcessor(with_weights=\"pre-trained YOLOv5 weights/21-2-25 1k-digits YOLOv5-weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.53 s, sys: 320 ms, total: 7.85 s\n",
      "Wall time: 8.39 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ip.process_new_images(folder_path=\"Image Repo/unlabeled/21-3-18 rowing 8-12 /\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resolving conflicts\n",
    "> overlapping bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import PIL\n",
    "\n",
    "def yoloBox_to_standard(result):\n",
    "  \"\"\"\n",
    "  The standard YOLOv5 coordinate format is normed to 1. Need to extract the \n",
    "  original's image width and height to convert to a standard cartesian plane.\n",
    "\n",
    "  Args:\n",
    "    result: a dictionary that includes both \n",
    "        * a key called \"filename\" that points to the original image\n",
    "        * a key called \"predictions\" created when the image is parsed with the\n",
    "          YOLOv5 model \n",
    "\n",
    "  Returns:\n",
    "    Convert the predictions converted to a full-scale Cartesian coordinate system.\n",
    "  \"\"\"\n",
    "  out = {}\n",
    "  for k,v in result.items():\n",
    "    out[k] = v\n",
    "\n",
    "  PILim= PIL.Image.open(result[\"preimage location\"])\n",
    "  width, height = PILim.width, PILim.height\n",
    "\n",
    "  out[\"predictions\"] = []\n",
    "  for prediction in result[\"predictions\"]:\n",
    "    bbox = prediction[\"bbox\"]\n",
    "    out[\"predictions\"].append({\n",
    "        \"class\": (int(prediction[\"class index\"])+1)%10,\n",
    "        \"confidence\": prediction[\"confidence\"],\n",
    "        \"height\": PILim.height*bbox[\"height\"],\n",
    "        \"width\": PILim.width*bbox[\"width\"],\n",
    "        \"x\": PILim.width*(bbox[\"xcent\"] - bbox[\"width\"]/2),\n",
    "        \"y\": PILim.height*(bbox[\"ycent\"] - bbox[\"height\"]/2)\n",
    "        })\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "  \"Source: https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\"\n",
    "  # determine the (x, y)-coordinates of the intersection rectangle\n",
    "  xA = max(boxA[0], boxB[0])\n",
    "  yA = max(boxA[1], boxB[1])\n",
    "  xB = min(boxA[2], boxB[2])\n",
    "  yB = min(boxA[3], boxB[3])\n",
    "\n",
    "  # compute the area of intersection rectangle\n",
    "  interArea = (xB - xA) * (yB - yA)\n",
    "\n",
    "  # compute the area of both the prediction and ground-truth\n",
    "  # rectangles\n",
    "  boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "  boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "\n",
    "  # compute the intersection over union by taking the intersection\n",
    "  # area and dividing it by the sum of prediction + ground-truth\n",
    "  # areas - the interesection area\n",
    "  iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "  # return the intersection over union value\n",
    "  return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sorted_api(predictions):\n",
    "  \"\"\"sorts a list of prediction in ascending x order\n",
    "\n",
    "  Args:\n",
    "    an array of dictionaries containing bbox predictions\n",
    "\n",
    "  Returns:\n",
    "    the bounding boxes read off from left to right\"\"\"\n",
    "  return sorted(predictions, key=lambda x: x['x'])\n",
    "\n",
    "def digit_arr_to_string(predictions):\n",
    "  \"\"\"\"Human-friendly interpretation of predictions\"\"\"\n",
    "  return \"\".join(str(int(x['class'])%90) for x in sorted_api(predictions))\n",
    "\n",
    "\n",
    "def corners(r):\n",
    "  \"\"\" \n",
    "  Defines the relevante coordinate information\n",
    "\n",
    "  Args: \n",
    "    r: converts a DataFrame row to a tuple\n",
    "    \n",
    "  Returns:\n",
    "    Caretensian coordinate information (anchor in the lower left)\n",
    "  \"\"\"\n",
    "  return (r['x'], r['y'], r['x'] + r['width'], r['y'] + r['height'])\n",
    "\n",
    "def calculate_iou(frame, threshold = .4):\n",
    "  \"\"\"\n",
    "  Determines the pair of bounding boxes that has the highest IoU over the :threshold:\n",
    "\n",
    "  Args:\n",
    "    frame: the output of either a YOLOv5 model call or an API call\n",
    "    threshold: maximum percentage of overlap allowed between the bounding boxes\n",
    "  \"\"\"\n",
    "  previous = None\n",
    "  overlappers = None\n",
    "\n",
    "  for i in range(len(sorted_api(frame))):\n",
    "    row = sorted_api(frame)[i]\n",
    "    row[\"index\"] = i\n",
    "    if previous is None:\n",
    "      previous = row\n",
    "      continue\n",
    "\n",
    "    iou = bb_intersection_over_union(corners(previous), corners(row))\n",
    "    if iou > threshold:\n",
    "      tmp = {row[\"confidence\"]: row, previous[\"confidence\"]: previous}\n",
    "      overlappers = {\n",
    "        \"iou\": iou,\n",
    "        \"least confident\": tmp[min(tmp.keys())]\n",
    "      }\n",
    "\n",
    "    previous = row\n",
    "  return overlappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: the following functions work together to form the core of parsing bounding\n",
    "box information.\n",
    "\n",
    "> Warning: I assume that screens have no more than 4 objects in them.\n",
    "\n",
    "If extra bounding boxesare detected, I find the pair of boxes that has the highest IoU (Intersection \n",
    "over Union). Of that pair, I identify the box that has the a lower associated\n",
    "confidence and remove it.\n",
    "\n",
    "If there is still an excess of bounding boxes but no boxes that are overlapping\n",
    "in excess of the :threshold:, then bounding boxes with the most deviated y are \n",
    "removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_predictions(predictions_arr):\n",
    "  while len(predictions_arr) > 0 and len(predictions_arr) != len(remove_worst(predictions_arr)):\n",
    "    predictions_arr = remove_worst(predictions_arr)\n",
    "  return predictions_arr\n",
    "\n",
    "def remove_worst_offender(res, threshold):\n",
    "  overlappers = calculate_iou(res)\n",
    "  \n",
    "  if overlappers is not None:\n",
    "    included = [x for x in res if x[\"index\"] != overlappers[\"least confident\"][\"index\"]]\n",
    "    return included\n",
    "  else: #look for y outlier\n",
    "    df = pd.DataFrame.from_dict(res)\n",
    "    # print(df.sort_values(by=\"confidence\",ascending=False))\n",
    "    df[\"zscore\"] = abs(df.y - df.y.mean())/df.y.std() #FIXME\n",
    "    return df[df[\"zscore\"] < 1.5].to_dict('records')\n",
    "\n",
    "def remove_worst(res, threshold=.4):\n",
    "  return remove_worst_offender(res, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import io, PIL, base64\n",
    "\n",
    "def raw_parse_from_json(json_elements, loaded_detector):\n",
    "  \"\"\"\n",
    "  Helper to interpret raw base 64 images. Creates a new JPG file from the base64\n",
    "  data and feeds the image to the Detector\n",
    "\n",
    "  Args:\n",
    "    json_elements: array of dictionaries from file. Assert existence of a \"base64\"\n",
    "      key where the images are stored\n",
    "    loaded_detector: a valid Detector\n",
    "  \n",
    "  returns:\n",
    "    the result of processessing all of the base64 images\n",
    "  \"\"\"\n",
    "  out = []\n",
    "  for idx in range(len(json_elements)):\n",
    "    from64name = Defaults._itername(\"(\", \") image.jpg\")\n",
    "    b64image = json_elements[idx][\"base64\"]\n",
    "    im = PIL.Image.open(io.BytesIO(base64.b64decode(b64image)))\n",
    "    im.save(from64name)\n",
    "    \n",
    "    single = loaded_detector.process_image(from64name)\n",
    "    single[\"preimage location\"] = from64name\n",
    "    out.append(single)\n",
    "  return out\n",
    "\n",
    "def parse_from_json(json_elements, loaded_detector):\n",
    "  \"\"\"\n",
    "  Gets a JSON file containing the base64 images of data to label pushed through\n",
    "  a Detector. Post-processing automatically de-norms the YOLOv5 bounding boxes \n",
    "  based on the original image's height and width\n",
    "\n",
    "  Args:\n",
    "    json_elements: array of dictionaries from file. Assert existence of a \"base64\"\n",
    "      key where the images are stored\n",
    "    loaded_detector: a valid Detector\n",
    "  \n",
    "  returns:\n",
    "    the bounding boxes associated with each image\n",
    "  \"\"\"\n",
    "  preprocessed = raw_parse_from_json(json_elements, loaded_detector)\n",
    "  ds = DataSet(preprocessed)\n",
    "  processed=ds.export()\n",
    "\n",
    "  for i in range(len(processed)):\n",
    "    processed[i][\"preimage location\"] = processed[i][\"filename\"]\n",
    "\n",
    "  postprocess = []\n",
    "  for x in processed:\n",
    "    postprocess.append(yoloBox_to_standard(x))\n",
    "  return postprocess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
