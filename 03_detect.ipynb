{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/Coding/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# \"\"\"\n",
    "# the weights and images are stored my Google Drive\n",
    "# \"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# hope you're already connected to the google drive!\n",
    "%cd \"/content/drive/MyDrive/Coding/ModelAssistedLabel/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/Coding/ModelAssistedLabel\n",
      "install nbdev: y\n",
      "git commit -m: integration with new uploaded data\n",
      "hooks\n",
      "diffs\n",
      "Converted 00_config.ipynb.\n",
      "Converted 01_split.ipynb.\n",
      "Converted 02_train.ipynb.\n",
      "Converted 03_detect.ipynb.\n",
      "Converted index.ipynb.\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/index.ipynb\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/03_detect.ipynb\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.0.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.0.0)\n",
      "converting /content/drive/My Drive/Coding/ModelAssistedLabel/index.ipynb to README.md\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# %matplotlib inline\n",
    "%run \"_Synch.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Objects\n",
    "> wrapping `yolov5/detect.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import os, torch\n",
    "os.chdir(\"yolov5\")\n",
    "from pathlib import Path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.general import check_img_size, non_max_suppression\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import scale_coords, xyxy2xywh\n",
    "from utils.torch_utils import select_device\n",
    "from models.experimental import attempt_load\n",
    "import cv2, os, base64, json\n",
    "os.chdir(\"..\")\n",
    "\n",
    "os.system(\"pip install pillow\")\n",
    "\n",
    "class Detector:\n",
    "  \"\"\"A wrapper for training loading saved YOLOv5 weights\n",
    "  \n",
    "  requirements:\n",
    "    GPU enabled\"\"\"\n",
    "  def __init__(self, weight_path, conf_threshold = .4, iou_threshold = .45, imgsz = 416, save_dir=\"save_dir\", write_annotated_images_to_disk=False):\n",
    "    \"\"\" Constructor. I pulled the default numeric values above directly from the\n",
    "    detect.py file. I added the option to save model output to both images and \n",
    "    to txt files\n",
    "\n",
    "    Args:\n",
    "      weight_path: the path to which the saved weights are stored\n",
    "      conf_threshold: lower bound on the acceptable level of uncertainty for a \n",
    "                      bounding box\n",
    "      iou_threshold: IoU helps determine how overlapped two shapes are.\n",
    "      imgsz: resolution of image to process (assumes square)\n",
    "      save_dir: where to write annotated images\n",
    "      write_annotated_images_to_disk: save human-friendly annotated images to disk \n",
    "    \"\"\"\n",
    "    \n",
    "    self.weight_path = weight_path\n",
    "    self.conf_threshold = conf_threshold\n",
    "    self.iou_threshold = iou_threshold\n",
    "    self.imgsz = imgsz\n",
    "    self.device = select_device()\n",
    "    self.model = attempt_load(self.weight_path, map_location=self.device)  # load FP32 model\n",
    "    self.names = self.model.module.names if hasattr(self.model, 'module') else self.model.names\n",
    "    self.imgsz = check_img_size(self.imgsz, s=self.model.stride.max())  # check img_size\n",
    "    self.half = self.device.type != 'cuda'\n",
    "    if self.half:\n",
    "      self.model.half()  # to FP16\n",
    "\n",
    "    self.write_annotated_images_to_disk = write_annotated_images_to_disk\n",
    "    self.save_dir = save_dir\n",
    "\n",
    "  def make_dir(dir):\n",
    "    \"\"\"makes a directory provided that the directiory doesn't already exist\n",
    "    \n",
    "    Args:\n",
    "      dir: Directory to create a path towards\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir):\n",
    "      os.makedirs(dir)\n",
    "\n",
    "  def process_image(self, source):\n",
    "    \"\"\"Runs on the model with pre-specified weights an input image. See original\n",
    "    detect.py for more details\n",
    "\n",
    "    Args:\n",
    "      source: A string path to pre-specified weights for the model\n",
    "      save_unscuffed: create copy of the pre-image\n",
    "\n",
    "    Reurns:\n",
    "      A JSON-serializable object encoding bounding box information\n",
    "    \"\"\"\n",
    "    override = None\n",
    "    if os.path.exists(self.save_dir):\n",
    "      override = input(f\"Save directory '{self.save_dir}' exists. \\n 'Enter' to continue, anything else to cancel operation\")\n",
    "\n",
    "    if override is None or len(override) == 0:\n",
    "      return self.__process_image__(source)\n",
    "\n",
    "    assert False, \"this code shouldn't run\"\n",
    "\n",
    "\n",
    "  #not saving images speed up processing dramatically\n",
    "  def __process_image__(self, source):\n",
    "    \"helper for process_image\"\n",
    "    results = []\n",
    "    img = torch.zeros((1, 3, self.imgsz, self.imgsz), device=self.device)  # init img\n",
    "    _ = self.model(img.half() if self.half else img) if self.device.type != 'cpu' else None  # run once\n",
    "    dataset = LoadImages(source, img_size=self.imgsz)\n",
    "\n",
    "    if self.write_annotated_images_to_disk:\n",
    "      save_dir = Path(self.save_dir)\n",
    "      Detector.make_dir(save_dir)\n",
    "\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "      tmp = {}\n",
    "      img = torch.from_numpy(img).to(self.device)\n",
    "      img = img.half() if self.half else img.float()  # uint8 to fp16/32\n",
    "      img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "      \n",
    "      if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "      pred = self.model(img, augment=False)[0]\n",
    "      pred = non_max_suppression(pred, self.conf_threshold, self.iou_threshold, agnostic=False)\n",
    "      \n",
    "      for i, det in enumerate(pred):\n",
    "        p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
    "        p = Path(p) \n",
    "        \n",
    "        s += '%gx%g ' % img.shape[2:]  # print string\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "\n",
    "        if len(det):\n",
    "          det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "          for c in det[:, -1].unique():\n",
    "              n = (det[:, -1] == c).sum()  # detections per class\n",
    "              s += f'{n} {self.names[int(c)]}s, '  # add to string\n",
    "          \n",
    "          if self.write_annotated_images_to_disk:\n",
    "            tmp[\"unscuffed\"] = f\"{save_dir}/unscuffed-{p.name}\"\n",
    "            cv2.imwrite(tmp[\"unscuffed\"], im0)\n",
    "\n",
    "          tmp[\"predictions\"] = []\n",
    "          for *xyxy, conf, cls in reversed(det):\n",
    "            xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "            line = (cls, *xywh, conf) # label format\n",
    "            tmp[\"predictions\"].append(('%g ' * len(line)).rstrip() % line)\n",
    "\n",
    "            # save image with bboxes drawn on top\n",
    "            if self.write_annotated_images_to_disk: \n",
    "              label = f'{self.names[int(cls)]} {conf:.2f}'\n",
    "              plot_one_box(xyxy, im0, label=label, color=[0,0,200], line_thickness=5)\n",
    "              tmp[\"labeled\"] = f\"{save_dir}/labeled-{p.name}\"\n",
    "              cv2.imwrite(tmp[\"labeled\"], im0)\n",
    "\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-friendly Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human-readable information about the class indentities is stored in the data.yaml folder. By default, the data.yaml file is created from the `Defaults` class. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: ../train/images\n",
      "val: ../valid/images\n",
      "\n",
      "nc: 10\n",
      "names: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n"
     ]
    }
   ],
   "source": [
    "from ModelAssistedLabel.core import Defaults\n",
    "print(Defaults().data_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the \"names\" variable to a python-friendly format, we do the following manipulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, re\n",
    "\n",
    "# needs to be wrapped in quotes to parse as dict\n",
    "substitute = \"names\"\n",
    "\n",
    "#select last line\n",
    "classlist = Defaults().data_yaml.split(\"\\n\")[-1]\n",
    "\n",
    "#add quotes around `names` ONLY around the start of a string\n",
    "classlist = re.sub('^%s' % substitute, f\"'{substitute}'\", classlist)\n",
    "\n",
    "#surround the string in curly braces to tell python it's a dict\n",
    "classlist = f\"{{{classlist}}}\"\n",
    "\n",
    "# parse string as dict\n",
    "classlist = ast.literal_eval(classlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now here is the value of the classes as used by yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'names': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the YOLOv5 Output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ModelAssistedLabel.detect import Detector\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "import PIL\n",
    "\n",
    "class Viewer:\n",
    "  \"\"\" After supplying this class with the paths to (1) several pre-generated weights,\n",
    "  and (2) several new images, you can create a model from each weight and a prediction\n",
    "  for each image from each model.\n",
    "  \"\"\"\n",
    "  def __init__(self, weight_path, class_arr):\n",
    "    \"\"\"\n",
    "    constructor builds the detectors (may be relatively time-intensive) and stores \n",
    "    the targets\n",
    "\n",
    "    Args:\n",
    "      weights_paths: an array of paths to weights\n",
    "    \"\"\"\n",
    "    self.detector = Detector(weight_path=weight_path)\n",
    "    self.class_arr = class_arr\n",
    "    self.last_result = []\n",
    "  \n",
    "  def process(self, images, plot_results=True, show_labels=True,figsize=(20,10)):\n",
    "    \"\"\"\n",
    "    Runs an image set through this class's detector.\n",
    "\n",
    "    Args:\n",
    "      images: array of images\n",
    "      plot_results: display the images with superimposed bounding boxes\n",
    "      show_labels: whether or not to show the human-friendly labels about the bounding boxes\n",
    "      figsize: passed to plt\n",
    "\n",
    "    Returns:\n",
    "      an array of dicts. Dicts relate each set of predictions to an image path\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for image in images:\n",
    "      result = self.detector.process_image(image)\n",
    "      results.append({\"img path\": image, \"predictions\": result})\n",
    "      #re-calculate predictions with \"de-normalized\" values\n",
    "      predictions = self.__yolov5_pred_to_standard__(image, result)[\"predictions\"]\n",
    "      #show the image with super-imposed bounding boxes\n",
    "      if plot_results:\n",
    "        Viewer.__plot_with_bbox__(image,predictions,show_labels=show_labels, figsize=figsize)\n",
    "    self.last_result.append(results)\n",
    "\n",
    "  def __plot_with_bbox__(img_path, predictions, show_labels, figsize):\n",
    "    \"\"\"display the rectangles on top of the image using pyplot\n",
    "\n",
    "    Args:\n",
    "      img_path: path to the image of interest\n",
    "      predictions: the output from the Detector's process image\n",
    "      figsize: parameter for fig\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    #open/display the image\n",
    "    im0 = PIL.Image.open(img_path)\n",
    "    ax.imshow(im0)\n",
    "\n",
    "    for prediction in predictions:\n",
    "      # Create a Rectangle patch\n",
    "      rect = patches.Rectangle((prediction['x'], prediction['y']), prediction['width'], prediction['height'], linewidth=1, edgecolor='r', facecolor='none')\n",
    "      if show_labels:\n",
    "        ax.annotate(prediction[\"class\"], xy=(prediction['x'], prediction['y']-10), color='r', fontsize=20)\n",
    "      # Add the patch to the Axes\n",
    "      ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "  def __yolov5_pred_to_standard__(self, image, result):\n",
    "    \"\"\"\n",
    "    The standard YOLOv5 coordinate format is normed to 1. Need to extract the \n",
    "    original's image width and height to convert to a standard cartesian plane.\n",
    "\n",
    "    Args:\n",
    "      image: path to image\n",
    "      result: a dictionary that includes both \n",
    "          * a key called \"filename\" that points to the original image\n",
    "          * a key called \"predictions\" created when the image is parsed with the\n",
    "            YOLOv5 model \n",
    "\n",
    "    Returns:\n",
    "      Convert the predictions converted to a full-scale Cartesian coordinate system.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    out[\"image path\"] = image\n",
    "    PILim= PIL.Image.open(image)\n",
    "    width, height = PILim.width, PILim.height\n",
    "\n",
    "    out[\"predictions\"] = []\n",
    "    for prediction in result[\"predictions\"]:\n",
    "      bbox = prediction.split(\" \")\n",
    "      out[\"predictions\"].append({\n",
    "          \"class\": self.class_arr[int(bbox[0])],\n",
    "          \"confidence\": float(bbox[5]),\n",
    "          \"height\": int(PILim.height*float(bbox[4])),\n",
    "          \"width\": int(PILim.width*float(bbox[3])),\n",
    "          \"x\": int(PILim.width*(float(bbox[1]) - float(bbox[3])/2)),\n",
    "          \"y\": int(PILim.height*(float(bbox[2]) - float(bbox[4])/2))\n",
    "          })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grad a sample of some new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "repo = \"Image Repo/unlabeled/21-3-18 rowing 8-12 /\"\n",
    "new_images = [os.path.join(repo, x) for x in os.listdir(repo)]\n",
    "samples = random.sample(new_images, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a `Viewer` object to investigate the behavior of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "weight_path = 'pre-trained weights/21-2-25 1k-digits YOLOv5-weights.pt'\n",
    "v = Viewer([weight_path], classlist['names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Output hidden; open in https://colab.research.google.com to view."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v.process(samples, plot_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the results are stored away in the `last_result` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'img path': 'Image Repo/unlabeled/21-3-18 rowing 8-12 /164.jpg',\n",
       "  'predictions': {'predictions': ['2 0.263672 0.379861 0.0226563 0.0958333 0.546299',\n",
       "    '2 0.323047 0.386806 0.0210938 0.0875 0.803381',\n",
       "    '9 0.292969 0.381944 0.0234375 0.0888889 0.825718',\n",
       "    '6 0.352344 0.386806 0.01875 0.0930556 0.8395']}},\n",
       " {'img path': 'Image Repo/unlabeled/21-3-18 rowing 8-12 /122.jpg',\n",
       "  'predictions': {'predictions': ['2 0.304688 0.377778 0.021875 0.0833333 0.556505',\n",
       "    '2 0.33125 0.382639 0.0203125 0.0847222 0.723794',\n",
       "    '2 0.275781 0.373611 0.01875 0.0888889 0.842041',\n",
       "    '1 0.357422 0.386806 0.0226563 0.0875 0.920106']}},\n",
       " {'img path': 'Image Repo/unlabeled/21-3-18 rowing 8-12 /16.jpg',\n",
       "  'predictions': {'predictions': ['8 0.380859 0.388889 0.0226563 0.0888889 0.435701',\n",
       "    '7 0.381641 0.389583 0.0226563 0.0902778 0.503729',\n",
       "    '3 0.353906 0.381944 0.0203125 0.0833333 0.546678',\n",
       "    '2 0.298047 0.377083 0.0210938 0.0930556 0.820826',\n",
       "    '4 0.325 0.379861 0.021875 0.0875 0.867799']}},\n",
       " {'img path': 'Image Repo/unlabeled/21-3-18 rowing 8-12 /111.jpg',\n",
       "  'predictions': {'predictions': ['2 0.303906 0.378472 0.021875 0.0847222 0.418617',\n",
       "    '4 0.358203 0.391667 0.0226563 0.0888889 0.755997',\n",
       "    '2 0.274609 0.375694 0.0210938 0.0902778 0.769457']}},\n",
       " {'img path': 'Image Repo/unlabeled/21-3-18 rowing 8-12 /89.jpg',\n",
       "  'predictions': {'predictions': ['2 0.308984 0.378472 0.0179687 0.0847222 0.404879',\n",
       "    '6 0.366016 0.390972 0.0179687 0.0847222 0.636839',\n",
       "    '2 0.275781 0.374306 0.0203125 0.0902778 0.71588']}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.last_result[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
