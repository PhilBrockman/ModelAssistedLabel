{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/Coding/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# \"\"\"\n",
    "# the weights and images are stored my Google Drive\n",
    "# \"\"\"\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# hope you're already connected to the google drive!\n",
    "%cd \"/content/drive/MyDrive/Coding/ModelAssistedLabel/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Objects\n",
    "> wrapping `detect.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class wrapper for detect.py that supporting writing data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "from utils.plots import plot_one_box\n",
    "\n",
    "from utils.general import check_img_size, non_max_suppression\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import scale_coords, xyxy2xywh\n",
    "from utils.torch_utils import select_device\n",
    "from models.experimental import attempt_load\n",
    "import cv2, os, base64, json\n",
    "\n",
    "os.system(\"pip install colour\")\n",
    "os.system(\"pip install pillow\")\n",
    "\n",
    "class Detector:\n",
    "  \"\"\"A wrapper for training loading saved YOLOv5 weights\n",
    "  \n",
    "  requirements:\n",
    "    GPU enabled\"\"\"\n",
    "  def __init__(self, weight_path, conf_threshold = .4, iou_threshold = .45, imgsz = 416, save_dir=\"save_dir\", save_labeled_img = True, log_bboxes_as_txt = True, ):\n",
    "    \"\"\" Constructor. I pulled the default numeric values above directly from the\n",
    "    detect.py file. I added the option to save model output to both images and \n",
    "    to txt files\n",
    "\n",
    "    Args:\n",
    "      weight_path: the path to which the saved weights are stored\n",
    "      conf_threshold: lower bound on the acceptable level of uncertainty for a \n",
    "                      bounding box\n",
    "      iou_threshold: IoU helps determine how overlapped two shapes are.\n",
    "      imgsz: resolution of image to process (assumes square)\n",
    "      save_labeled_img: save a copy of the image with visibile bounding boxing\n",
    "      log_bboxes_as_txt: create 1 (True) or 0 (False) txt files per bounding box \n",
    "    \"\"\"\n",
    "    \n",
    "    self.weight_path = weight_path\n",
    "    self.conf_threshold = conf_threshold\n",
    "    self.iou_threshold = iou_threshold\n",
    "    self.imgsz = imgsz\n",
    "    self.device = select_device()\n",
    "    self.model = attempt_load(self.weight_path, map_location=self.device)  # load FP32 model\n",
    "    self.names = self.model.module.names if hasattr(self.model, 'module') else self.model.names\n",
    "    self.imgsz = check_img_size(self.imgsz, s=self.model.stride.max())  # check img_size\n",
    "    self.half = self.device.type != 'cuda'\n",
    "    if self.half:\n",
    "      self.model.half()  # to FP16\n",
    "\n",
    "    self.save_labeled_img = save_labeled_img\n",
    "    self.log_bboxes_as_txt = log_bboxes_as_txt\n",
    "    self.save_dir = save_dir\n",
    "\n",
    "  def process_image(self, source, save_unscuffed=True):\n",
    "    \"\"\"Runs on the model with pre-specified weights an input image. See original\n",
    "    detect.py for more details\n",
    "\n",
    "    Args:\n",
    "      source: A string path to pre-specified weights for the model\n",
    "      save_unscuffed: create copy of the pre-image\n",
    "\n",
    "    Reurns:\n",
    "      A JSON-serializable object encoding bounding box information\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    img = torch.zeros((1, 3, self.imgsz, self.imgsz), device=self.device)  # init img\n",
    "    _ = self.model(img.half() if self.half else img) if self.device.type != 'cpu' else None  # run once\n",
    "    dataset = LoadImages(source, img_size=self.imgsz)\n",
    "    save_dir = Path(self.save_dir)\n",
    "    make_dir(save_dir)\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "      tmp = {}\n",
    "      tmp[\"txt\"] = []\n",
    "      img = torch.from_numpy(img).to(self.device)\n",
    "      img = img.half() if self.half else img.float()  # uint8 to fp16/32\n",
    "      img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "      if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "      pred = self.model(img, augment=False)[0]\n",
    "      pred = non_max_suppression(pred, self.conf_threshold, self.iou_threshold, agnostic=False)\n",
    "      \n",
    "      for i, det in enumerate(pred):\n",
    "        p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
    "        p = Path(p) \n",
    "        save_path = str(save_dir / p.name) \n",
    "        \n",
    "        make_dir(str(save_dir / 'labels'))\n",
    "        txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')\n",
    "        \n",
    "        s += '%gx%g ' % img.shape[2:]  # print string\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "\n",
    "        if len(det):\n",
    "          det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "          for c in det[:, -1].unique():\n",
    "              n = (det[:, -1] == c).sum()  # detections per class\n",
    "              s += f'{n} {self.names[int(c)]}s, '  # add to string\n",
    "          \n",
    "          if save_unscuffed:\n",
    "            tmp[\"unscuffed\"] = f\"{save_dir}/unscuffed-{p.name}\"\n",
    "            cv2.imwrite(tmp[\"unscuffed\"], im0)\n",
    "\n",
    "          for *xyxy, conf, cls in reversed(det):\n",
    "            if self.log_bboxes_as_txt:  \n",
    "              xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "              line = (cls, *xywh, conf) if True else (cls, *xywh)  # label format\n",
    "              i = 0\n",
    "              run = True\n",
    "              while run:\n",
    "                txt_path_ =  f'{txt_path}-{i}.txt'\n",
    "                if not os.path.exists(txt_path_):\n",
    "                  open(txt_path_, 'a').close()\n",
    "                  tmp[\"txt\"].append(txt_path_)\n",
    "                  run = False\n",
    "                i += 1\n",
    "              with open(txt_path_, 'a') as f:\n",
    "                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "              if True: \n",
    "                label = f'{self.names[int(cls)]} {conf:.2f}'\n",
    "                plot_one_box(xyxy, im0, label=label, color=[0,0,200], line_thickness=5)\n",
    "\n",
    "            # save image with bboxes drawn on top\n",
    "            if self.save_labeled_img:\n",
    "              tmp[\"labeled\"] = f\"{save_dir}/labeled-{p.name}\"\n",
    "              cv2.imwrite(tmp[\"labeled\"], im0)\n",
    "\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created these helper functions support the modifcations to detect.py to allow\n",
    "writing output to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def make_dir(dir):\n",
    "  \"\"\"makes a directory provided that the directiory doesn't already exist\n",
    "  \n",
    "  Args:\n",
    "    dir: Directory to create a path towards\n",
    "  \"\"\"\n",
    "\n",
    "  if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "\n",
    "def touch_file(fpath):\n",
    "  \"\"\"`touch`es a file\"\"\"\n",
    "  open(fpath, 'a').close()\n",
    "\n",
    "\n",
    "def _itername(pre, post):\n",
    "  \"\"\"If function terminates, returns the lowest conflict-free file path \n",
    "  formatted as '{pre}X{post}' where X is the string representation of a natural\n",
    "  number\n",
    "  \n",
    "  args:\n",
    "    pre: filename before the counter\n",
    "    post: filename after the counter\n",
    "\n",
    "  returns:\n",
    "    A unique structured filename\n",
    "  \"\"\"\n",
    "  counter = 0\n",
    "  while True:\n",
    "    counter += 1\n",
    "    fpath = f'{pre}{counter}{post}'\n",
    "    if not os.path.exists(fpath):\n",
    "      return fpath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data set: preparing bounding boxes for post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, base64, json\n",
    "\n",
    "os.system(\"pip install colour\")\n",
    "os.system(\"pip install pillow\")\n",
    "\n",
    "import PIL.Image\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import cv2,  base64\n",
    "\n",
    "def draw_screen(image):\n",
    "  \"\"\"Converts a cv2 imread to base 64\n",
    "\n",
    "  Args:\n",
    "    image: resultant value of cv2 imread \n",
    "  \"\"\"\n",
    "  _, buffer = cv2.imencode('.jpg', image)\n",
    "  jpg_as_text = base64.b64encode(buffer)\n",
    "  return jpg_as_text.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import cv2\n",
    "\n",
    "class Frame:\n",
    "  \"\"\"Container for holding an image along with any predictions\"\"\"\n",
    "  def __init__(self, jpg_file, predictions = []):\n",
    "    \"\"\"Args:\n",
    "      jpg_file: string path to a jpeg image\n",
    "      predictions: array of text files containing bounding box information for \n",
    "        this image\"\"\"\n",
    "    self.source = jpg_file\n",
    "    self.image = cv2.imread(self.source)\n",
    "    \n",
    "    #this line make JSON serialization difficult. Yolo(v5) to the consequences!\n",
    "    self.predictions = [Prediction(x, self) for x in predictions] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "class Prediction:\n",
    "  def __init__(self, txt_file=None, parent = None):\n",
    "    \"\"\"\n",
    "    Parses the first line of a txt file. Uses YOLOv5 format:\n",
    "\n",
    "    CLASS, XCENTER, YCENTER, WIDTH, HEIGHT, CONFIDENCE\n",
    "\n",
    "    Note that x&y axis have been normed to the interval 0-1. So expect CLASS\n",
    "    to be an `int` while the remaining values will be of type `float`.\n",
    "\n",
    "    In the event that a text file is not specified, all values will be set to -1.\n",
    "    \"\"\"\n",
    "    if txt_file:\n",
    "      self.txt_file = txt_file\n",
    "      s = open(txt_file, \"r\").read().split('\\n')[0]\n",
    "      self.original_txt = s\n",
    "      s = s.split(\" \")\n",
    "    else:\n",
    "      s = [-1]*6\n",
    "    self.class_label = s[0]\n",
    "    self.confidence = s[-1]\n",
    "    s = [float(x) for x in s[1:5]]\n",
    "    labels = \"xcent ycent width height\".split(\" \")\n",
    "    self.bbox = (dict(zip(labels, s))) #zooped\n",
    "    self.parent = parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def preds(i):\n",
    "  \"\"\"\n",
    "  Returns:\n",
    "    Extracts the array of predictions from model's results\n",
    "  \"\"\"\n",
    "  #legacy compatability\n",
    "  if \"digits\" in i.keys():\n",
    "    preds = i[\"digits\"][\"txt\"]\n",
    "  else:\n",
    "    preds = i[\"txt\"]\n",
    "  return preds\n",
    "\n",
    "class DataSet():\n",
    "  \"\"\"holds together a series of Frame objects with the eventual goal of exporting\n",
    "  the data contained therein to a JSON-friendly format\"\"\"\n",
    "  def __init__(self, image_list):\n",
    "    \"\"\"Highly tuned to the Detector class\n",
    "    \n",
    "    image_list: an array of frames processed by a Detector\"\"\"\n",
    "    self.frames = [Frame(jpg_file=image[\"preimage location\"], predictions=preds(image)) for image in image_list]\n",
    "\n",
    "  def export(self, base64Image=True):\n",
    "    \"\"\"\n",
    "    Exports all of the data I could access from detect.py\n",
    "\n",
    "    Args:\n",
    "      base64Image: very heavy string. If you don't need the image in base64, set to false\n",
    "\n",
    "    Returns:\n",
    "      JSON-friendly object (array of dictionaries (that themselves contain arrays :( )))\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for s in self.frames:\n",
    "      try:\n",
    "        tmp = {}\n",
    "        tmp[\"filename\"] = s.source\n",
    "        tmp[\"predictions\"] = []\n",
    "        if base64Image:\n",
    "          tmp['base64'] = draw_screen(s.image)\n",
    "        for prediction in s.predictions:\n",
    "          td = {}\n",
    "          td['bbox'] = prediction.bbox\n",
    "          td[\"class index\"] = prediction.class_label\n",
    "          td['confidence'] = prediction.confidence\n",
    "          tmp[\"predictions\"].append(td)\n",
    "        out.append(tmp)\n",
    "      except Exception as e:\n",
    "        print(f\"failed on {s.source} with error {e}\")\n",
    "    return out\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resolving conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "  \"Source: https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\"\n",
    "  # determine the (x, y)-coordinates of the intersection rectangle\n",
    "  xA = max(boxA[0], boxB[0])\n",
    "  yA = max(boxA[1], boxB[1])\n",
    "  xB = min(boxA[2], boxB[2])\n",
    "  yB = min(boxA[3], boxB[3])\n",
    "\n",
    "  # compute the area of intersection rectangle\n",
    "  interArea = (xB - xA) * (yB - yA)\n",
    "\n",
    "  # compute the area of both the prediction and ground-truth\n",
    "  # rectangles\n",
    "  boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "  boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "\n",
    "  # compute the intersection over union by taking the intersection\n",
    "  # area and dividing it by the sum of prediction + ground-truth\n",
    "  # areas - the interesection area\n",
    "  iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "  # return the intersection over union value\n",
    "  return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sorted_api(predictions):\n",
    "  \"\"\"sorts a list of prediction in ascending x order\n",
    "\n",
    "  Args:\n",
    "    an array of dictionaries containing bbox predictions\n",
    "\n",
    "  Returns:\n",
    "    the bounding boxes read off from left to right\"\"\"\n",
    "  return sorted(predictions, key=lambda x: x['x'])\n",
    "\n",
    "def digit_arr_to_string(predictions):\n",
    "  \"\"\"\"Human-friendly interpretation of predictions\"\"\"\n",
    "  return \"\".join(str(int(x['class'])%90) for x in sorted_api(predictions))\n",
    "\n",
    "\n",
    "def corners(r):\n",
    "  \"\"\" \n",
    "  Defines the relevante coordinate information\n",
    "\n",
    "  Args: \n",
    "    r: converts a DataFrame row to a tuple\n",
    "    \n",
    "  Returns:\n",
    "    Caretensian coordinate information (anchor in the lower left)\n",
    "  \"\"\"\n",
    "  return (r['x'], r['y'], r['x'] + r['width'], r['y'] + r['height'])\n",
    "\n",
    "def calculate_iou(frame, threshold = .4):\n",
    "  \"\"\"\n",
    "  Determines the pair of bounding boxes that has the highest IoU over the :threshold:\n",
    "\n",
    "  Args:\n",
    "    frame: the output of either a YOLOv5 model call or an API call\n",
    "    threshold: maximum percentage of overlap allowed between the bounding boxes\n",
    "  \"\"\"\n",
    "  previous = None\n",
    "  overlappers = None\n",
    "\n",
    "  for i in range(len(sorted_api(frame))):\n",
    "    row = sorted_api(frame)[i]\n",
    "    row[\"index\"] = i\n",
    "    if previous is None:\n",
    "      previous = row\n",
    "      continue\n",
    "\n",
    "    iou = bb_intersection_over_union(corners(previous), corners(row))\n",
    "    if iou > threshold:\n",
    "      tmp = {row[\"confidence\"]: row, previous[\"confidence\"]: previous}\n",
    "      overlappers = {\n",
    "        \"iou\": iou,\n",
    "        \"least confident\": tmp[min(tmp.keys())]\n",
    "      }\n",
    "\n",
    "    previous = row\n",
    "  return overlappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: the following functions work together to form the core of parsing bounding\n",
    "box information.\n",
    "\n",
    "> Warning: I assume that screens have no more than 4 objects in them.\n",
    "\n",
    "If extra bounding boxesare detected, I find the pair of boxes that has the highest IoU (Intersection \n",
    "over Union). Of that pair, I identify the box that has the a lower associated\n",
    "confidence and remove it.\n",
    "\n",
    "If there is still an excess of bounding boxes but no boxes that are overlapping\n",
    "in excess of the :threshold:, then bounding boxes with the most deviated y are \n",
    "removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_predictions(predictions_arr):\n",
    "  while len(predictions_arr) > 0 and len(predictions_arr) != len(remove_worst(predictions_arr)):\n",
    "    predictions_arr = remove_worst(predictions_arr)\n",
    "  return predictions_arr\n",
    "\n",
    "def remove_worst_offender(res, threshold):\n",
    "  overlappers = calculate_iou(res)\n",
    "  \n",
    "  if overlappers is not None:\n",
    "    included = [x for x in res if x[\"index\"] != overlappers[\"least confident\"][\"index\"]]\n",
    "    return included\n",
    "  else: #look for y outlier\n",
    "    df = pd.DataFrame.from_dict(res)\n",
    "    # print(df.sort_values(by=\"confidence\",ascending=False))\n",
    "    df[\"zscore\"] = abs(df.y - df.y.mean())/df.y.std() #FIXME\n",
    "    return df[df[\"zscore\"] < 1.5].to_dict('records')\n",
    "\n",
    "def remove_worst(res, threshold=.4):\n",
    "  return remove_worst_offender(res, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import PIL\n",
    "\n",
    "def yoloBox_to_standard(result):\n",
    "  \"\"\"\n",
    "  The standard YOLOv5 coordinate format is normed to 1. Need to extract the \n",
    "  original's image width and height to convert to a standard cartesian plane.\n",
    "\n",
    "  Args:\n",
    "    result: a dictionary that includes both \n",
    "        * a key called \"filename\" that points to the original image\n",
    "        * a key called \"predictions\" created when the image is parsed with the\n",
    "          YOLOv5 model \n",
    "\n",
    "  Returns:\n",
    "    Convert the predictions converted to a full-scale Cartesian coordinate system.\n",
    "  \"\"\"\n",
    "  out = {}\n",
    "  for k,v in result.items():\n",
    "    out[k] = v\n",
    "\n",
    "  PILim= PIL.Image.open(result[\"preimage location\"])\n",
    "  width, height = PILim.width, PILim.height\n",
    "\n",
    "  out[\"predictions\"] = []\n",
    "  for prediction in result[\"predictions\"]:\n",
    "    bbox = prediction[\"bbox\"]\n",
    "    out[\"predictions\"].append({\n",
    "        \"class\": (int(prediction[\"class index\"])+1)%10,\n",
    "        \"confidence\": prediction[\"confidence\"],\n",
    "        \"height\": PILim.height*bbox[\"height\"],\n",
    "        \"width\": PILim.width*bbox[\"width\"],\n",
    "        \"x\": PILim.width*(bbox[\"xcent\"] - bbox[\"width\"]/2),\n",
    "        \"y\": PILim.height*(bbox[\"ycent\"] - bbox[\"height\"]/2)\n",
    "        })\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import io, PIL, base64\n",
    "\n",
    "def raw_parse_from_json(json_elements, loaded_detector):\n",
    "  \"\"\"\n",
    "  Helper to interpret raw base 64 images. Creates a new JPG file from the base64\n",
    "  data and feeds the image to the Detector\n",
    "\n",
    "  Args:\n",
    "    json_elements: array of dictionaries from file. Assert existence of a \"base64\"\n",
    "      key where the images are stored\n",
    "    loaded_detector: a valid Detector\n",
    "  \n",
    "  returns:\n",
    "    the result of processessing all of the base64 images\n",
    "  \"\"\"\n",
    "  out = []\n",
    "  for idx in range(len(json_elements)):\n",
    "    from64name = _itername(\"(\", \") image.jpg\")\n",
    "    b64image = json_elements[idx][\"base64\"]\n",
    "    im = PIL.Image.open(io.BytesIO(base64.b64decode(b64image)))\n",
    "    im.save(from64name)\n",
    "    \n",
    "    single = loaded_detector.process_image(from64name)\n",
    "    single[\"preimage location\"] = from64name\n",
    "    out.append(single)\n",
    "  return out\n",
    "\n",
    "def parse_from_json(json_elements, loaded_detector):\n",
    "  \"\"\"\n",
    "  Gets a JSON file containing the base64 images of data to label pushed through\n",
    "  a Detector. Post-processing automatically de-norms the YOLOv5 bounding boxes \n",
    "  based on the original image's height and width\n",
    "\n",
    "  Args:\n",
    "    json_elements: array of dictionaries from file. Assert existence of a \"base64\"\n",
    "      key where the images are stored\n",
    "    loaded_detector: a valid Detector\n",
    "  \n",
    "  returns:\n",
    "    the bounding boxes associated with each image\n",
    "  \"\"\"\n",
    "  preprocessed = raw_parse_from_json(json_elements, loaded_detector)\n",
    "  ds = DataSet(preprocessed)\n",
    "  processed=ds.export()\n",
    "\n",
    "  for i in range(len(processed)):\n",
    "    processed[i][\"preimage location\"] = processed[i][\"filename\"]\n",
    "\n",
    "  postprocess = []\n",
    "  for x in processed:\n",
    "    postprocess.append(yoloBox_to_standard(x))\n",
    "  return postprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Environment\n",
    "\n",
    "My weights and images are stored in my Google Drive. You may need to configure your settings differently according to your specific setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "the weights and images are stored my Google Drive\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Coding/ModelAssistedLabel\n"
     ]
    }
   ],
   "source": [
    "%cd \"/content/drive/MyDrive/Coding/ModelAssistedLabel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Coding/ModelAssistedLabel\n",
      "install nbdev: ye\n",
      "git commit -m: hunting the checkmark1\n",
      "Converted 00_config.ipynb.\n",
      "Converted 01_split.ipynb.\n",
      "Converted 02_train.ipynb.\n",
      "Converted index.ipynb.\n",
      "converting: /content/drive/My Drive/Coding/ModelAssistedLabel/index.ipynb\n",
      "converting /content/drive/My Drive/Coding/ModelAssistedLabel/index.ipynb to README.md\n",
      "Executing: git config --local include.path ../.gitconfig\n",
      "Success: hooks are installed and repo's .gitconfig is now trusted\n",
      "\n",
      "[master 27e8c0d] hunting the checkmark1\n",
      " 1 file changed, 13 insertions(+), 13 deletions(-)\n",
      "Counting objects: 3, done.\n",
      "Delta compression using up to 2 threads.\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 511 bytes | 170.00 KiB/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/PhilBrockman/ModelAssistedLabel.git\n",
      "   5fe1621..27e8c0d  master -> master\n",
      "Branch 'master' set up to track remote branch 'master' from 'origin'.\n"
     ]
    }
   ],
   "source": [
    "%run \"_Synch.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to send curl command\n",
    "#@title Point me to some local paths\n",
    "\n",
    "# trained with YOLOv5 train.py\n",
    "#@markdown file location where the model's weights are being stored\n",
    "weights_filename = '21-2-25 1k-digits YOLOv5-weights.pt' #@param {type: \"string\"}\n",
    "\n",
    "# a JSON array of dictionaries. Assumes images are stored in the base64 format\n",
    "# in the dictionary's key called \"base64\"\n",
    "#@markdown file location to the JSON file that has image to be evaluated. \n",
    "#@markdown [See more](https://gist.github.com/PhilBrockman/776a86a16d93038294b0fd8c1be63582?short_path=a22f722)\n",
    "#@markdown about how I am generating JSON from images\n",
    "base64_images = \"./additional lcd images/21-3-3 control-camera-light.json\" #@param {type: \"string\"}\n",
    "\n",
    "\n",
    "#@markdown `curl` location & access token is defined by Roboflow API \n",
    "RUN_ROBOFLOW_API = \"False\" #@param [\"False\", \"True\"]\n",
    "URL_WITH_ACCESS_TOKEN = \"https://infer.roboflow.com/<DB}>access_token=<API_TOKEN>\" #@param {type: \"string\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interact with Models\n",
    "\n",
    "Data has been loaded. Weights are findable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pathway to the weights\n",
    "resource_folder = \"/content/drive/MyDrive/Coding/Roboflow/try it out\"\n",
    "weights_path = os.path.join(resource_folder, weights_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "# load a model based on pre-calculated weights \n",
    "digitDetector = Detector(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the json in the **base64_images** path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the base64 images into memory\n",
    "with open(os.path.join(resource_folder, base64_images)) as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a convenience sample of the dataset.\n",
    "import random\n",
    "data = random.sample(data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /content/yolov5/(1) image.jpg: image 1/1 /content/yolov5/(2) image.jpg: image 1/1 /content/yolov5/(3) image.jpg: image 1/1 /content/yolov5/(4) image.jpg: image 1/1 /content/yolov5/(5) image.jpg: image 1/1 /content/yolov5/(6) image.jpg: image 1/1 /content/yolov5/(7) image.jpg: image 1/1 /content/yolov5/(8) image.jpg: image 1/1 /content/yolov5/(9) image.jpg: image 1/1 /content/yolov5/(10) image.jpg: image 1/1 /content/yolov5/(11) image.jpg: image 1/1 /content/yolov5/(12) image.jpg: image 1/1 /content/yolov5/(13) image.jpg: image 1/1 /content/yolov5/(14) image.jpg: image 1/1 /content/yolov5/(15) image.jpg: image 1/1 /content/yolov5/(16) image.jpg: image 1/1 /content/yolov5/(17) image.jpg: image 1/1 /content/yolov5/(18) image.jpg: image 1/1 /content/yolov5/(19) image.jpg: image 1/1 /content/yolov5/(20) image.jpg: image 1/1 /content/yolov5/(21) image.jpg: image 1/1 /content/yolov5/(22) image.jpg: image 1/1 /content/yolov5/(23) image.jpg: image 1/1 /content/yolov5/(24) image.jpg: image 1/1 /content/yolov5/(25) image.jpg: image 1/1 /content/yolov5/(26) image.jpg: image 1/1 /content/yolov5/(27) image.jpg: image 1/1 /content/yolov5/(28) image.jpg: image 1/1 /content/yolov5/(29) image.jpg: image 1/1 /content/yolov5/(30) image.jpg: image 1/1 /content/yolov5/(31) image.jpg: image 1/1 /content/yolov5/(32) image.jpg: image 1/1 /content/yolov5/(33) image.jpg: image 1/1 /content/yolov5/(34) image.jpg: image 1/1 /content/yolov5/(35) image.jpg: image 1/1 /content/yolov5/(36) image.jpg: image 1/1 /content/yolov5/(37) image.jpg: image 1/1 /content/yolov5/(38) image.jpg: image 1/1 /content/yolov5/(39) image.jpg: image 1/1 /content/yolov5/(40) image.jpg: image 1/1 /content/yolov5/(41) image.jpg: image 1/1 /content/yolov5/(42) image.jpg: image 1/1 /content/yolov5/(43) image.jpg: image 1/1 /content/yolov5/(44) image.jpg: image 1/1 /content/yolov5/(45) image.jpg: image 1/1 /content/yolov5/(46) image.jpg: image 1/1 /content/yolov5/(47) image.jpg: image 1/1 /content/yolov5/(48) image.jpg: image 1/1 /content/yolov5/(49) image.jpg: image 1/1 /content/yolov5/(50) image.jpg: image 1/1 /content/yolov5/(51) image.jpg: image 1/1 /content/yolov5/(52) image.jpg: image 1/1 /content/yolov5/(53) image.jpg: image 1/1 /content/yolov5/(54) image.jpg: image 1/1 /content/yolov5/(55) image.jpg: image 1/1 /content/yolov5/(56) image.jpg: image 1/1 /content/yolov5/(57) image.jpg: image 1/1 /content/yolov5/(58) image.jpg: image 1/1 /content/yolov5/(59) image.jpg: image 1/1 /content/yolov5/(60) image.jpg: image 1/1 /content/yolov5/(61) image.jpg: image 1/1 /content/yolov5/(62) image.jpg: image 1/1 /content/yolov5/(63) image.jpg: image 1/1 /content/yolov5/(64) image.jpg: image 1/1 /content/yolov5/(65) image.jpg: image 1/1 /content/yolov5/(66) image.jpg: image 1/1 /content/yolov5/(67) image.jpg: image 1/1 /content/yolov5/(68) image.jpg: image 1/1 /content/yolov5/(69) image.jpg: image 1/1 /content/yolov5/(70) image.jpg: image 1/1 /content/yolov5/(71) image.jpg: image 1/1 /content/yolov5/(72) image.jpg: image 1/1 /content/yolov5/(73) image.jpg: image 1/1 /content/yolov5/(74) image.jpg: image 1/1 /content/yolov5/(75) image.jpg: image 1/1 /content/yolov5/(76) image.jpg: image 1/1 /content/yolov5/(77) image.jpg: image 1/1 /content/yolov5/(78) image.jpg: image 1/1 /content/yolov5/(79) image.jpg: image 1/1 /content/yolov5/(80) image.jpg: image 1/1 /content/yolov5/(81) image.jpg: image 1/1 /content/yolov5/(82) image.jpg: image 1/1 /content/yolov5/(83) image.jpg: image 1/1 /content/yolov5/(84) image.jpg: image 1/1 /content/yolov5/(85) image.jpg: image 1/1 /content/yolov5/(86) image.jpg: image 1/1 /content/yolov5/(87) image.jpg: image 1/1 /content/yolov5/(88) image.jpg: image 1/1 /content/yolov5/(89) image.jpg: image 1/1 /content/yolov5/(90) image.jpg: image 1/1 /content/yolov5/(91) image.jpg: image 1/1 /content/yolov5/(92) image.jpg: image 1/1 /content/yolov5/(93) image.jpg: image 1/1 /content/yolov5/(94) image.jpg: image 1/1 /content/yolov5/(95) image.jpg: image 1/1 /content/yolov5/(96) image.jpg: image 1/1 /content/yolov5/(97) image.jpg: image 1/1 /content/yolov5/(98) image.jpg: image 1/1 /content/yolov5/(99) image.jpg: image 1/1 /content/yolov5/(100) image.jpg: image 1/1 /content/yolov5/(101) image.jpg: image 1/1 /content/yolov5/(102) image.jpg: image 1/1 /content/yolov5/(103) image.jpg: image 1/1 /content/yolov5/(104) image.jpg: image 1/1 /content/yolov5/(105) image.jpg: image 1/1 /content/yolov5/(106) image.jpg: image 1/1 /content/yolov5/(107) image.jpg: image 1/1 /content/yolov5/(108) image.jpg: image 1/1 /content/yolov5/(109) image.jpg: image 1/1 /content/yolov5/(110) image.jpg: image 1/1 /content/yolov5/(111) image.jpg: image 1/1 /content/yolov5/(112) image.jpg: image 1/1 /content/yolov5/(113) image.jpg: image 1/1 /content/yolov5/(114) image.jpg: image 1/1 /content/yolov5/(115) image.jpg: image 1/1 /content/yolov5/(116) image.jpg: image 1/1 /content/yolov5/(117) image.jpg: image 1/1 /content/yolov5/(118) image.jpg: image 1/1 /content/yolov5/(119) image.jpg: image 1/1 /content/yolov5/(120) image.jpg: image 1/1 /content/yolov5/(121) image.jpg: image 1/1 /content/yolov5/(122) image.jpg: image 1/1 /content/yolov5/(123) image.jpg: image 1/1 /content/yolov5/(124) image.jpg: image 1/1 /content/yolov5/(125) image.jpg: image 1/1 /content/yolov5/(126) image.jpg: image 1/1 /content/yolov5/(127) image.jpg: image 1/1 /content/yolov5/(128) image.jpg: image 1/1 /content/yolov5/(129) image.jpg: image 1/1 /content/yolov5/(130) image.jpg: image 1/1 /content/yolov5/(131) image.jpg: image 1/1 /content/yolov5/(132) image.jpg: image 1/1 /content/yolov5/(133) image.jpg: image 1/1 /content/yolov5/(134) image.jpg: image 1/1 /content/yolov5/(135) image.jpg: image 1/1 /content/yolov5/(136) image.jpg: image 1/1 /content/yolov5/(137) image.jpg: image 1/1 /content/yolov5/(138) image.jpg: image 1/1 /content/yolov5/(139) image.jpg: image 1/1 /content/yolov5/(140) image.jpg: image 1/1 /content/yolov5/(141) image.jpg: image 1/1 /content/yolov5/(142) image.jpg: image 1/1 /content/yolov5/(143) image.jpg: image 1/1 /content/yolov5/(144) image.jpg: image 1/1 /content/yolov5/(145) image.jpg: image 1/1 /content/yolov5/(146) image.jpg: image 1/1 /content/yolov5/(147) image.jpg: image 1/1 /content/yolov5/(148) image.jpg: image 1/1 /content/yolov5/(149) image.jpg: image 1/1 /content/yolov5/(150) image.jpg: image 1/1 /content/yolov5/(151) image.jpg: image 1/1 /content/yolov5/(152) image.jpg: image 1/1 /content/yolov5/(153) image.jpg: image 1/1 /content/yolov5/(154) image.jpg: image 1/1 /content/yolov5/(155) image.jpg: image 1/1 /content/yolov5/(156) image.jpg: image 1/1 /content/yolov5/(157) image.jpg: image 1/1 /content/yolov5/(158) image.jpg: image 1/1 /content/yolov5/(159) image.jpg: image 1/1 /content/yolov5/(160) image.jpg: image 1/1 /content/yolov5/(161) image.jpg: image 1/1 /content/yolov5/(162) image.jpg: image 1/1 /content/yolov5/(163) image.jpg: image 1/1 /content/yolov5/(164) image.jpg: image 1/1 /content/yolov5/(165) image.jpg: image 1/1 /content/yolov5/(166) image.jpg: image 1/1 /content/yolov5/(167) image.jpg: image 1/1 /content/yolov5/(168) image.jpg: image 1/1 /content/yolov5/(169) image.jpg: image 1/1 /content/yolov5/(170) image.jpg: image 1/1 /content/yolov5/(171) image.jpg: image 1/1 /content/yolov5/(172) image.jpg: image 1/1 /content/yolov5/(173) image.jpg: image 1/1 /content/yolov5/(174) image.jpg: image 1/1 /content/yolov5/(175) image.jpg: image 1/1 /content/yolov5/(176) image.jpg: image 1/1 /content/yolov5/(177) image.jpg: image 1/1 /content/yolov5/(178) image.jpg: image 1/1 /content/yolov5/(179) image.jpg: image 1/1 /content/yolov5/(180) image.jpg: image 1/1 /content/yolov5/(181) image.jpg: image 1/1 /content/yolov5/(182) image.jpg: image 1/1 /content/yolov5/(183) image.jpg: image 1/1 /content/yolov5/(184) image.jpg: image 1/1 /content/yolov5/(185) image.jpg: image 1/1 /content/yolov5/(186) image.jpg: image 1/1 /content/yolov5/(187) image.jpg: image 1/1 /content/yolov5/(188) image.jpg: image 1/1 /content/yolov5/(189) image.jpg: image 1/1 /content/yolov5/(190) image.jpg: image 1/1 /content/yolov5/(191) image.jpg: image 1/1 /content/yolov5/(192) image.jpg: image 1/1 /content/yolov5/(193) image.jpg: image 1/1 /content/yolov5/(194) image.jpg: image 1/1 /content/yolov5/(195) image.jpg: image 1/1 /content/yolov5/(196) image.jpg: image 1/1 /content/yolov5/(197) image.jpg: image 1/1 /content/yolov5/(198) image.jpg: image 1/1 /content/yolov5/(199) image.jpg: image 1/1 /content/yolov5/(200) image.jpg: "
     ]
    }
   ],
   "source": [
    "# parse the base64 data and run through the YOLOv5 model\n",
    "postprocess=parse_from_json( data, loaded_detector=digitDetector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If **RUN_ROBOFLOW_API** is `True`, then the images from the processed JSON will be `curl`ed to **URL_WITH_ACCESS_TOKEN** \n",
    "\n",
    "If **RUN_ROBOFLOW_API** is `False`, then the predictions will uniformly be `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfurl_curl_predictions(filename):\n",
    "  \"\"\"\n",
    "  Queries the Roboflow API at :URL_WITH_ACCESS_TOKEN: with a chosen image. \n",
    "\n",
    "  Args:\n",
    "    filename: path to the image to parse through the API\n",
    "\n",
    "  Returns:\n",
    "    a list of predictions if a valide URL. Otherwise returns an empty list.\n",
    "  \"\"\"\n",
    "  if RUN_ROBOFLOW_API == \"True\":\n",
    "    r = !base64 \"{filename}\" | curl -d @- \"{URL_WITH_ACCESS_TOKEN}\"\n",
    "    return json.loads(r[0])[\"predictions\"]\n",
    "  else:\n",
    "    return []\n",
    "\n",
    "# for each image, collect the predictions made by API\n",
    "api_res = []\n",
    "for i in range(0, len(postprocess)):\n",
    "  api_res.append(unfurl_curl_predictions(postprocess[i][\"filename\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disagree: 180', 'agree: 20']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yolo</th>\n",
       "      <th>yolo original</th>\n",
       "      <th>api</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>029</td>\n",
       "      <td>029</td>\n",
       "      <td>4029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>029</td>\n",
       "      <td>029</td>\n",
       "      <td>9029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>029</td>\n",
       "      <td>029</td>\n",
       "      <td>4029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>029</td>\n",
       "      <td>029</td>\n",
       "      <td>9029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3031</td>\n",
       "      <td>3031</td>\n",
       "      <td>9031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>032</td>\n",
       "      <td>0232</td>\n",
       "      <td>9032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>033</td>\n",
       "      <td>033</td>\n",
       "      <td>9033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4035</td>\n",
       "      <td>4035</td>\n",
       "      <td>9035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>036</td>\n",
       "      <td>036</td>\n",
       "      <td>9036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4037</td>\n",
       "      <td>4037</td>\n",
       "      <td>4037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   yolo yolo original   api\n",
       "0   029           029  4029\n",
       "1   029           029  9029\n",
       "2   029           029  4029\n",
       "3   029           029  9029\n",
       "4  3031          3031  9031\n",
       "5   032          0232  9032\n",
       "6   033           033  9033\n",
       "7  4035          4035  9035\n",
       "8   036           036  9036\n",
       "9  4037          4037  4037"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify images where the two models agree and where they disagree\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#determine the YOLOv5 predictions\n",
    "df[\"yolo\"] = [digit_arr_to_string(interpret_predictions(x[\"predictions\"])) for x in postprocess]\n",
    "\n",
    "#capture the original YOLOv5 predictions\n",
    "df[\"yolo original\"] = [digit_arr_to_string((x[\"predictions\"])) for x in postprocess]\n",
    "\n",
    "\n",
    "if RUN_ROBOFLOW_API == \"True\":\n",
    "  #determine the API's predictions\n",
    "  df[\"api\"] = [digit_arr_to_string(interpret_predictions(x)) for x in api_res]\n",
    "\n",
    "  named_subsets = {\n",
    "    \"disagree\": df[df[\"yolo\"] != df[\"api\"]], #do the models come to the same conclusion?\n",
    "    \"agree\": df[df[\"yolo\"] == df[\"api\"]] #is there a difference?\n",
    "  } \n",
    "\n",
    "  # get a rough sense for the numbers involved\n",
    "  print([x + \": \" + str(len(named_subsets[x])) for x in named_subsets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through a dataframe and view a sampling of the predictions \n",
    "# along with the original\n",
    "\n",
    "for i,row in df.sample(5).iterrows():\n",
    "  print(\"index\", i)\n",
    "  print(\"yolo:\", row[\"yolo\"], \"<<<\", row[\"yolo original\"])\n",
    "  if RUN_ROBOFLOW_API == \"True\":\n",
    "    print(\"api: \", row[\"api\"])\n",
    "  display(PIL.Image.open(postprocess[int(i)][\"filename\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the checkbox is unmarked, no file will be created, no file will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed data (including YOLOv5 and API output) to JSON\n",
    "outfilename = \"YY-MM-DD save results.json\" #@param {type:\"string\"}\n",
    "save = False #@param {type:\"boolean\"}\n",
    "\n",
    "if save:\n",
    "  outJSON = {}\n",
    "  outJSON[\"processed\"] = postprocess\n",
    "  outJSON[\"df\"] = df.to_dict()\n",
    "  with open(outfilename, \"w\") as outfile:\n",
    "    json.dump(outJSON, outfile)\n",
    "\n",
    "  # download the JSON for safe-keeping\n",
    "  from google.colab import files\n",
    "  files.download(outfilename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
