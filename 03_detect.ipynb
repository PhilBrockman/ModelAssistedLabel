{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from ModelAssistedLabel.config import Defaults\n",
    "Defaults.to_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Objects\n",
    "> wrapping `yolov5/detect.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os, torch\n",
    "os.chdir(\"yolov5\")\n",
    "from pathlib import Path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.general import check_img_size, non_max_suppression\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import scale_coords, xyxy2xywh\n",
    "from utils.torch_utils import select_device\n",
    "from models.experimental import attempt_load\n",
    "import cv2, os, base64, json\n",
    "os.chdir(\"..\")\n",
    "\n",
    "os.system(\"pip install pillow\")\n",
    "\n",
    "class Detector:\n",
    "  \"\"\"A wrapper for training loading saved YOLOv5 weights\n",
    "  \n",
    "  requirements:\n",
    "    GPU enabled\"\"\"\n",
    "  def __init__(self, weight_path, conf_threshold = .4, iou_threshold = .45, imgsz = 416, save_dir=\"save_dir\", write_annotated_images_to_disk=False):\n",
    "    \"\"\" Constructor. I pulled the default numeric values above directly from the\n",
    "    detect.py file. I added the option to save model output to both images and \n",
    "    to txt files\n",
    "\n",
    "    Args:\n",
    "      weight_path: the path to which the saved weights are stored\n",
    "      conf_threshold: lower bound on the acceptable level of uncertainty for a \n",
    "                      bounding box\n",
    "      iou_threshold: IoU helps determine how overlapped two shapes are.\n",
    "      imgsz: resolution of image to process (assumes square)\n",
    "      save_dir: where to write annotated images\n",
    "      write_annotated_images_to_disk: save human-friendly annotated images to disk \n",
    "    \"\"\"\n",
    "    \n",
    "    self.weight_path = weight_path\n",
    "    self.conf_threshold = conf_threshold\n",
    "    self.iou_threshold = iou_threshold\n",
    "    self.imgsz = imgsz\n",
    "    self.device = select_device()\n",
    "    self.model = attempt_load(self.weight_path, map_location=self.device)  # load FP32 model\n",
    "    self.names = self.model.module.names if hasattr(self.model, 'module') else self.model.names\n",
    "    self.imgsz = check_img_size(self.imgsz, s=self.model.stride.max())  # check img_size\n",
    "    self.half = self.device.type != 'cuda'\n",
    "    if self.half:\n",
    "      self.model.half()  # to FP16\n",
    "\n",
    "    self.write_annotated_images_to_disk = write_annotated_images_to_disk\n",
    "    self.save_dir = save_dir\n",
    "\n",
    "  def make_dir(dir):\n",
    "    \"\"\"makes a directory provided that the directiory doesn't already exist\n",
    "    \n",
    "    Args:\n",
    "      dir: Directory to create a path towards\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dir):\n",
    "      os.makedirs(dir)\n",
    "\n",
    "  def process_image(self, source):\n",
    "    \"\"\"Runs on the model with pre-specified weights an input image. See original\n",
    "    detect.py for more details\n",
    "\n",
    "    Args:\n",
    "      source: A string path to pre-specified weights for the model\n",
    "      save_unscuffed: create copy of the pre-image\n",
    "\n",
    "    Reurns:\n",
    "      A JSON-serializable object encoding bounding box information\n",
    "    \"\"\"\n",
    "    override = None\n",
    "    if os.path.exists(self.save_dir):\n",
    "      override = input(f\"Save directory '{self.save_dir}' exists. \\n 'Enter' to continue, anything else to cancel operation\")\n",
    "\n",
    "    if override is None or len(override) == 0:\n",
    "      return self.__process_image__(source)\n",
    "\n",
    "    assert False, \"this code shouldn't run\"\n",
    "\n",
    "\n",
    "  #not saving images speed up processing dramatically\n",
    "  def __process_image__(self, source):\n",
    "    \"helper for process_image\"\n",
    "    results = []\n",
    "    img = torch.zeros((1, 3, self.imgsz, self.imgsz), device=self.device)  # init img\n",
    "    _ = self.model(img.half() if self.half else img) if self.device.type != 'cpu' else None  # run once\n",
    "    dataset = LoadImages(source, img_size=self.imgsz)\n",
    "\n",
    "    if self.write_annotated_images_to_disk:\n",
    "      save_dir = Path(self.save_dir)\n",
    "      Detector.make_dir(save_dir)\n",
    "\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "      tmp = {}\n",
    "      tmp[\"predictions\"] = []\n",
    "\n",
    "      img = torch.from_numpy(img).to(self.device)\n",
    "      img = img.half() if self.half else img.float()  # uint8 to fp16/32\n",
    "      img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "      \n",
    "      if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "      pred = self.model(img, augment=False)[0]\n",
    "      pred = non_max_suppression(pred, self.conf_threshold, self.iou_threshold, agnostic=False)\n",
    "      \n",
    "      for i, det in enumerate(pred):\n",
    "        p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
    "        p = Path(p) \n",
    "        \n",
    "        s += '%gx%g ' % img.shape[2:]  # print string\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "        if len(det):\n",
    "          det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "          for c in det[:, -1].unique():\n",
    "              n = (det[:, -1] == c).sum()  # detections per class\n",
    "              s += f'{n} {self.names[int(c)]}s, '  # add to string\n",
    "          \n",
    "          if self.write_annotated_images_to_disk:\n",
    "            tmp[\"unscuffed\"] = f\"{save_dir}/unscuffed-{p.name}\"\n",
    "            cv2.imwrite(tmp[\"unscuffed\"], im0)\n",
    "\n",
    "          for *xyxy, conf, cls in reversed(det):\n",
    "            xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "            line = (cls, *xywh, conf) # label format\n",
    "            tmp[\"predictions\"].append(('%g ' * len(line)).rstrip() % line)\n",
    "\n",
    "            # save image with bboxes drawn on top\n",
    "            if self.write_annotated_images_to_disk: \n",
    "              label = f'{self.names[int(cls)]} {conf:.2f}'\n",
    "              plot_one_box(xyxy, im0, label=label, color=[0,0,200], line_thickness=5)\n",
    "              tmp[\"labeled\"] = f\"{save_dir}/labeled-{p.name}\"\n",
    "              cv2.imwrite(tmp[\"labeled\"], im0)\n",
    "          results.append(tmp)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = 'pre-trained weights/21-2-25 1k-digits YOLOv5-weights.pt'\n",
    "\n",
    "repos = []\n",
    "repos.append(\"Image Repo/unlabeled/21-3-18 rowing 8-12 /\")\n",
    "repos.append(\"Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00\")\n",
    "repos.append(\"Image Repo/unlabeled/21-3-22 rowing (200) 7:50-12:50\")\n",
    "\n",
    "dirs = []\n",
    "for repo in repos:\n",
    "  files = os.listdir(repo)\n",
    "  absolute_paths = [os.path.join(repo, file) for file in files]\n",
    "  dirs.append(absolute_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "detector = Detector(weight_path=weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /content/drive/My Drive/Coding/ModelAssistedLabel/Image Repo/unlabeled/21-3-18 rowing 8-12 /100.jpg: 0 | [{'predictions': ['4 0.329688 0.385417 0.021875 0.0902778 0.809661', '2 0.275781 0.375694 0.0203125 0.0902778 0.821206', '7 0.358594 0.388889 0.021875 0.0888889 0.854578']}]\n",
      "image 1/1 /content/drive/My Drive/Coding/ModelAssistedLabel/Image Repo/unlabeled/21-3-18 rowing 8-12 /101.jpg: 1 | [{'predictions': ['9 0.329297 0.384722 0.0210938 0.0861111 0.655793', '2 0.275391 0.375694 0.0210938 0.0902778 0.687569', '9 0.357812 0.392361 0.0234375 0.0875 0.749733']}]\n",
      "image 1/1 /content/drive/My Drive/Coding/ModelAssistedLabel/Image Repo/unlabeled/21-3-18 rowing 8-12 /102.jpg: 2 | [{'predictions': ['6 0.366016 0.390278 0.0164062 0.0861111 0.723955', '2 0.275391 0.375 0.0195312 0.0888889 0.786876', '9 0.331641 0.384722 0.0210938 0.0861111 0.851035']}]\n",
      "image 1/1 /content/drive/My Drive/Coding/ModelAssistedLabel/Image Repo/unlabeled/21-3-18 rowing 8-12 /103.jpg: 3 | [{'predictions': ['2 0.275781 0.375 0.01875 0.0861111 0.825081', '9 0.330078 0.385417 0.0226563 0.0875 0.87784', '2 0.360156 0.390278 0.0203125 0.0888889 0.895363']}]\n",
      "image 1/1 /content/drive/My Drive/Coding/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/19.jpg: 4 | []\n",
      "image 1/1 /content/drive/My Drive/Coding/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/6.jpg: 5 | []\n",
      "image 1/1 /content/drive/My Drive/Coding/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/25.jpg: 6 | [{'predictions': ['3 0.827734 0.869444 0.0351562 0.122222 0.50515']}]\n",
      "image 1/1 /content/drive/My Drive/Coding/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/9.jpg: 7 | []\n",
      "image 1/1 /content/drive/My Drive/Coding/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 7:50-12:50/7.jpg: 8 | [{'predictions': ['0 0.441797 0.385417 0.0148437 0.0736111 0.849962', '9 0.412891 0.378472 0.0226563 0.0791667 0.858775', '9 0.364453 0.372222 0.0226563 0.0833333 0.8981', '7 0.389062 0.375694 0.021875 0.0791667 0.932286']}]\n",
      "image 1/1 /content/drive/My Drive/Coding/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 7:50-12:50/0.jpg: 9 | [{'predictions': ['4 0.411719 0.378472 0.021875 0.0819444 0.866474', '9 0.365234 0.371528 0.0226563 0.0819444 0.889749', '9 0.436719 0.381944 0.0234375 0.075 0.911735']}]\n",
      "image 1/1 /content/drive/My Drive/Coding/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 7:50-12:50/4.jpg: 10 | [{'predictions': ['6 0.394922 0.372917 0.0164062 0.0763889 0.435396', '4 0.411328 0.377083 0.0210938 0.0791667 0.895521', '9 0.365625 0.371528 0.021875 0.0791667 0.896913', '5 0.436328 0.379861 0.0210938 0.0763889 0.917066']}]\n",
      "image 1/1 /content/drive/My Drive/Coding/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 7:50-12:50/12.jpg: 11 | [{'predictions': ['9 0.412109 0.377778 0.0226563 0.0805556 0.842007', '9 0.364844 0.372222 0.0234375 0.0861111 0.853243', '7 0.435937 0.383333 0.0234375 0.0805556 0.907213', '7 0.388672 0.375 0.0226563 0.0833333 0.929839']}]\n"
     ]
    }
   ],
   "source": [
    "samples = dirs[0][3:7] + dirs[1][3:7] + dirs[2][3:7]\n",
    "for i in range(len(samples)):\n",
    "  res = detector.process_image(samples[i])\n",
    "  print(i, \"|\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human-friendly Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human-readable information about the class indentities is stored in the data.yaml folder. By default, the data.yaml file is created from the `Defaults` class. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: ../train/images\n",
      "val: ../valid/images\n",
      "\n",
      "nc: 10\n",
      "names: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']\n"
     ]
    }
   ],
   "source": [
    "from ModelAssistedLabel.core import Defaults\n",
    "print(Defaults().data_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the \"names\" variable to a python-friendly format, we do the following manipulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, re\n",
    "\n",
    "# needs to be wrapped in quotes to parse as dict\n",
    "substitute = \"names\"\n",
    "\n",
    "#select last line\n",
    "classlist = Defaults().data_yaml.split(\"\\n\")[-1]\n",
    "\n",
    "#add quotes around `names` ONLY around the start of a string\n",
    "classlist = re.sub('^%s' % substitute, f\"'{substitute}'\", classlist)\n",
    "\n",
    "#surround the string in curly braces to tell python it's a dict\n",
    "classlist = f\"{{{classlist}}}\"\n",
    "\n",
    "# parse string as dict\n",
    "classlist = ast.literal_eval(classlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now here is the value of the classes as used by yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'names': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the YOLOv5 Output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ModelAssistedLabel.detect import Detector\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "import PIL\n",
    "\n",
    "class Viewer:\n",
    "  \"\"\" Connects a set of pre-trained weights to an image. Also incorporates \n",
    "  the human-friendly class labels, as opposed to dealing with the label's index\n",
    "  \"\"\"\n",
    "  def __init__(self, weight_path, class_arr):\n",
    "    \"\"\"\n",
    "    constructor builds the detectors (may be relatively time-intensive) and stores \n",
    "    the targets\n",
    "\n",
    "    Args:\n",
    "      weights_paths: an array of paths to weights\n",
    "    \"\"\"\n",
    "    self.detector = Detector(weight_path=weight_path)\n",
    "    self.class_arr = class_arr\n",
    "    self.last_result = []\n",
    "  \n",
    "  def plot_for(self, image, show_labels=True, figsize=(20,10)):\n",
    "    \"\"\" (temporarily) overlay predictions onto the image\n",
    "\n",
    "    Args:\n",
    "      image: path to image\n",
    "\n",
    "    Returns: metadata in the form of an array of dicts \n",
    "    \"\"\"\n",
    "    predictions = self.predict_for(image)[\"predictions\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    #open/display the image\n",
    "    im0 = PIL.Image.open(image)\n",
    "    ax.imshow(im0)\n",
    "\n",
    "    for prediction in predictions:\n",
    "      # Create a Rectangle patch\n",
    "      rect = patches.Rectangle((prediction['x'], prediction['y']), prediction['width'], prediction['height'], linewidth=1, edgecolor='r', facecolor='none')\n",
    "      if show_labels:\n",
    "        ax.annotate(prediction[\"class\"], xy=(prediction['x'], prediction['y']-10), color='r', fontsize=20)\n",
    "      # Add the patch to the Axes\n",
    "      ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "    return predictions\n",
    "\n",
    "    \n",
    "  def predict_for(self, image):\n",
    "    \"\"\"\n",
    "    The standard YOLOv5 coordinate format is normed to 1. Need to extract the \n",
    "    original's image width and height to convert to a standard cartesian plane.\n",
    "\n",
    "    Args:\n",
    "      image: path to image\n",
    "\n",
    "    Returns:\n",
    "      Convert the predictions converted to a full-scale Cartesian coordinate system.\n",
    "    \"\"\"\n",
    "    assert os.path.exists(image)\n",
    "    #initialize return\n",
    "    out = {}\n",
    "    out[\"image path\"] = image\n",
    "    out[\"predictions\"] = []\n",
    "\n",
    "    #process the image in yolov5\n",
    "    results = self.detector.process_image(image)\n",
    "\n",
    "    #need height/width to de-norm\n",
    "    PILim= PIL.Image.open(image)\n",
    "    width, height = PILim.width, PILim.height\n",
    "\n",
    "    if len(results) > 0:\n",
    "      print(\">>>\", results)\n",
    "      for prediction in results[0]['predictions']:\n",
    "        bbox = prediction.split(\" \")\n",
    "        out[\"predictions\"].append({\n",
    "            \"class\":       self.class_arr[int(bbox[0])],\n",
    "            \"confidence\":               float(bbox[5]),\n",
    "            \"height\": int(PILim.height* float(bbox[4])),\n",
    "            \"width\":  int(PILim.width * float(bbox[3])),\n",
    "            \"x\":      int(PILim.width *(float(bbox[1]) - float(bbox[3])/2)),\n",
    "            \"y\":      int(PILim.height*(float(bbox[2]) - float(bbox[4])/2)),\n",
    "            \"yolov5 format\": prediction\n",
    "            })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a `Viewer` object to investigate the behavior of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "v = Viewer([weight_path], classlist['names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Output hidden; open in https://colab.research.google.com to view."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "results = []\n",
    "for image in samples:\n",
    "  result = v.plot_for(image)\n",
    "  results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /content/drive/My Drive/Coding/ModelAssistedLabel/Image Repo/unlabeled/21-3-22 rowing (200) 1:53-7:00/23.jpg: "
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import random\n",
    "repo = repos[1]\n",
    "result = v.detector.process_image(image)\n",
    "new_images = [os.path.join(repo, x) for x in os.listdir(repo)]\n",
    "# samples = random.sample(new_images, 5)\n",
    "# v.process(samples, plot_results=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
